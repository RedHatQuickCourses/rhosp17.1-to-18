<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Migrating databases to the control plane :: Upgrade a RHOSP 17.1 to RHOSO 18 using the adoption mechanism</title>
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../../_/css/site.css">
    <script>var uiRootPath = '../../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../../..">Upgrade a RHOSP 17.1 to RHOSO 18 using the adoption mechanism</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/REPLACEREPONAME/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="REPLACEREPONAME" data-version="1">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../../index.html">FIXME Course Title</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../../index.html">Upgrade a RHOSP 17.1 to RHOSO 18 using the adoption mechanism</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../prereqs.html">Perform OpenShift Prerequisite</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../install-operators.html">Install the Red Hat OpenStack Platform Service Operators</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../network-isolation.html">Prepare OCP for OpenStack Network Isolation</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../adoption-overview.html">RHOSP 17.1 to RHOSO 18 adoption overview</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../migrating-databases.html">Migrating RHOSP 17.1 Database to RHOSO 18</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../adoption-cp.html">Control plane Adoption</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../adoption-dp.html">Data plane Adoption</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">FIXME Course Title</span>
    <span class="version">1</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../../index.html">FIXME Course Title</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../../index.html">1</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../../index.html">FIXME Course Title</a></li>
    <li><a href="assembly_migrating-databases-to-the-control-plane.html">Migrating databases to the control plane</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Migrating databases to the control plane</h1>
<div id="preamble">
<div class="sectionbody">
<div class="paragraph _abstract">
<p>To begin creating the control plane, enable back-end services and import the databases from your original {rhos_prev_long} {rhos_prev_ver} deployment.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="proc_retrieving-topology-specific-service-configuration_migrating-databases"><a class="anchor" href="#proc_retrieving-topology-specific-service-configuration_migrating-databases"></a>Retrieving topology-specific service configuration</h2>
<div class="sectionbody">
<div class="paragraph _abstract">
<p>Before you migrate your databases to the {rhos_long} control plane, retrieve the topology-specific service configuration from your {rhos_prev_long} ({OpenStackShort}) environment. You need this configuration for the following reasons:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>To check your current database for inaccuracies</p>
</li>
<li>
<p>To ensure that you have the data you need before the migration</p>
</li>
<li>
<p>To compare your {OpenStackShort} database with the adopted {rhos_acro} database
Get the controller VIP, to use it later in the environment variable SOURCE_MARIADB_IP.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>From the workstation, access to the director host and change to the stack user:</p>
</div>
<div class="paragraph">
<p>+</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">ssh 172.25.250.15</code></pre>
</div>
</div>
<div class="paragraph">
<p>Escalate to root:</p>
</div>
<div class="paragraph">
<p>+</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">sudo -i</code></pre>
</div>
</div>
<div class="paragraph">
<p>Change to the stack user:</p>
</div>
<div class="paragraph">
<p>+</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">su - stack</code></pre>
</div>
</div>
<div class="paragraph">
<p>Load the stackrc file:</p>
</div>
<div class="paragraph">
<p>+</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">. stackrc</code></pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>In the <strong>director host</strong>, get the internal_api VIP from this file:</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">cat /home/stack/templates/overcloud-vip-deployed.yaml</code></pre>
</div>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Sample output:</p>
</li>
</ol>
</div>
</li>
</ul>
</div>
<div class="listingblock">
<div class="content">
<pre>[...]
  VipPortMap:
    external:
      ip_address: 172.25.250.109
      ip_address_uri: 172.25.250.109
      ip_subnet: 172.25.250.109/24
    internal_api:
      ip_address: 192.168.52.55
      ip_address_uri: 192.168.52.55
      ip_subnet: 192.168.52.55/24
    storage:
      ip_address: 192.168.50.78
      ip_address_uri: 192.168.50.78
      ip_subnet: 192.168.50.78/24
    storage_mgmt:
      ip_address: 192.168.51.41
      ip_address_uri: 192.168.51.41
      ip_subnet: 192.168.51.41/24
[...]</pre>
</div>
</div>
<div class="paragraph">
<p>You can also get the internal_api VIP by accessing one of the controller nodes:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">[student@director ~] metalsmith list</code></pre>
</div>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Sample output:</p>
</li>
</ol>
</div>
<div class="listingblock">
<div class="content">
<pre>+--------------------------------------+---------------------+--------------------------------------+-------------------------+--------+-------------------------+
| UUID                                 | Node Name           | Allocation UUID                      | Hostname                | State  | IP Addresses            |
+--------------------------------------+---------------------+--------------------------------------+-------------------------+--------+-------------------------+
| 1c708c39-91d6-4500-a395-42c081adea78 | overcloud-control0  | 3b077ad2-ddb4-462d-a573-4a3ab6aa7b84 | overcloud-controller-0  | ACTIVE | ctlplane=172.25.249.16  |
| ffca0025-c9b9-44ff-8aa1-a15d2b6f9163 | overcloud-control1  | a6416542-3e50-45c8-b824-6673722d861e | overcloud-controller-1  | ACTIVE | ctlplane=172.25.249.17  |
| 4b493c16-2dfe-4db6-985b-4aa119e98b14 | overcloud-control2  | 8d82bd92-a096-4841-8214-841a0af1f62d | overcloud-controller-2  | ACTIVE | ctlplane=172.25.249.18  |
| 91e2a13e-0b62-4181-88a3-178efd70c941 | overcloud-compute0  | 3e8426f4-e8bc-40d2-84aa-3c68ccca0dad | overcloud-novacompute-0 | ACTIVE | ctlplane=172.25.249.19  |
| ed1f3a65-e80f-458b-9c2d-873e9073e3bb | overcloud-compute1  | 1f6c4f74-f7cd-4c4f-bd41-89083d441d9d | overcloud-novacompute-1 | ACTIVE | ctlplane=172.25.249.22  |
| 51d45604-8bf8-43f9-9718-d704398296e3 | overcloud-networker | b5f71cd8-5f42-46a8-8c51-a75608745744 | overcloud-networker     | ACTIVE | ctlplane=172.25.249.250 |
+--------------------------------------+---------------------+--------------------------------------+-------------------------+--------+-------------------------+

[source,bash,role=execute,subs=attributes]</pre>
</div>
</div>
<div class="paragraph">
<p>ssh tripleo-admin@172.25.249.16</p>
</div>
<div class="listingblock">
<div class="content">
<pre>[source,bash,role=execute,subs=attributes]</pre>
</div>
</div>
<div class="paragraph">
<p>sudo grep -rI 'listen mysql' -A10 /var/lib/config-data/puppet-generated/ | grep bind</p>
</div>
<div class="listingblock">
<div class="content">
<pre>. Sample output:</pre>
</div>
</div>
<div class="paragraph">
<p>/var/lib/config-data/puppet-generated/haproxy/etc/haproxy/haproxy.cfg-  bind 192.168.52.55:3306 transparent</p>
</div>
<div class="listingblock">
<div class="content">
<pre>[source,bash,subs=attributes]</pre>
</div>
</div>
<div class="paragraph">
<p>logout</p>
</div>
<div class="listingblock">
<div class="content">
<pre>Then the values for SOURCE_MARIADB_IP and CONTROLLER_IP are:
+
SOURCE_MARIADB_IP is then 192.168.52.55
CONTROLLER1_IP is then 172.25.249.16
CONTROLLER2_IP is then 172.25.249.17
CONTROLLER3_IP is then 172.25.249.18
+

In the *director host*, copy the overcloud-passwords from the director node to the student user home directory:
+
[source,bash,role=execute,subs=attributes]</pre>
</div>
</div>
<div class="paragraph">
<p>sudo cp /home/stack/overcloud-deploy/overcloud/overcloud-passwords.yaml /home/student/overcloud-passwords.yaml
sudo chown student:student /home/student/overcloud-passwords.yaml
sudo cp /home/stack/.ssh/id_rsa_tripleo* /home/student/.ssh/
sudo chown student:student /home/student/.ssh/id_rsa_tripleo
sudo chown student:student /home/student/.ssh/id_rsa_tripleo.pub</p>
</div>
<div class="listingblock">
<div class="content">
<pre>+
In the *workstation host*, copy the overcloud-passwords from the director node to the utiliy node:
+
[source,bash,role=execute,subs=attributes]</pre>
</div>
</div>
<div class="paragraph">
<p>scp student@172.25.250.15:~/overcloud-passwords.yaml lab@utility:~/overcloud-passwords.yaml
scp student@172.25.250.15:~/.ssh/id_rsa_tripleo* lab@utility:~/.ssh/</p>
</div>
<div class="listingblock">
<div class="content">
<pre>+
In the *utility host* log in to the Red Hat registry and create the secret, it will be used to pull the helper pod images:
+
[source,bash,role=execute,subs=attributes]</pre>
</div>
</div>
<div class="paragraph">
<p>oc get secret/pull-secret -n openshift-config -o json | jq -r '.data.".dockerconfigjson"' | base64 -d &gt; authfile</p>
</div>
<div class="listingblock">
<div class="content">
<pre>Authenticate to the Red Hat registry:
+
[source,bash,role=execute,subs=attributes]</pre>
</div>
</div>
<div class="paragraph">
<p>podman login registry.redhat.io --authfile authfile</p>
</div>
<div class="listingblock">
<div class="content">
<pre>Set the pull secret:
+
[source,bash,role=execute,subs=attributes]</pre>
</div>
</div>
<div class="paragraph">
<p>oc set data secret/pull-secret -n openshift-config --from-file=.dockerconfigjson=authfile</p>
</div>
<div class="listingblock">
<div class="content">
<pre>+
.Prerequisites

. In the *utility host*, define the following shell variables. Replace the example values with values that are correct for your environment:
+
[source,bash,role=execute]</pre>
</div>
</div>
<div class="paragraph">
<p>PASSWORD_FILE="$HOME/overcloud-passwords.yaml"
MARIADB_IMAGE=registry.redhat.io/rhoso/openstack-mariadb-rhel9:18.0
declare -A TRIPLEO_PASSWORDS
CELLS="default"
for CELL in $(echo $CELLS); do
  TRIPLEO_PASSWORDS[$CELL]="$PASSWORD_FILE"
done
declare -A SOURCE_DB_ROOT_PASSWORD
for CELL in $(echo $CELLS); do
  SOURCE_DB_ROOT_PASSWORD[$CELL]=$(cat ${TRIPLEO_PASSWORDS[$CELL]} | grep ' MysqlRootPassword:' | awk -F ': ' '{ print $2; }')
done</p>
</div>
<div class="listingblock">
<div class="content">
<pre>+

. Define the following shell variables. Replace the example values with values that are correct for your environment:
+
[source,bash,role=execute]</pre>
</div>
</div>
<div class="paragraph">
<p>MARIADB_CLIENT_ANNOTATIONS='--annotations=k8s.v1.cni.cncf.io/networks=internalapi'
MARIADB_RUN_OVERRIDES="$MARIADB_CLIENT_ANNOTATIONS"
CONTROLLER1_SSH="ssh -i /home/lab/.ssh/id_rsa_tripleo tripleo-admin@172.25.249.16"</p>
</div>
<div class="paragraph">
<p>declare -A SOURCE_MARIADB_IP
SOURCE_MARIADB_IP[default]=192.168.52.55</p>
</div>
<div class="listingblock">
<div class="content">
<pre>[NOTE]
The source cloud always uses the same password for cells databases. For that reason, the same passwords file is used for all cells stacks. However, split-stack topology allows using different passwords files for each stack.

.Procedure

. Export the shell variables for the following outputs and test the connection to the {OpenStackShort} database:
+
[source,bash,role=execute]</pre>
</div>
</div>
<div class="paragraph">
<p>unset PULL_OPENSTACK_CONFIGURATION_DATABASES
declare -xA PULL_OPENSTACK_CONFIGURATION_DATABASES
for CELL in $(echo $CELLS); do
  PULL_OPENSTACK_CONFIGURATION_DATABASES[$CELL]=$(oc run mariadb-client-1-$CELL ${MARIADB_RUN_OVERRIDES} -q --image ${MARIADB_IMAGE} -i --rm --restart=Never&#8201;&#8212;&#8201;\
    mysql -rsh "${SOURCE_MARIADB_IP[$CELL]}" -uroot -p"${SOURCE_DB_ROOT_PASSWORD[$CELL]}" -e 'SHOW databases;')
done</p>
</div>
<div class="listingblock">
<div class="content">
<pre>+
If the connection is successful, the expected output is nothing.
+
[NOTE]
The `nova`, `nova_api`, and `nova_cell0` databases are included in the same database host for the main overcloud {orchestration_first_ref} stack.
Additional cells use the `nova` database of their local Galera clusters.

. Run `mysqlcheck` on the {OpenStackShort} database to check for inaccuracies:
+
[source,bash,role=execute]</pre>
</div>
</div>
<div class="paragraph">
<p>unset PULL_OPENSTACK_CONFIGURATION_MYSQLCHECK_NOK
declare -xA PULL_OPENSTACK_CONFIGURATION_MYSQLCHECK_NOK
run_mysqlcheck() {
  PULL_OPENSTACK_CONFIGURATION_MYSQLCHECK_NOK=$(oc run mariadb-client-2-$1 ${MARIADB_RUN_OVERRIDES} -q --image ${MARIADB_IMAGE} -i --rm --restart=Never&#8201;&#8212;&#8201;\
    mysqlcheck --all-databases -h ${SOURCE_MARIADB_IP[$CELL]} -u root -p"${SOURCE_DB_ROOT_PASSWORD[$CELL]}" | grep -v OK)
}
for CELL in $(echo $CELLS); do
  run_mysqlcheck $CELL
done
if [ "$PULL_OPENSTACK_CONFIGURATION_MYSQLCHECK_NOK" != "" ]; then
  for CELL in $(echo $CELLS); do
    MYSQL_UPGRADE=$(oc run mariadb-client-3 ${MARIADB_RUN_OVERRIDES} -q --image ${MARIADB_IMAGE} -i --rm --restart=Never&#8201;&#8212;&#8201;\
      mysql_upgrade -v -h ${SOURCE_MARIADB_IP[$CELL]} -u root -p"${SOURCE_DB_ROOT_PASSWORD[$CELL]}")
    # rerun mysqlcheck to check if problem is resolved
    run_mysqlcheck
  done
fi</p>
</div>
<div class="listingblock">
<div class="content">
<pre>. Get the {compute_service_first_ref} cell mappings:
+
[source,bash,role=execute]</pre>
</div>
</div>
<div class="paragraph">
<p>export PULL_OPENSTACK_CONFIGURATION_NOVADB_MAPPED_CELLS=$(oc run mariadb-client-1 ${MARIADB_RUN_OVERRIDES} -q --image ${MARIADB_IMAGE} -i --rm --restart=Never&#8201;&#8212;&#8201;\
    mysql -rsh "${SOURCE_MARIADB_IP['default']}" -uroot -p"${SOURCE_DB_ROOT_PASSWORD['default']}" nova_api -e \
    'select uuid,name,transport_url,database_connection,disabled from cell_mappings;')</p>
</div>
<div class="listingblock">
<div class="content">
<pre>. Get the hostnames of the registered Compute services:
+
[source,bash,role=execute]</pre>
</div>
</div>
<div class="paragraph">
<p>unset PULL_OPENSTACK_CONFIGURATION_NOVA_COMPUTE_HOSTNAMES
declare -xA PULL_OPENSTACK_CONFIGURATION_NOVA_COMPUTE_HOSTNAMES
for CELL in $(echo $CELLS); do
  PULL_OPENSTACK_CONFIGURATION_NOVA_COMPUTE_HOSTNAMES[$CELL]=$(oc run mariadb-client-4-$CELL ${MARIADB_RUN_OVERRIDES} -q --image ${MARIADB_IMAGE} -i --rm --restart=Never&#8201;&#8212;&#8201;\
    mysql -rsh "${SOURCE_MARIADB_IP[$CELL]}" -uroot -p"${SOURCE_DB_ROOT_PASSWORD[$CELL]}" -e \
      "select host from nova.services where services.binary='nova-compute' and deleted=0;")
done</p>
</div>
<div class="listingblock">
<div class="content">
<pre>. Get the list of the mapped {compute_service} cells:
+
[source,bash,role=execute]</pre>
</div>
</div>
<div class="paragraph">
<p>export PULL_OPENSTACK_CONFIGURATION_NOVAMANAGE_CELL_MAPPINGS=$($CONTROLLER1_SSH sudo podman exec -it nova_conductor nova-manage cell_v2 list_cells)</p>
</div>
<div class="listingblock">
<div class="content">
<pre>. Store the exported variables for future use:
+
[source,bash,role=execute]</pre>
</div>
</div>
<div class="paragraph">
<p>unset SRIOV_AGENTS
declare -xA SRIOV_AGENTS
for CELL in $(echo $CELLS); do
  RCELL=$CELL
  [ "$CELL" = "$DEFAULT_CELL_NAME" ] &amp;&amp; RCELL=default
  cat &gt; ~/.source_cloud_exported_variables_$CELL &lt;&lt; EOF
unset PULL_OPENSTACK_CONFIGURATION_DATABASES
unset PULL_OPENSTACK_CONFIGURATION_MYSQLCHECK_NOK
unset PULL_OPENSTACK_CONFIGURATION_NOVA_COMPUTE_HOSTNAMES
declare -xA PULL_OPENSTACK_CONFIGURATION_DATABASES
declare -xA PULL_OPENSTACK_CONFIGURATION_MYSQLCHECK_NOK
declare -xA PULL_OPENSTACK_CONFIGURATION_NOVA_COMPUTE_HOSTNAMES
PULL_OPENSTACK_CONFIGURATION_DATABASES[$CELL]="$(oc run mariadb-client-5-$CELL ${MARIADB_RUN_OVERRIDES} -q --image ${MARIADB_IMAGE} -i --rm --restart=Never&#8201;&#8212;&#8201;\
  mysql -rsh ${SOURCE_MARIADB_IP[$RCELL]} -uroot -p${SOURCE_DB_ROOT_PASSWORD[$RCELL]} -e 'SHOW databases;')"
PULL_OPENSTACK_CONFIGURATION_MYSQLCHECK_NOK[$CELL]="$(oc run mariadb-client-6-$CELL ${MARIADB_RUN_OVERRIDES} -q --image ${MARIADB_IMAGE} -i --rm --restart=Never&#8201;&#8212;&#8201;\
  mysqlcheck --all-databases -h ${SOURCE_MARIADB_IP[$RCELL]} -u root -p${SOURCE_DB_ROOT_PASSWORD[$RCELL]} | grep -v OK)"
PULL_OPENSTACK_CONFIGURATION_NOVA_COMPUTE_HOSTNAMES[$CELL]="$(oc run mariadb-client-7-$CELL ${MARIADB_RUN_OVERRIDES} -q --image ${MARIADB_IMAGE} -i --rm --restart=Never&#8201;&#8212;&#8201;\
  mysql -rsh ${SOURCE_MARIADB_IP[$RCELL]} -uroot -p${SOURCE_DB_ROOT_PASSWORD[$RCELL]} -e \
  "select host from nova.services where services.binary='nova-compute' and deleted=0;")"
if [ "$RCELL" = "default" ]; then
  PULL_OPENSTACK_CONFIGURATION_NOVADB_MAPPED_CELLS="$(oc run mariadb-client-2 ${MARIADB_RUN_OVERRIDES} -q --image ${MARIADB_IMAGE} -i --rm --restart=Never&#8201;&#8212;&#8201;\
    mysql -rsh ${SOURCE_MARIADB_IP[$RCELL]} -uroot -p${SOURCE_DB_ROOT_PASSWORD[$RCELL]} nova_api -e \
      'select uuid,name,transport_url,database_connection,disabled from cell_mappings;')"
  PULL_OPENSTACK_CONFIGURATION_NOVAMANAGE_CELL_MAPPINGS="$($CONTROLLER1_SSH sudo podman exec -it nova_conductor nova-manage cell_v2 list_cells)"
fi
EOF
done
chmod 0600 ~/.source_cloud_exported_variables*</p>
</div>
<div class="listingblock">
<div class="content">
<pre>+

.Next steps

This configuration and the exported values are required later, during the data plane adoption post-checks. After the {OpenStackShort} control plane services are shut down, if any of the exported values are lost, re-running the `export` command fails because the control plane services are no longer running on the source cloud, and the data cannot be retrieved. To avoid data loss, preserve the exported values in an environment file before shutting down the control plane services.

:leveloffset!:

:leveloffset: +1

:_mod-docs-content-type: PROCEDURE
[id="deploying-backend-services_{context}"]

= Deploying back-end services

[role="_abstract"]
Create the `OpenStackControlPlane` custom resource (CR) with the basic back-end services deployed, and disable all the {rhos_prev_long} ({OpenStackShort}) services. This CR is the foundation of the control plane.

.Prerequisites

* The cloud that you want to adopt is running, and it is on the latest minor version of {OpenStackShort} {rhos_prev_ver}.
* All control plane and data plane hosts of the source cloud are running, and continue to run throughout the adoption procedure.
* The `openstack-operator` is deployed, but `OpenStackControlPlane` is
not deployed.

* There are free PVs available for Galera and RabbitMQ.
* We will use the existing {OpenStackShort} deployment password:
+
[source,bash,role=execute]</pre>
</div>
</div>
<div class="paragraph">
<p>declare -A TRIPLEO_PASSWORDS
TRIPLEO_PASSWORDS[default]="$HOME/overcloud-passwords.yaml"
ADMIN_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' AdminPassword:' | awk -F ': ' '{ print $2; }')</p>
</div>
<div class="listingblock">
<div class="content">
<pre>* Set the service password variables to match the original deployment.
Database passwords can differ in the control plane environment, but
you must synchronize the service account passwords.
+
For example, in developer environments with {OpenStackPreviousInstaller} Standalone, the passwords can be extracted:
+
[source,bash,role=execute]</pre>
</div>
</div>
<div class="paragraph">
<p>AODH_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' AodhPassword:' | awk -F ': ' '{ print $2; }')
BARBICAN_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' BarbicanPassword:' | awk -F ': ' '{ print $2; }')
CEILOMETER_METERING_SECRET=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' CeilometerMeteringSecret:' | awk -F ': ' '{ print $2; }')
CEILOMETER_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' CeilometerPassword:' | awk -F ': ' '{ print $2; }')
CINDER_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' CinderPassword:' | awk -F ': ' '{ print $2; }')
GLANCE_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' GlancePassword:' | awk -F ': ' '{ print $2; }')
HEAT_AUTH_ENCRYPTION_KEY=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' HeatAuthEncryptionKey:' | awk -F ': ' '{ print $2; }')
HEAT_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' HeatPassword:' | awk -F ': ' '{ print $2; }')
HEAT_STACK_DOMAIN_ADMIN_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' HeatStackDomainAdminPassword:' | awk -F ': ' '{ print $2; }')
IRONIC_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' IronicPassword:' | awk -F ': ' '{ print $2; }')
MANILA_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' ManilaPassword:' | awk -F ': ' '{ print $2; }')
NEUTRON_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' NeutronPassword:' | awk -F ': ' '{ print $2; }')
NOVA_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' NovaPassword:' | awk -F ': ' '{ print $2; }')
OCTAVIA_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' OctaviaPassword:' | awk -F ': ' '{ print $2; }')
PLACEMENT_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' PlacementPassword:' | awk -F ': ' '{ print $2; }')
SWIFT_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' SwiftPassword:' | awk -F ': ' '{ print $2; }')</p>
</div>
<div class="listingblock">
<div class="content">
<pre>.Procedure

. Create the {OpenStackShort} secret.
+
[source,bash,role=execute]</pre>
</div>
</div>
<div class="paragraph">
<p>cd /home/lab/labrepo/files
oc apply -f osp-ng-ctlplane-secret.yaml</p>
</div>
<div class="listingblock">
<div class="content">
<pre>. If the `$ADMIN_PASSWORD` is different than the password you set
in `osp-secret`, amend the `AdminPassword` key in the `osp-secret`:
+
[source,bash,role=execute]</pre>
</div>
</div>
<div class="paragraph">
<p>oc set data secret/osp-secret "AdminPassword=$ADMIN_PASSWORD"</p>
</div>
<div class="listingblock">
<div class="content">
<pre>. Set service account passwords in `osp-secret` to match the service
account passwords from the original deployment:
+
[source,bash,role=execute]</pre>
</div>
</div>
<div class="paragraph">
<p>oc set data secret/osp-secret "AodhPassword=$AODH_PASSWORD"
oc set data secret/osp-secret "BarbicanPassword=$BARBICAN_PASSWORD"
oc set data secret/osp-secret "CeilometerPassword=$CEILOMETER_PASSWORD"
oc set data secret/osp-secret "CinderPassword=$CINDER_PASSWORD"
oc set data secret/osp-secret "GlancePassword=$GLANCE_PASSWORD"
oc set data secret/osp-secret "HeatAuthEncryptionKey=$HEAT_AUTH_ENCRYPTION_KEY"
oc set data secret/osp-secret "HeatPassword=$HEAT_PASSWORD"
oc set data secret/osp-secret "HeatStackDomainAdminPassword=$HEAT_STACK_DOMAIN_ADMIN_PASSWORD"
oc set data secret/osp-secret "IronicPassword=$IRONIC_PASSWORD"
oc set data secret/osp-secret "IronicInspectorPassword=$IRONIC_PASSWORD"
oc set data secret/osp-secret "ManilaPassword=$MANILA_PASSWORD"
oc set data secret/osp-secret "MetadataSecret=$METADATA_SECRET"
oc set data secret/osp-secret "NeutronPassword=$NEUTRON_PASSWORD"
oc set data secret/osp-secret "NovaPassword=$NOVA_PASSWORD"
oc set data secret/osp-secret "OctaviaPassword=$OCTAVIA_PASSWORD"
oc set data secret/osp-secret "PlacementPassword=$PLACEMENT_PASSWORD"
oc set data secret/osp-secret "SwiftPassword=$SWIFT_PASSWORD"</p>
</div>
<div class="listingblock">
<div class="content">
<pre>. Deploy the `OpenStackControlPlane` CR. Ensure that you only enable the DNS, Galera, Memcached, and RabbitMQ services. All other services must
be disabled:
+
[source,shell]</pre>
</div>
</div>
<div class="paragraph">
<p>oc apply -f osp-ng-ctlplane-deploy-backend.yaml</p>
</div>
<div class="listingblock">
<div class="content">
<pre>+

.Verification

* Verify that the Galera and RabbitMQ status is `Running` for all defined cells:
+</pre>
</div>
</div>
<div class="paragraph">
<p>RENAMED_CELLS="cell1"
oc get pod openstack-galera-0 -o jsonpath='{.status.phase}{"\n"}'
oc get pod rabbitmq-server-0 -o jsonpath='{.status.phase}{"\n"}'
for CELL in $(echo $RENAMED_CELLS); do
  oc get pod openstack-$CELL-galera-0 -o jsonpath='{.status.phase}{"\n"}'
  oc get pod rabbitmq-$CELL-server-0 -o jsonpath='{.status.phase}{"\n"}'
done</p>
</div>
<div class="listingblock">
<div class="content">
<pre>+
The given cells names are later referred to by using the environment variable `RENAMED_CELLS`.
During the database migration procedure, the Nova cells are renamed. `RENAMED_CELLS` variable represents the new cell names used in the RHOSO deployment.

* Ensure that the statuses of all the Rabbitmq and Galera CRs are `Setup complete`:
+</pre>
</div>
</div>
<div class="paragraph">
<p>oc get Rabbitmqs,Galera
NAME                                                                  STATUS   MESSAGE
rabbitmq.rabbitmq.openstack.org/rabbitmq                              True     Setup complete
rabbitmq.rabbitmq.openstack.org/rabbitmq-cell1                        True     Setup complete</p>
</div>
<div class="paragraph">
<p>NAME                                                               READY   MESSAGE
galera.mariadb.openstack.org/openstack                             True     Setup complete
galera.mariadb.openstack.org/openstack-cell1                       True     Setup complete</p>
</div>
<div class="listingblock">
<div class="content">
<pre>* Verify that the `OpenStackControlPlane` CR is waiting for deployment
  of the `openstackclient` pod:
+</pre>
</div>
</div>
<div class="paragraph">
<p>oc get OpenStackControlPlane openstack
NAME        STATUS    MESSAGE
openstack   Unknown   OpenStackControlPlane Client not started</p>
</div>
<div class="listingblock">
<div class="content">
<pre>:leveloffset: 1

:leveloffset: +1

:_mod-docs-content-type: PROCEDURE
[id="configuring-a-ceph-backend_{context}"]

= Configuring a {Ceph} back end

[role="_abstract"]
If your {rhos_prev_long} ({OpenStackShort}) {rhos_prev_ver} deployment uses a {Ceph} back end for any service, such as {image_service_first_ref}, {block_storage_first_ref}, {compute_service_first_ref}, or {rhos_component_storage_file_first_ref}, you must configure the custom resources (CRs) to use the same back end in the {rhos_long} {rhos_curr_ver} deployment.

[NOTE]
To run `ceph` commands, you must use SSH to connect to a {Ceph} node and run `sudo cephadm shell`. This generates a Ceph orchestrator container that enables you to run administrative commands against the {CephCluster} cluster. If you deployed the {CephCluster} cluster by using {OpenStackPreviousInstaller}, you can launch the `cephadm` shell from an {OpenStackShort} Controller node.

.Prerequisites

* The `OpenStackControlPlane` CR is created.
* If your {OpenStackShort} {rhos_prev_ver} deployment uses the {rhos_component_storage_file}, the openstack keyring is updated.
Using the same user across all services makes it simpler to create a common {Ceph} secret that includes the keyring and `ceph.conf` file and propagate the secret to all the services that need it.
* The following shell variables are defined. Replace the following example values with values that are correct for your environment:
+
[source,bash,role=execute]
[subs=+quotes]</pre>
</div>
</div>
<div class="paragraph">
<p>CEPH_SSH="ssh -i /home/lab/.ssh/lab_rsa student@ceph"
CEPH_KEY=$($CEPH_SSH "sudo cat /etc/ceph/ceph.client.openstack.keyring | base64 -w 0")
CEPH_CONF=$($CEPH_SSH "sudo cat /etc/ceph/ceph.conf | base64 -w 0")</p>
</div>
<div class="listingblock">
<div class="content">
<pre>.Procedure

. Create the `ceph-conf-files` secret that includes the {Ceph} configuration:
+
[source,bash,role=execute]</pre>
</div>
</div>
<div class="paragraph">
<p>oc apply -f - &lt;&lt;EOF
apiVersion: v1
data:
  ceph.client.openstack.keyring: $CEPH_KEY
  ceph.conf: $CEPH_CONF
kind: Secret
metadata:
  name: ceph-conf-files
type: Opaque
EOF</p>
</div>
<div class="listingblock">
<div class="content">
<pre>+
The content of the file should be similar to the following example:
+
[source,yaml]</pre>
</div>
</div>
<div class="paragraph">
<p>apiVersion: v1
kind: Secret
metadata:
  name: ceph-conf-files
stringData:
  ceph.client.openstack.keyring: |
    [client.openstack]
        key = &lt;secret key&gt;
        caps mgr = "allow *"
        caps mon = "allow r, profile rbd"
        caps osd = "pool=vms, profile rbd pool=volumes, profile rbd pool=images, allow rw pool manila_data'
  ceph.conf: |
    [global]
    fsid = 7a1719e8-9c59-49e2-ae2b-d7eb08c695d4
    mon_host = 10.1.1.2,10.1.1.3,10.1.1.4 &lt;1&gt;</p>
</div>
<div class="listingblock">
<div class="content">
<pre>+
&lt;1&gt; If you use IPv6, use brackets for the `mon_host`. For example:
`mon_host = [v2:[fd00:cccc::100]:3300/0,v1:[fd00:cccc::100]:6789/0]`

. In your `OpenStackControlPlane` CR, inject `ceph.conf` and `ceph.client.openstack.keyring` to the {OpenStackShort} services that are defined in the propagation list. For example:
+
[source,yaml]</pre>
</div>
</div>
<div class="paragraph">
<p>oc patch openstackcontrolplane openstack --type=merge --patch '
spec:
  extraMounts:
    - name: v1
      region: r1
      extraVol:
        - propagation:
          - CinderVolume
          - CinderBackup
          - GlanceAPI
          extraVolType: Ceph
          volumes:
          - name: ceph
            projected:
              sources:
              - secret:
                  name: ceph-conf-files
          mounts:
          - name: ceph
            mountPath: "/etc/ceph"
            readOnly: true
'</p>
</div>
<div class="listingblock">
<div class="content">
<pre>:leveloffset: 1

:leveloffset: +1

:_mod-docs-content-type: PROCEDURE
[id="stopping-openstack-services_{context}"]

= Stopping {rhos_prev_long} services

[role="_abstract"]
Before you start the {rhos_long} adoption, you must stop the {rhos_prev_long} ({OpenStackShort}) services to avoid inconsistencies in the data that you migrate for the data plane adoption. Inconsistencies are caused by resource changes after the database is copied to the new deployment.

You should not stop the infrastructure management services yet, such as:

* Database
* RabbitMQ
* HAProxy Load Balancer
* Ceph-nfs
* Compute service
* Containerized modular libvirt daemons
* {object_storage_first_ref} back-end services

.Prerequisites

* Ensure that there no long-running tasks that require the services that you plan to stop, such as instance live migrations, volume migrations, volume creation, backup and restore, attaching, detaching, and other similar operations:
+
* Collect the services topology-specific configuration. For more information, see xref:proc_retrieving-topology-specific-service-configuration_migrating-databases[Retrieving topology-specific service configuration].
* Define the following shell variables. The values are examples and refer to a single node standalone {OpenStackPreviousInstaller} deployment. Replace these example values with values that are correct for your environment:
+
* Specify the IP addresses of all Controller nodes, for example:
+
[source,bash,role=execute]
[subs=+quotes]</pre>
</div>
</div>
<div class="paragraph">
<p>CONTROLLER1_SSH="ssh -i /home/lab/.ssh/id_rsa_tripleo tripleo-admin@172.25.249.16"
CONTROLLER2_SSH="ssh -i /home/lab/.ssh/id_rsa_tripleo tripleo-admin@172.25.249.17"
CONTROLLER3_SSH="ssh -i /home/lab/.ssh/id_rsa_tripleo tripleo-admin@172.25.249.18"</p>
</div>
<div class="listingblock">
<div class="content">
<pre>.Procedure

. Disable {OpenStackShort} control plane services:
+
[source,bash,role=execute]</pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_update_the_services_list_to_be_stopped"><a class="anchor" href="#_update_the_services_list_to_be_stopped"></a>Update the services list to be stopped</h2>
<div class="sectionbody">
<div class="paragraph">
<p>ServicesToStop=("tripleo_aodh_api.service"
                "tripleo_aodh_api_cron.service"
                "tripleo_aodh_evaluator.service"
                "tripleo_aodh_listener.service"
                "tripleo_aodh_notifier.service"
                "tripleo_ceilometer_agent_central.service"
                "tripleo_ceilometer_agent_notification.service"
                "tripleo_octavia_api.service"
                "tripleo_octavia_health_manager.service"
                "tripleo_octavia_rsyslog.service"
                "tripleo_octavia_driver_agent.service"
                "tripleo_octavia_housekeeping.service"
                "tripleo_octavia_worker.service"
                "tripleo_horizon.service"
                "tripleo_keystone.service"
                "tripleo_barbican_api.service"
                "tripleo_barbican_worker.service"
                "tripleo_barbican_keystone_listener.service"
                "tripleo_cinder_api.service"
                "tripleo_cinder_api_cron.service"
                "tripleo_cinder_scheduler.service"
                "tripleo_cinder_volume.service"
                "tripleo_cinder_backup.service"
                "tripleo_collectd.service"
                "tripleo_glance_api.service"
                "tripleo_gnocchi_api.service"
                "tripleo_gnocchi_metricd.service"
                "tripleo_gnocchi_statsd.service"
                "tripleo_manila_api.service"
                "tripleo_manila_api_cron.service"
                "tripleo_manila_scheduler.service"
                "tripleo_neutron_api.service"
                "tripleo_placement_api.service"
                "tripleo_nova_api_cron.service"
                "tripleo_nova_api.service"
                "tripleo_nova_conductor.service"
                "tripleo_nova_metadata.service"
                "tripleo_nova_scheduler.service"
                "tripleo_nova_vnc_proxy.service"
                "tripleo_aodh_api.service"
                "tripleo_aodh_api_cron.service"
                "tripleo_aodh_evaluator.service"
                "tripleo_aodh_listener.service"
                "tripleo_aodh_notifier.service"
                "tripleo_ceilometer_agent_central.service"
                "tripleo_ceilometer_agent_compute.service"
                "tripleo_ceilometer_agent_ipmi.service"
                "tripleo_ceilometer_agent_notification.service"
                "tripleo_ovn_cluster_northd.service"
                "tripleo_ironic_neutron_agent.service"
                "tripleo_ironic_api.service"
                "tripleo_ironic_inspector.service"
                "tripleo_ironic_conductor.service"
                "tripleo_ironic_inspector_dnsmasq.service"
                "tripleo_ironic_pxe_http.service"
                "tripleo_ironic_pxe_tftp.service")</p>
</div>
<div class="paragraph">
<p>PacemakerResourcesToStop=("openstack-cinder-volume"
                          "openstack-cinder-backup"
                          "openstack-manila-share")</p>
</div>
<div class="paragraph">
<p>echo "Stopping systemd OpenStack services"
for service in ${ServicesToStop[*]}; do
    for i in {1..3}; do
        SSH_CMD=CONTROLLER${i}_SSH
        if [ ! -z "${!SSH_CMD}" ]; then
            echo "Stopping the $service in controller $i"
            if ${!SSH_CMD} sudo systemctl is-active $service; then
                ${!SSH_CMD} sudo systemctl stop $service
            fi
        fi
    done
done</p>
</div>
<div class="paragraph">
<p>echo "Checking systemd OpenStack services"
for service in ${ServicesToStop[*]}; do
    for i in {1..3}; do
        SSH_CMD=CONTROLLER${i}_SSH
        if [ ! -z "${!SSH_CMD}" ]; then
            if ! ${!SSH_CMD} systemctl show $service | grep ActiveState=inactive &gt;/dev/null; then
                echo "ERROR: Service $service still running on controller $i"
            else
                echo "OK: Service $service is not running on controller $i"
            fi
        fi
    done
done</p>
</div>
<div class="paragraph">
<p>echo "Stopping pacemaker OpenStack services"
for i in {1..3}; do
    SSH_CMD=CONTROLLER${i}_SSH
    if [ ! -z "${!SSH_CMD}" ]; then
        echo "Using controller $i to run pacemaker commands"
        for resource in ${PacemakerResourcesToStop[*]}; do
            if ${!SSH_CMD} sudo pcs resource config $resource &amp;&gt;/dev/null; then
                echo "Stopping $resource"
                ${!SSH_CMD} sudo pcs resource disable $resource
            else
                echo "Service $resource not present"
            fi
    done
        break
    fi
done</p>
</div>
<div class="paragraph">
<p>echo "Checking pacemaker OpenStack services"
for i in {1..3}; do
    SSH_CMD=CONTROLLER${i}_SSH
    if [ ! -z "${!SSH_CMD}" ]; then
        echo "Using controller $i to run pacemaker commands"
        for resource in ${PacemakerResourcesToStop[*]}; do
            if ${!SSH_CMD} sudo pcs resource config $resource &amp;&gt;/dev/null; then
                if ! ${!SSH_CMD} sudo pcs resource status $resource | grep Started; then
                    echo "OK: Service $resource is stopped"
                else
                    echo "ERROR: Service $resource is started"
                fi
            fi
        done
        break
    fi
    done</p>
</div>
<div class="listingblock">
<div class="content">
<pre>+
If the status of each service is `OK`, then the services stopped successfully.

:leveloffset: 1

:leveloffset: +1

:_mod-docs-content-type: PROCEDURE
[id="migrating-databases-to-mariadb-instances_{context}"]

= Migrating databases to MariaDB instances

[role="_abstract"]
Migrate your databases from the original {rhos_prev_long} ({OpenStackShort}) deployment to the MariaDB instances in the {rhocp_long} cluster.

//[NOTE]
//TOD(bogdando): For OSPDo, this example scenario describes a simple single-cell setup.
//TODO(kgilliga): I hid the same note in the Compute adoption procedure. Will likely reinstate this after multi-cell is released.

.Prerequisites

* Ensure that the control plane MariaDB and RabbitMQ are running, and that no other control plane services are running.
* Retrieve the topology-specific service configuration. For more information, see xref:proc_retrieving-topology-specific-service-configuration_migrating-databases[Retrieving topology-specific service configuration].
* Stop the {OpenStackShort} services. For more information, see xref:stopping-openstack-services_{context}[Stopping {rhos_prev_long} services].
* Ensure that there is network routability between the original MariaDB and the MariaDB for the control plane.
* Define the following shell variables. Replace the following example values with values that are correct for your environment:
+
[source,bash,role=execute]</pre>
</div>
</div>
<div class="paragraph">
<p>STORAGE_CLASS=nfs-storage
MARIADB_IMAGE=registry.redhat.io/rhoso/openstack-mariadb-rhel9:18.0</p>
</div>
<div class="paragraph">
<p>CELLS="default"
DEFAULT_CELL_NAME="cell1"
RENAMED_CELLS="$DEFAULT_CELL_NAME"</p>
</div>
<div class="paragraph">
<p>CHARACTER_SET=utf8
COLLATION=utf8_general_ci</p>
</div>
<div class="paragraph">
<p>declare -A PODIFIED_DB_ROOT_PASSWORD
for CELL in $(echo "super $RENAMED_CELLS"); do
  PODIFIED_DB_ROOT_PASSWORD[$CELL]=$(oc get -o json secret/osp-secret | jq -r .data.DbRootPassword | base64 -d)
done</p>
</div>
<div class="paragraph">
<p>declare -A PODIFIED_MARIADB_IP
for CELL in $(echo "super $RENAMED_CELLS"); do
  if [ "$CELL" = "super" ]; then
    PODIFIED_MARIADB_IP[$CELL]=$(oc get svc --selector "mariadb/name=openstack" -ojsonpath='{.items[0].spec.clusterIP}')
  else
    PODIFIED_MARIADB_IP[$CELL]=$(oc get svc --selector "mariadb/name=openstack-$CELL" -ojsonpath='{.items[0].spec.clusterIP}')
  fi
done</p>
</div>
<div class="paragraph">
<p>declare -A TRIPLEO_PASSWORDS
for CELL in $(echo $CELLS); do
  if [ "$CELL" = "default" ]; then
    TRIPLEO_PASSWORDS[default]="$HOME/overcloud-passwords.yaml"
  else
    # in a split-stack source cloud, it should take a stack-specific passwords file instead
    TRIPLEO_PASSWORDS[$CELL]="$HOME/overcloud-passwords.yaml"
  fi
done</p>
</div>
<div class="paragraph">
<p>declare -A SOURCE_DB_ROOT_PASSWORD
for CELL in $(echo $CELLS); do
  SOURCE_DB_ROOT_PASSWORD[$CELL]=$(cat ${TRIPLEO_PASSWORDS[$CELL]} | grep ' MysqlRootPassword:' | awk -F ': ' '{ print $2; }')
done</p>
</div>
<div class="paragraph">
<p>declare -A SOURCE_MARIADB_IP
SOURCE_MARIADB_IP[default]=192.168.52.55
SOURCE_MARIADB_IP[cell1]=192.168.52.55</p>
</div>
<div class="listingblock">
<div class="content">
<pre>+

[NOTE]
A standalone {OpenStackPreviousInstaller} environment only creates a 'default' cell, which should be the only `CELLS` value in this case. The `DEFAULT_CELL_NAME` value should be `cell1`.

[NOTE]
The `super` is the top-scope Nova API upcall database instance. A super conductor connects to that database. In subsequent examples, the upcall and cells databases use the same password that is defined in `osp-secret`. Old passwords are only needed to prepare the data exports.

* To get the values for `SOURCE_MARIADB_IP`, query the puppet-generated configurations in the Controller and CellController nodes:
+
[source,bash,role=execute]</pre>
</div>
</div>
<div class="paragraph">
<p>sudo grep -rI 'listen mysql' -A10 /var/lib/config-data/puppet-generated/ | grep bind</p>
</div>
<div class="listingblock">
<div class="content">
<pre>+
The source cloud always uses the same password for cells databases. For that reason, the same passwords file is used for all cells stacks. However, split-stack topology allows using different passwords files for each stack.

* Prepare the MariaDB adoption helper pod:

. Create a temporary volume claim and a pod for the database data copy. Edit the volume claim storage request if necessary, to give it enough space for the overcloud databases:
+
[source,bash,role=execute]</pre>
</div>
</div>
<div class="paragraph">
<p>oc apply -f - &lt;&lt;EOF
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mariadb-data
spec:
  storageClassName: $STORAGE_CLASS
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
---
apiVersion: v1
kind: Pod
metadata:
  name: mariadb-copy-data
  annotations:
    openshift.io/scc: anyuid
    k8s.v1.cni.cncf.io/networks: internalapi
  labels:
    app: adoption
spec:
  containers:
  - image: $MARIADB_IMAGE
    command: [ "sh", "-c", "sleep infinity"]
    name: adoption
    volumeMounts:
    - mountPath: /backup
      name: mariadb-data
  securityContext:
    allowPrivilegeEscalation: false
    capabilities:
      drop: ALL
    runAsNonRoot: true
    seccompProfile:
      type: RuntimeDefault
  volumes:
  - name: mariadb-data
    persistentVolumeClaim:
      claimName: mariadb-data
EOF</p>
</div>
<div class="listingblock">
<div class="content">
<pre>. Wait for the pod to be ready:
+</pre>
</div>
</div>
<div class="paragraph">
<p>oc wait --for condition=Ready pod/mariadb-copy-data --timeout=30s</p>
</div>
<div class="listingblock">
<div class="content">
<pre>.Procedure

. Get the count of source databases with the `NOK` (not-OK) status:
+</pre>
</div>
</div>
<div class="paragraph">
<p>for CELL in $(echo $CELLS); do
  oc rsh mariadb-copy-data mysql -h "${SOURCE_MARIADB_IP[$CELL]}" -uroot -p"${SOURCE_DB_ROOT_PASSWORD[$CELL]}" -e "SHOW databases;"
done</p>
</div>
<div class="listingblock">
<div class="content">
<pre>. Check that `mysqlcheck` had no errors:
+</pre>
</div>
</div>
<div class="paragraph">
<p>for CELL in $(echo $CELLS); do
  set +u
  . ~/.source_cloud_exported_variables_$CELL
  set -u
done
test -z "$PULL_OPENSTACK_CONFIGURATION_MYSQLCHECK_NOK"  || [ "x$PULL_OPENSTACK_CONFIGURATION_MYSQLCHECK_NOK" = "x " ] &amp;&amp; echo "OK" || echo "CHECK FAILED"</p>
</div>
<div class="listingblock">
<div class="content">
<pre>. Test the connection to the control plane upcall and cells databases:
+</pre>
</div>
</div>
<div class="paragraph">
<p>for CELL in $(echo "super $RENAMED_CELLS"); do
  oc run mariadb-client --image $MARIADB_IMAGE -i --rm --restart=Never&#8201;&#8212;&#8201;\
    mysql -rsh "${PODIFIED_MARIADB_IP[$CELL]}" -uroot -p"${PODIFIED_DB_ROOT_PASSWORD[$CELL]}" -e 'SHOW databases;'
done</p>
</div>
<div class="listingblock">
<div class="content">
<pre>+
[NOTE]
You must transition Compute services that you import later into a superconductor architecture by deleting the old service records in the cell databases, starting with `cell1`. New records are registered with different hostnames that are provided by the {compute_service} operator. All Compute services, except the Compute agent, have no internal state, and you can safely delete their service records. You also need to rename the former `default` cell to `DEFAULT_CELL_NAME`.

. Create a dump of the original databases:
+</pre>
</div>
</div>
<div class="paragraph">
<p>for CELL in $(echo $CELLS); do
  oc rsh mariadb-copy-data &lt;&lt; EOF
    mysql -h"${SOURCE_MARIADB_IP[$CELL]}" -uroot -p"${SOURCE_DB_ROOT_PASSWORD[$CELL]}" \
    -N -e "show databases" | grep -E -v "schema|mysql|gnocchi|aodh" | \
    while read dbname; do
      echo "Dumping $CELL cell \${dbname}";
      mysqldump -h"${SOURCE_MARIADB_IP[$CELL]}" -uroot -p"${SOURCE_DB_ROOT_PASSWORD[$CELL]}" \
        --single-transaction --complete-insert --skip-lock-tables --lock-tables=0 \
        "\${dbname}" &gt; /backup/"${CELL}.\${dbname}".sql;
    done
EOF
done</p>
</div>
<div class="listingblock">
<div class="content">
<pre>+
Note filtering the information and performance schema tables.
Gnocchi is no longer used as a metric store as well

. Restore the databases from `.sql` files into the control plane MariaDB:
+</pre>
</div>
</div>
<div class="paragraph">
<p>for CELL in $(echo $CELLS); do
  RCELL=$CELL
  [ "$CELL" = "default" ] &amp;&amp; RCELL=$DEFAULT_CELL_NAME
  oc rsh mariadb-copy-data &lt;&lt; EOF
    declare -A db_name_map
    db_name_map['nova']="nova_$RCELL"
    db_name_map['ovs_neutron']='neutron'
    db_name_map['ironic-inspector']='ironic_inspector'
    declare -A db_cell_map
    db_cell_map['nova']="nova_$DEFAULT_CELL_NAME"
    db_cell_map["nova_$RCELL"]="nova_$RCELL"
    declare -A db_server_map
    db_server_map['default']=${PODIFIED_MARIADB_IP['super']}
    db_server_map["nova"]=${PODIFIED_MARIADB_IP[$DEFAULT_CELL_NAME]}
    db_server_map["nova_$RCELL"]=${PODIFIED_MARIADB_IP[$RCELL]}
    declare -A db_server_password_map
    db_server_password_map['default']=${PODIFIED_DB_ROOT_PASSWORD['super']}
    db_server_password_map["nova"]=${PODIFIED_DB_ROOT_PASSWORD[$DEFAULT_CELL_NAME]}
    db_server_password_map["nova_$RCELL"]=${PODIFIED_DB_ROOT_PASSWORD[$RCELL]}
    cd /backup
    for db_file in \$(ls ${CELL}.*.sql); do
      db_name=\$(echo \${db_file} | awk -F'.' '{ print \$2; }')
      [[ "$CELL" != "default" &amp;&amp; ! -v "db_cell_map[\${db_name}]" ]] &amp;&amp; continue
      if [[ "$CELL" == "default" &amp;&amp; -v "db_cell_map[\${db_name}]" ]] ; then
        target=$DEFAULT_CELL_NAME
      elif [[ "$CELL" == "default" &amp;&amp; ! -v "db_cell_map[\${db_name}]" ]] ; then
        target=super
      else
        target=$RCELL
      fi
      renamed_db_file="\${target}_new.\${db_name}.sql"
      mv -f \${db_file} \${renamed_db_file}
      if [[ -v "db_name_map[\${db_name}]" ]]; then
        echo "renaming $CELL cell \${db_name} to \$target \${db_name_map[\${db_name}]}"
        db_name=\${db_name_map[\${db_name}]}
      fi
      db_server=\${db_server_map["default"]}
      if [[ -v "db_server_map[\${db_name}]" ]]; then
        db_server=\${db_server_map[\${db_name}]}
      fi
      db_password=\${db_server_password_map['default']}
      if [[ -v "db_server_password_map[\${db_name}]" ]]; then
        db_password=\${db_server_password_map[\${db_name}]}
      fi
      echo "creating $CELL cell \${db_name} in \$target \${db_server}"
      mysql -h"\${db_server}" -uroot "-p\${db_password}" -e \
        "CREATE DATABASE IF NOT EXISTS \${db_name} DEFAULT \
        CHARACTER SET ${CHARACTER_SET} DEFAULT COLLATE ${COLLATION};"
      echo "importing $CELL cell \${db_name} into \$target \${db_server} from \${renamed_db_file}"
      mysql -h "\${db_server}" -uroot "-p\${db_password}" "\${db_name}" &lt; "\${renamed_db_file}"
    done
    if [ "$CELL" = "default" ] ; then
      mysql -h "\${db_server_map['default']}" -uroot -p"\${db_server_password_map['default']}" -e \
        "update nova_api.cell_mappings set name='$DEFAULT_CELL_NAME' where name='default';"
    fi
    mysql -h "\${db_server_map["nova_$RCELL"]}" -uroot -p"\${db_server_password_map["nova_$RCELL"]}" -e \
      "delete from nova_${RCELL}.services where host not like '%nova_${RCELL}-%' and services.binary != 'nova-compute';"
EOF
done</p>
</div>
<div class="listingblock">
<div class="content">
<pre>+

.Verification

Compare the following outputs with the topology-specific service configuration.
For more information, see xref:proc_retrieving-topology-specific-service-configuration_migrating-databases[Retrieving topology-specific service configuration].

. Check that the databases are imported correctly:
+</pre>
</div>
</div>
<div class="paragraph">
<p>set u
. ~/.source_cloud_exported_variables_default
set -u
dbs=$(oc exec openstack-galera-0 -c galera -- mysql -rs -uroot -p"${PODIFIED_DB_ROOT_PASSWORD['super']}" -e 'SHOW databases;')
echo $dbs | grep -Eq '\bkeystone\b' &amp;&amp; echo "OK" || echo "CHECK FAILED"
echo $dbs | grep -Eq '\bneutron\b' &amp;&amp; echo "OK" || echo "CHECK FAILED"
echo "${PULL_OPENSTACK_CONFIGURATION_DATABASES[@]}" | grep -Eq '\bovs_neutron\b' &amp;&amp; echo "OK" || echo "CHECK FAILED"
novadb_mapped_cells=$(oc exec openstack-galera-0 -c galera -- mysql -rs -uroot -p"${PODIFIED_DB_ROOT_PASSWORD['super']}" \
nova_api -e 'select uuid,name,transport_url,database_connection,disabled from cell_mappings;')
uuidf='\S{8,}-\S{4,}-\S{4,}-\S{4,}-\S{12,}'
default=$(printf "%s\n" "$PULL_OPENSTACK_CONFIGURATION_NOVADB_MAPPED_CELLS" | sed -rn "s/^($uuidf)\s+default\b.*$/\1/p")
difference=$(diff -ZNua \
  &lt;(printf "%s\n" "$PULL_OPENSTACK_CONFIGURATION_NOVADB_MAPPED_CELLS") \
  &lt;(printf "%s\n" "$novadb_mapped_cells")) || true
if [ "$DEFAULT_CELL_NAME" != "default" ]; then
  printf "%s\n" "$difference" | grep -qE "^\-$default\s+default\b" &amp;&amp; echo "OK" || echo "CHECK FAILED"
  printf "%s\n" "$difference" | grep -qE "^\$default\s+$DEFAULT_CELL_NAME\b" &amp;&amp; echo "OK" || echo "CHECK FAILED"
  [ $(grep -E "^[-\+]$uuidf" &lt;&lt;&lt;"$difference" | wc -l) -eq 2 ] &amp;&amp; echo "OK" || echo "CHECK FAILED"
else
  [ "x$difference" = "x" ] &amp;&amp; echo "OK" || echo "CHECK FAILED"
fi
for CELL in $(echo $RENAMED_CELLS); do
  RCELL=$CELL
  [ "$CELL" = "$DEFAULT_CELL_NAME" ] &amp;&amp; RCELL=default
  set +u
  . ~/.source_cloud_exported_variables_$RCELL
  set -u
  c1dbs=$(oc exec openstack-$CELL-galera-0 -c galera&#8201;&#8212;&#8201;mysql -rs -uroot -p${PODIFIED_DB_ROOT_PASSWORD[$CELL]} -e 'SHOW databases;')
  echo $c1dbs | grep -Eq "\bnova_${CELL}\b" &amp;&amp; echo "OK" || echo "CHECK FAILED"
  novadb_svc_records=$(oc exec openstack-$CELL-galera-0 -c galera&#8201;&#8212;&#8201;mysql -rs -uroot -p${PODIFIED_DB_ROOT_PASSWORD[$CELL]} \
    nova_$CELL -e "select host from services where services.binary='nova-compute' and deleted=0 order by host asc;")
  diff -Z &lt;(echo "x$novadb_svc_records") &lt;(echo "x${PULL_OPENSTACK_CONFIGURATION_NOVA_COMPUTE_HOSTNAMES[@]}") &amp;&amp; echo "OK" || echo "CHECK FAILED"
done</p>
</div>
<div class="listingblock">
<div class="content">
<pre>+

. Delete the `mariadb-data` pod and the `mariadb-copy-data` persistent volume claim that contains the database backup:
+
[NOTE]
Consider taking a snapshot of them before deleting.
+</pre>
</div>
</div>
<div class="paragraph">
<p>oc delete pod mariadb-copy-data
oc delete pvc mariadb-data</p>
</div>
<div class="listingblock">
<div class="content">
<pre>[NOTE]
During the pre-checks and post-checks, the `mariadb-client` pod might return a pod security warning related to the `restricted:latest` security context constraint. This warning is due to default security context constraints and does not prevent the admission controller from creating a pod. You see a warning for the short-lived pod, but it does not interfere with functionality.
For more information, see link:https://learn.redhat.com/t5/DO280-Red-Hat-OpenShift/About-pod-security-standards-and-warnings/m-p/32502[About pod security standards and warnings].

:leveloffset: 1

:leveloffset: +1

:_mod-docs-content-type: PROCEDURE
[id="migrating-ovn-data_{context}"]

= Migrating OVN data

[role="_abstract"]
Migrate the data in the OVN databases from the original {rhos_prev_long} deployment to `ovsdb-server` instances that are running in the {rhocp_long} cluster.

.Prerequisites

* The `OpenStackControlPlane` resource is created.
* `NetworkAttachmentDefinition` custom resources (CRs) for the original cluster are defined. Specifically, the `internalapi` network is defined.
* The original {networking_first_ref} and OVN `northd` are not running.
* There is network routability between the control plane services and the adopted cluster.
* The cloud is migrated to the Modular Layer 2 plug-in with Open Virtual Networking (ML2/OVN) mechanism driver.
* Define the following shell variables. Replace the example values with values that are correct for your environment:
+</pre>
</div>
</div>
<div class="paragraph">
<p>STORAGE_CLASS=nfs-storage
OVSDB_IMAGE=registry.redhat.io/rhoso/openstack-ovn-base-rhel9:18.0
SOURCE_OVSDB_IP=192.168.52.16</p>
</div>
<div class="listingblock">
<div class="content">
<pre>+
To get the value to set `SOURCE_OVSDB_IP`, query the puppet-generated configurations in a Controller node:
+</pre>
</div>
</div>
<div class="paragraph">
<p>grep -rI 'ovn_[ns]b_conn' /var/lib/config-data/puppet-generated/</p>
</div>
<div class="listingblock">
<div class="content">
<pre>.Procedure
. Prepare a temporary `PersistentVolume` claim and the helper pod for the OVN backup. Adjust the storage requests for a large database, if needed:
+
[source,yaml]</pre>
</div>
</div>
<div class="paragraph">
<p>oc apply -f - &lt;&lt;EOF
---
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: ovn-data-cert
spec:
  commonName: ovn-data-cert
  secretName: ovn-data-cert
  issuerRef:
    name: rootca-internal
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ovn-data
spec:
  storageClassName: $STORAGE_CLASS
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
---
apiVersion: v1
kind: Pod
metadata:
  name: ovn-copy-data
  annotations:
    openshift.io/scc: anyuid
    k8s.v1.cni.cncf.io/networks: internalapi
  labels:
    app: adoption
spec:
  containers:
  - image: $OVSDB_IMAGE
    command: [ "sh", "-c", "sleep infinity"]
    name: adoption
    volumeMounts:
    - mountPath: /backup
      name: ovn-data
    - mountPath: /etc/pki/tls/misc
      name: ovn-data-cert
      readOnly: true
  securityContext:
    allowPrivilegeEscalation: false
    capabilities:
      drop: ALL
    runAsNonRoot: true
    seccompProfile:
      type: RuntimeDefault
  volumes:
  - name: ovn-data
    persistentVolumeClaim:
      claimName: ovn-data
  - name: ovn-data-cert
    secret:
      secretName: ovn-data-cert
EOF</p>
</div>
<div class="listingblock">
<div class="content">
<pre>+

. Wait for the pod to be ready:
+</pre>
</div>
</div>
<div class="paragraph">
<p>oc wait --for=condition=Ready pod/ovn-copy-data --timeout=30s</p>
</div>
<div class="listingblock">
<div class="content">
<pre>. Back up your OVN databases:
* If you did not enable TLS everywhere, run the following command:
+</pre>
</div>
</div>
<div class="paragraph">
<p>oc exec ovn-copy-data&#8201;&#8212;&#8201;bash -c "ovsdb-client backup tcp:$SOURCE_OVSDB_IP:6641 &gt; /backup/ovs-nb.db"
oc exec ovn-copy-data&#8201;&#8212;&#8201;bash -c "ovsdb-client backup tcp:$SOURCE_OVSDB_IP:6642 &gt; /backup/ovs-sb.db"</p>
</div>
<div class="listingblock">
<div class="content">
<pre>+

. Start the control plane OVN database services prior to import, with `northd` disabled:
+
[source,yaml]</pre>
</div>
</div>
<div class="paragraph">
<p>oc patch openstackcontrolplane openstack --type=merge --patch '
spec:
  ovn:
    enabled: true
    template:
      ovnDBCluster:
        ovndbcluster-nb:
          replicas: 3
          dbType: NB
          storageRequest: 10G
          networkAttachment: internalapi
        ovndbcluster-sb:
          replicas: 3
          dbType: SB
          storageRequest: 10G
          networkAttachment: internalapi
      ovnNorthd:
        replicas: 0
'</p>
</div>
<div class="listingblock">
<div class="content">
<pre>. Wait for the OVN database services to reach the `Running` phase:
+</pre>
</div>
</div>
<div class="paragraph">
<p>oc wait --for=jsonpath='{.status.phase}'=Running pod --selector=service=ovsdbserver-nb
oc wait --for=jsonpath='{.status.phase}'=Running pod --selector=service=ovsdbserver-sb</p>
</div>
<div class="listingblock">
<div class="content">
<pre>. Fetch the OVN database IP addresses on the `clusterIP` service network:
+</pre>
</div>
</div>
<div class="paragraph">
<p>PODIFIED_OVSDB_NB_IP=$(oc get svc --selector "statefulset.kubernetes.io/pod-name=ovsdbserver-nb-0" -ojsonpath='{.items[0].spec.clusterIP}')
PODIFIED_OVSDB_SB_IP=$(oc get svc --selector "statefulset.kubernetes.io/pod-name=ovsdbserver-sb-0" -ojsonpath='{.items[0].spec.clusterIP}')</p>
</div>
<div class="listingblock">
<div class="content">
<pre>. Upgrade the database schema for the backup files:
.. If you did not enable TLS everywhere, use the following command:
+</pre>
</div>
</div>
<div class="paragraph">
<p>oc exec ovn-copy-data&#8201;&#8212;&#8201;bash -c "ovsdb-client get-schema tcp:$PODIFIED_OVSDB_NB_IP:6641 &gt; /backup/ovs-nb.ovsschema &amp;&amp; ovsdb-tool convert /backup/ovs-nb.db /backup/ovs-nb.ovsschema"
oc exec ovn-copy-data&#8201;&#8212;&#8201;bash -c "ovsdb-client get-schema tcp:$PODIFIED_OVSDB_SB_IP:6642 &gt; /backup/ovs-sb.ovsschema &amp;&amp; ovsdb-tool convert /backup/ovs-sb.db /backup/ovs-sb.ovsschema"</p>
</div>
<div class="listingblock">
<div class="content">
<pre>. Restore the database backup to the new OVN database servers:
.. If you did not enable TLS everywhere, use the following command:
+</pre>
</div>
</div>
<div class="paragraph">
<p>oc exec ovn-copy-data&#8201;&#8212;&#8201;bash -c "ovsdb-client restore tcp:$PODIFIED_OVSDB_NB_IP:6641 &lt; /backup/ovs-nb.db"
oc exec ovn-copy-data&#8201;&#8212;&#8201;bash -c "ovsdb-client restore tcp:$PODIFIED_OVSDB_SB_IP:6642 &lt; /backup/ovs-sb.db"</p>
</div>
<div class="listingblock">
<div class="content">
<pre>. Check that the data was successfully migrated by running the following commands against the new database servers, for example:
+</pre>
</div>
</div>
<div class="paragraph">
<p>oc exec -it ovsdbserver-nb-0&#8201;&#8212;&#8201;ovn-nbctl show
oc exec -it ovsdbserver-sb-0&#8201;&#8212;&#8201;ovn-sbctl list Chassis</p>
</div>
<div class="listingblock">
<div class="content">
<pre>. Start the control plane `ovn-northd` service to keep both OVN databases in sync:
+
[source,yaml]</pre>
</div>
</div>
<div class="paragraph">
<p>oc patch openstackcontrolplane openstack --type=merge --patch '
spec:
  ovn:
    enabled: true
    template:
      ovnNorthd:
        replicas: 1
'</p>
</div>
<div class="listingblock">
<div class="content">
<pre>. Delete the `ovn-data` helper pod and the temporary `PersistentVolumeClaim` that is used to store OVN database backup files:
+</pre>
</div>
</div>
<div class="paragraph">
<p>oc delete --ignore-not-found=true pod ovn-copy-data
oc delete --ignore-not-found=true pvc ovn-data</p>
</div>
<div class="listingblock">
<div class="content">
<pre>+
[NOTE]
Consider taking a snapshot of the `ovn-data` helper pod and the temporary `PersistentVolumeClaim` before deleting them. For more information, see link:{defaultOCPURL}/storage/index#lvms-about-volume-snapsot_logical-volume-manager-storage[About volume snapshots] in _OpenShift Container Platform storage overview_.

. Stop the adopted OVN database servers:
+</pre>
</div>
</div>
<div class="paragraph">
<p>ServicesToStop=("tripleo_ovn_cluster_north_db_server.service"
                "tripleo_ovn_cluster_south_db_server.service")</p>
</div>
<div class="paragraph">
<p>echo "Stopping systemd OpenStack services"
for service in ${ServicesToStop[*]}; do
    for i in {1..3}; do
        SSH_CMD=CONTROLLER${i}_SSH
        if [ ! -z "${!SSH_CMD}" ]; then
            echo "Stopping the $service in controller $i"
            if ${!SSH_CMD} sudo systemctl is-active $service; then
                ${!SSH_CMD} sudo systemctl stop $service
            fi
        fi
    done
done</p>
</div>
<div class="paragraph">
<p>echo "Checking systemd OpenStack services"
for service in ${ServicesToStop[*]}; do
    for i in {1..3}; do
        SSH_CMD=CONTROLLER${i}_SSH
        if [ ! -z "${!SSH_CMD}" ]; then
            if ! ${!SSH_CMD} systemctl show $service | grep ActiveState=inactive &gt;/dev/null; then
                echo "ERROR: Service $service still running on controller $i"
            else
                echo "OK: Service $service is not running on controller $i"
            fi
        fi
    done
done</p>
</div>
<div class="listingblock">
<div class="content">
<pre>:leveloffset: 1

:!context:</pre>
</div>
</div>
</div>
</div>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../../_/js/site.js" data-ui-root-path="../../../../_"></script>
<script async src="../../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
