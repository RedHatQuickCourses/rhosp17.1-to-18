<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Untitled :: Upgrade a RHOSP 17.1 to RHOSO 18 using the adoption mechanism</title>
    <link rel="prev" href="adoption-overview.html">
    <link rel="next" href="adoption-cp.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">Upgrade a RHOSP 17.1 to RHOSO 18 using the adoption mechanism</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/REPLACEREPONAME/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="REPLACEREPONAME" data-version="1">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">FIXME Course Title</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../index.html">Upgrade a RHOSP 17.1 to RHOSO 18 using the adoption mechanism</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="prereqs.html">Perform OpenShift Prerequisite</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="install-operators.html">Install the Red Hat OpenStack Platform Service Operators</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="network-isolation.html">Prepare OCP for OpenStack Network Isolation</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="adoption-overview.html">RHOSP 17.1 to RHOSO 18 adoption overview</a>
  </li>
  <li class="nav-item is-current-page" data-depth="2">
    <a class="nav-link" href="migrating-databases.html">Migrating RHOSP 17.1 Database to RHOSO 18</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="adoption-cp.html">Control plane Adoption</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="adoption-dp.html">Option - Data plane Adoption</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../LABENV/index.html">Lab Environment</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter1/index.html">Chapter 1</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter1/section1.html">Section 1</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter1/section2.html">Section 2</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter1/section3.html">Section 3</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter2/index.html">Chapter 2</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter2/section1.html">Section 1</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter3/index.html">Chapter 3</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter3/section1.html">Section 1</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter3/section2.html">Section 2</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../appendix/appendix.html">Appendix</a>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">FIXME Course Title</span>
    <span class="version">1</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">FIXME Course Title</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">1</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">FIXME Course Title</a></li>
    <li><a href="../index.html">Upgrade a RHOSP 17.1 to RHOSO 18 using the adoption mechanism</a></li>
    <li><a href="migrating-databases.html">Migrating RHOSP 17.1 Database to RHOSO 18</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<div class="sect1">
<h2 id="migrating-databases-to-the-control-plane_assembly"><a class="anchor" href="#migrating-databases-to-the-control-plane_assembly"></a>Migrating databases to the control plane</h2>
<div class="sectionbody">
<div class="paragraph _abstract">
<p>To begin creating the control plane, enable back-end services and import the databases from your original Red&#160;Hat OpenStack Platform 17.1 deployment.</p>
</div>
<div class="sect2">
<h3 id="proc_retrieving-topology-specific-service-configuration_migrating-databases"><a class="anchor" href="#proc_retrieving-topology-specific-service-configuration_migrating-databases"></a>Retrieving topology-specific service configuration</h3>
<div class="paragraph _abstract">
<p>Before you migrate your databases to the Red&#160;Hat OpenStack Services on OpenShift (RHOSO) control plane, retrieve the topology-specific service configuration from your Red&#160;Hat OpenStack Platform (RHOSP) environment. You need this configuration for the following reasons:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>To check your current database for inaccuracies</p>
</li>
<li>
<p>To ensure that you have the data you need before the migration</p>
</li>
<li>
<p>To compare your RHOSP database with the adopted RHOSO database
Get the controller VIP, to use it later in the environment variable SOURCE_MARIADB_IP.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>From the workstation, access to the director host and change to the stack user:</p>
</div>
<div class="paragraph">
<p>+</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">ssh 172.25.250.15
sudo -i
su - stack</code></pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>In the <strong>director host</strong>, get the internal_api VIP from this file:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">[student@director ~]cat /home/stack/templates/overcloud-vip-deployed.yaml

[...]
  VipPortMap:
    external:
      ip_address: 172.25.250.109
      ip_address_uri: 172.25.250.109
      ip_subnet: 172.25.250.109/24
    internal_api:
      ip_address: 192.168.52.55
      ip_address_uri: 192.168.52.55
      ip_subnet: 192.168.52.55/24
    storage:
      ip_address: 192.168.50.78
      ip_address_uri: 192.168.50.78
      ip_subnet: 192.168.50.78/24
    storage_mgmt:
      ip_address: 192.168.51.41
      ip_address_uri: 192.168.51.41
      ip_subnet: 192.168.51.41/24
[...]

[student@director ~] metalsmith list
+--------------------------------------+---------------------+--------------------------------------+-------------------------+--------+-------------------------+
| UUID                                 | Node Name           | Allocation UUID                      | Hostname                | State  | IP Addresses            |
+--------------------------------------+---------------------+--------------------------------------+-------------------------+--------+-------------------------+
| 1c708c39-91d6-4500-a395-42c081adea78 | overcloud-control0  | 3b077ad2-ddb4-462d-a573-4a3ab6aa7b84 | overcloud-controller-0  | ACTIVE | ctlplane=172.25.249.16  |
| ffca0025-c9b9-44ff-8aa1-a15d2b6f9163 | overcloud-control1  | a6416542-3e50-45c8-b824-6673722d861e | overcloud-controller-1  | ACTIVE | ctlplane=172.25.249.17  |
| 4b493c16-2dfe-4db6-985b-4aa119e98b14 | overcloud-control2  | 8d82bd92-a096-4841-8214-841a0af1f62d | overcloud-controller-2  | ACTIVE | ctlplane=172.25.249.18  |
| 91e2a13e-0b62-4181-88a3-178efd70c941 | overcloud-compute0  | 3e8426f4-e8bc-40d2-84aa-3c68ccca0dad | overcloud-novacompute-0 | ACTIVE | ctlplane=172.25.249.19  |
| ed1f3a65-e80f-458b-9c2d-873e9073e3bb | overcloud-compute1  | 1f6c4f74-f7cd-4c4f-bd41-89083d441d9d | overcloud-novacompute-1 | ACTIVE | ctlplane=172.25.249.22  |
| 51d45604-8bf8-43f9-9718-d704398296e3 | overcloud-networker | b5f71cd8-5f42-46a8-8c51-a75608745744 | overcloud-networker     | ACTIVE | ctlplane=172.25.249.250 |
+--------------------------------------+---------------------+--------------------------------------+-------------------------+--------+-------------------------+

[student@director ~] ssh tripleo-admin@172.25.249.16
[tripleo-admin@overcloud-controller-0 ~]$
sudo grep -rI 'listen mysql' -A10 /var/lib/config-data/puppet-generated/ | grep bind
/var/lib/config-data/puppet-generated/haproxy/etc/haproxy/haproxy.cfg-  bind 192.168.52.55:3306 transparent

[tripleo-admin@overcloud-controller-0 ~]$ logout</code></pre>
</div>
</div>
<div class="paragraph">
<p>SOURCE_MARIADB_IP is then 192.168.52.55
CONTROLLER1_IP is then 172.25.249.16</p>
</div>
<div class="paragraph">
<p>In the <strong>director host</strong>, copy the overcloud-passwords from the director node to the student user home directory:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">sudo cp /home/stack/overcloud-deploy/overcloud/overcloud-passwords.yaml /home/student/overcloud-passwords.yaml
sudo chown student:student /home/student/overcloud-passwords.yaml
sudo cp /home/stack/.ssh/id_rsa_tripleo* /home/student/.ssh/
sudo chown student:student /home/student/.ssh/id_rsa_tripleo
sudo chown student:student /home/student/.ssh/id_rsa_tripleo.pub</code></pre>
</div>
</div>
<div class="paragraph">
<p>In the <strong>workstation host</strong>, copy the overcloud-passwords from the director node to the utiliy node:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">scp student@172.25.250.15:~/overcloud-passwords.yaml lab@utility:~/overcloud-passwords.yaml
scp student@172.25.250.15:~/.ssh/id_rsa_tripleo* lab@utility:~/.ssh/</code></pre>
</div>
</div>
<div class="olist arabic">
<div class="title">Prerequisites</div>
<ol class="arabic">
<li>
<p>In the <strong>utility host</strong>, define the following shell variables. Replace the example values with values that are correct for your environment:</p>
<div class="listingblock">
<div class="content">
<pre>$ PASSWORD_FILE="$HOME/overcloud-passwords.yaml"
$ MARIADB_IMAGE=registry.redhat.io/rhoso/openstack-mariadb-rhel9:18.0
$ declare -A TRIPLEO_PASSWORDS
$ CELLS="default cell1 cell2"
$ for CELL in $(echo $CELLS); do
&gt;    TRIPLEO_PASSWORDS[$CELL]="$PASSWORD_FILE"
&gt; done
$ declare -A SOURCE_DB_ROOT_PASSWORD
$ for CELL in $(echo $CELLS); do
&gt;     SOURCE_DB_ROOT_PASSWORD[$CELL]=$(cat ${TRIPLEO_PASSWORDS[$CELL]} | grep ' MysqlRootPassword:' | awk -F ': ' '{ print $2; }')
&gt; done</pre>
</div>
</div>
</li>
<li>
<p>Define the following shell variables. Replace the example values with values that are correct for your environment:</p>
<div class="listingblock">
<div class="content">
<pre>$ MARIADB_CLIENT_ANNOTATIONS='--annotations=k8s.v1.cni.cncf.io/networks=internalapi'
$ MARIADB_RUN_OVERRIDES="$MARIADB_CLIENT_ANNOTATIONS"

$ CONTROLLER1_SSH="ssh -i .ssh/id_rsa_tripleo tripleo-admin@172.25.249.16"

$ declare -A SOURCE_MARIADB_IP
$ SOURCE_MARIADB_IP[default]=192.168.52.55
$ SOURCE_MARIADB_IP[cell1]=192.168.52.55
$ SOURCE_MARIADB_IP[cell2]=192.168.52.55</pre>
</div>
</div>
</li>
</ol>
</div>
</li>
</ul>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The source cloud always uses the same password for cells databases. For that reason, the same passwords file is used for all cells stacks. However, split-stack topology allows using different passwords files for each stack.
</td>
</tr>
</table>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Export the shell variables for the following outputs and test the connection to the RHOSP database:</p>
<div class="listingblock">
<div class="content">
<pre>$ unset PULL_OPENSTACK_CONFIGURATION_DATABASES
$ declare -xA PULL_OPENSTACK_CONFIGURATION_DATABASES
$ for CELL in $(echo $CELLS); do
&gt;     PULL_OPENSTACK_CONFIGURATION_DATABASES[$CELL]=$(oc run mariadb-client-1-$CELL ${MARIADB_RUN_OVERRIDES} -q --image ${MARIADB_IMAGE} -i --rm --restart=Never -- \
&gt;         mysql -rsh "${SOURCE_MARIADB_IP[$CELL]}" -uroot -p"${SOURCE_DB_ROOT_PASSWORD[$CELL]}" -e 'SHOW databases;')
&gt; done</pre>
</div>
</div>
<div class="paragraph">
<p>If the connection is successful, the expected output is nothing.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The <code>nova</code>, <code>nova_api</code>, and <code>nova_cell0</code> databases are included in the same database host for the main overcloud Orchestration service (heat) stack.
Additional cells use the <code>nova</code> database of their local Galera clusters.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Run <code>mysqlcheck</code> on the RHOSP database to check for inaccuracies:</p>
<div class="listingblock">
<div class="content">
<pre>$ unset PULL_OPENSTACK_CONFIGURATION_MYSQLCHECK_NOK
$ declare -xA PULL_OPENSTACK_CONFIGURATION_MYSQLCHECK_NOK
$ run_mysqlcheck() {
&gt;     PULL_OPENSTACK_CONFIGURATION_MYSQLCHECK_NOK=$(oc run mariadb-client-2-$1 ${MARIADB_RUN_OVERRIDES} -q --image ${MARIADB_IMAGE} -i --rm --restart=Never -- \
&gt;         mysqlcheck --all-databases -h ${SOURCE_MARIADB_IP[$CELL]} -u root -p"${SOURCE_DB_ROOT_PASSWORD[$CELL]}" | grep -v OK)
&gt; }
$ for CELL in $(echo $CELLS); do
&gt;     run_mysqlcheck $CELL
&gt; done
$ if [ "$PULL_OPENSTACK_CONFIGURATION_MYSQLCHECK_NOK" != "" ]; then
&gt;     for CELL in $(echo $CELLS); do
&gt;         MYSQL_UPGRADE=$(oc run mariadb-client-3 ${MARIADB_RUN_OVERRIDES} -q --image ${MARIADB_IMAGE} -i --rm --restart=Never -- \
&gt;             mysql_upgrade -v -h ${SOURCE_MARIADB_IP[$CELL]} -u root -p"${SOURCE_DB_ROOT_PASSWORD[$CELL]}")
&gt;         # rerun mysqlcheck to check if problem is resolved
&gt;         run_mysqlcheck
&gt;     done
&gt; fi</pre>
</div>
</div>
</li>
<li>
<p>Get the Compute service (nova) cell mappings:</p>
<div class="listingblock">
<div class="content">
<pre>export PULL_OPENSTACK_CONFIGURATION_NOVADB_MAPPED_CELLS=$(oc run mariadb-client-1 ${MARIADB_RUN_OVERRIDES} -q --image ${MARIADB_IMAGE} -i --rm --restart=Never -- \
    mysql -rsh "${SOURCE_MARIADB_IP['default']}" -uroot -p"${SOURCE_DB_ROOT_PASSWORD['default']}" nova_api -e \
    'select uuid,name,transport_url,database_connection,disabled from cell_mappings;')</pre>
</div>
</div>
</li>
<li>
<p>Get the hostnames of the registered Compute services:</p>
<div class="listingblock">
<div class="content">
<pre>$ unset PULL_OPENSTACK_CONFIGURATION_NOVA_COMPUTE_HOSTNAMES
$ declare -xA PULL_OPENSTACK_CONFIGURATION_NOVA_COMPUTE_HOSTNAMES
$ for CELL in $(echo $CELLS); do
&gt;     PULL_OPENSTACK_CONFIGURATION_NOVA_COMPUTE_HOSTNAMES[$CELL]=$(oc run mariadb-client-4-$CELL ${MARIADB_RUN_OVERRIDES} -q --image ${MARIADB_IMAGE} -i --rm --restart=Never -- \
&gt;         mysql -rsh "${SOURCE_MARIADB_IP[$CELL]}" -uroot -p"${SOURCE_DB_ROOT_PASSWORD[$CELL]}" -e \
&gt;             "select host from nova.services where services.binary='nova-compute' and deleted=0;")
&gt; done</pre>
</div>
</div>
</li>
<li>
<p>Get the list of the mapped Compute service cells:</p>
<div class="listingblock">
<div class="content">
<pre>export PULL_OPENSTACK_CONFIGURATION_NOVAMANAGE_CELL_MAPPINGS=$($CONTROLLER1_SSH sudo podman exec -it nova_conductor nova-manage cell_v2 list_cells)</pre>
</div>
</div>
</li>
<li>
<p>Store the exported variables for future use:</p>
<div class="listingblock">
<div class="content">
<pre>$ unset SRIOV_AGENTS
$ declare -xA SRIOV_AGENTS <i class="conum" data-value="1"></i><b>(1)</b>
$ for CELL in $(echo $CELLS); do
&gt;     RCELL=$CELL
&gt;     [ "$CELL" = "$DEFAULT_CELL_NAME" ] &amp;&amp; RCELL=default
&gt;     cat &gt; ~/.source_cloud_exported_variables_$CELL &lt;&lt; EOF
&gt; unset PULL_OPENSTACK_CONFIGURATION_DATABASES
&gt; unset PULL_OPENSTACK_CONFIGURATION_MYSQLCHECK_NOK
&gt; unset PULL_OPENSTACK_CONFIGURATION_NOVA_COMPUTE_HOSTNAMES
&gt; declare -xA PULL_OPENSTACK_CONFIGURATION_DATABASES
&gt; declare -xA PULL_OPENSTACK_CONFIGURATION_MYSQLCHECK_NOK
&gt; declare -xA PULL_OPENSTACK_CONFIGURATION_NOVA_COMPUTE_HOSTNAMES
&gt; PULL_OPENSTACK_CONFIGURATION_DATABASES[$CELL]="$(oc run mariadb-client-5-$CELL ${MARIADB_RUN_OVERRIDES} -q --image ${MARIADB_IMAGE} -i --rm --restart=Never -- \
&gt;     mysql -rsh ${SOURCE_MARIADB_IP[$RCELL]} -uroot -p${SOURCE_DB_ROOT_PASSWORD[$RCELL]} -e 'SHOW databases;')"
&gt; PULL_OPENSTACK_CONFIGURATION_MYSQLCHECK_NOK[$CELL]="$(oc run mariadb-client-6-$CELL ${MARIADB_RUN_OVERRIDES} -q --image ${MARIADB_IMAGE} -i --rm --restart=Never -- \
&gt;     mysqlcheck --all-databases -h ${SOURCE_MARIADB_IP[$RCELL]} -u root -p${SOURCE_DB_ROOT_PASSWORD[$RCELL]} | grep -v OK)"
&gt; PULL_OPENSTACK_CONFIGURATION_NOVA_COMPUTE_HOSTNAMES[$CELL]="$(oc run mariadb-client-7-$CELL ${MARIADB_RUN_OVERRIDES} -q --image ${MARIADB_IMAGE} -i --rm --restart=Never -- \
&gt;     mysql -rsh ${SOURCE_MARIADB_IP[$RCELL]} -uroot -p${SOURCE_DB_ROOT_PASSWORD[$RCELL]} -e \
&gt;     "select host from nova.services where services.binary='nova-compute' and deleted=0;")"
&gt; if [ "$RCELL" = "default" ]; then
&gt;     PULL_OPENSTACK_CONFIGURATION_NOVADB_MAPPED_CELLS="$(oc run mariadb-client-2 ${MARIADB_RUN_OVERRIDES} -q --image ${MARIADB_IMAGE} -i --rm --restart=Never -- \
&gt;         mysql -rsh ${SOURCE_MARIADB_IP[$RCELL]} -uroot -p${SOURCE_DB_ROOT_PASSWORD[$RCELL]} nova_api -e \
&gt;             'select uuid,name,transport_url,database_connection,disabled from cell_mappings;')"
&gt;     PULL_OPENSTACK_CONFIGURATION_NOVAMANAGE_CELL_MAPPINGS="$($CONTROLLER1_SSH sudo podman exec -it nova_conductor nova-manage cell_v2 list_cells)"
&gt; fi
&gt; EOF
&gt; done
$ chmod 0600 ~/.source_cloud_exported_variables*</pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>If <code>neutron-sriov-nic-agent</code> agents are running in your RHOSP deployment, get the configuration to use for the data plane adoption.</td>
</tr>
</table>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<div class="title">Next steps</div>
<p>This configuration and the exported values are required later, during the data plane adoption post-checks. After the RHOSP control plane services are shut down, if any of the exported values are lost, re-running the <code>export</code> command fails because the control plane services are no longer running on the source cloud, and the data cannot be retrieved. To avoid data loss, preserve the exported values in an environment file before shutting down the control plane services.</p>
</div>
</div>
<div class="sect2">
<h3 id="deploying-backend-services_migrating-databases"><a class="anchor" href="#deploying-backend-services_migrating-databases"></a>Deploying back-end services</h3>
<div class="paragraph _abstract">
<p>Create the <code>OpenStackControlPlane</code> custom resource (CR) with the basic back-end services deployed, and disable all the Red&#160;Hat OpenStack Platform (RHOSP) services. This CR is the foundation of the control plane.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>The cloud that you want to adopt is running, and it is on the latest minor version of RHOSP 17.1.</p>
</li>
<li>
<p>All control plane and data plane hosts of the source cloud are running, and continue to run throughout the adoption procedure.</p>
</li>
<li>
<p>The <code>openstack-operator</code> is deployed, but <code>OpenStackControlPlane</code> is
not deployed.</p>
</li>
<li>
<p>Install the OpenStack Operators. For more information, see <a href="https://docs.redhat.com/en/documentation/red_hat_openstack_services_on_openshift/18.0/html-single/deploying_red_hat_openstack_services_on_openshift/index#assembly_installing-and-preparing-the-Operators">Installing and preparing the Operators</a> in <em>Deploying Red Hat OpenStack Services on OpenShift</em>.</p>
</li>
<li>
<p>If you enabled TLS everywhere (TLS-e) on the RHOSP environment, you must copy the <code>tls</code> root CA from the RHOSP environment to the <code>rootca-internal</code> issuer.</p>
</li>
<li>
<p>There are free PVs available for Galera and RabbitMQ.</p>
</li>
<li>
<p>Set the desired admin password for the control plane deployment. This can
be the admin password from your original deployment or a different password:</p>
<div class="listingblock">
<div class="content">
<pre>ADMIN_PASSWORD=SomePassword</pre>
</div>
</div>
<div class="paragraph">
<p>To use the existing RHOSP deployment password:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>declare -A TRIPLEO_PASSWORDS
TRIPLEO_PASSWORDS[default]="$HOME/overcloud-passwords.yaml"
ADMIN_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' AdminPassword:' | awk -F ': ' '{ print $2; }')</pre>
</div>
</div>
</li>
<li>
<p>Set the service password variables to match the original deployment.
Database passwords can differ in the control plane environment, but
you must synchronize the service account passwords.</p>
<div class="paragraph">
<p>For example, in developer environments with director Standalone, the passwords can be extracted:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>AODH_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' AodhPassword:' | awk -F ': ' '{ print $2; }')
BARBICAN_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' BarbicanPassword:' | awk -F ': ' '{ print $2; }')
CEILOMETER_METERING_SECRET=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' CeilometerMeteringSecret:' | awk -F ': ' '{ print $2; }')
CEILOMETER_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' CeilometerPassword:' | awk -F ': ' '{ print $2; }')
CINDER_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' CinderPassword:' | awk -F ': ' '{ print $2; }')
GLANCE_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' GlancePassword:' | awk -F ': ' '{ print $2; }')
HEAT_AUTH_ENCRYPTION_KEY=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' HeatAuthEncryptionKey:' | awk -F ': ' '{ print $2; }')
HEAT_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' HeatPassword:' | awk -F ': ' '{ print $2; }')
HEAT_STACK_DOMAIN_ADMIN_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' HeatStackDomainAdminPassword:' | awk -F ': ' '{ print $2; }')
IRONIC_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' IronicPassword:' | awk -F ': ' '{ print $2; }')
MANILA_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' ManilaPassword:' | awk -F ': ' '{ print $2; }')
NEUTRON_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' NeutronPassword:' | awk -F ': ' '{ print $2; }')
NOVA_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' NovaPassword:' | awk -F ': ' '{ print $2; }')
OCTAVIA_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' OctaviaPassword:' | awk -F ': ' '{ print $2; }')
PLACEMENT_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' PlacementPassword:' | awk -F ': ' '{ print $2; }')
SWIFT_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' SwiftPassword:' | awk -F ': ' '{ print $2; }')</pre>
</div>
</div>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Ensure that you are using the Red Hat OpenShift Container Platform (RHOCP) namespace where you want the
control plane to be deployed:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc project openstack</pre>
</div>
</div>
</li>
<li>
<p>Create the RHOSP secret. For more information, see <a href="https://docs.redhat.com/en/documentation/red_hat_openstack_services_on_openshift/18.0/html-single/deploying_red_hat_openstack_services_on_openshift/index#proc_providing-secure-access-to-the-RHOSO-services_preparing">Providing secure access to the Red Hat OpenStack Services on OpenShift services</a> in <em>Deploying Red Hat OpenStack Services on OpenShift</em>.</p>
</li>
<li>
<p>If the <code>$ADMIN_PASSWORD</code> is different than the password you set
in <code>osp-secret</code>, amend the <code>AdminPassword</code> key in the <code>osp-secret</code>:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc set data secret/osp-secret "AdminPassword=$ADMIN_PASSWORD"</pre>
</div>
</div>
</li>
<li>
<p>Set service account passwords in <code>osp-secret</code> to match the service
account passwords from the original deployment:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc set data secret/osp-secret "AodhPassword=$AODH_PASSWORD"
$ oc set data secret/osp-secret "BarbicanPassword=$BARBICAN_PASSWORD"
$ oc set data secret/osp-secret "CeilometerPassword=$CEILOMETER_PASSWORD"
$ oc set data secret/osp-secret "CinderPassword=$CINDER_PASSWORD"
$ oc set data secret/osp-secret "GlancePassword=$GLANCE_PASSWORD"
$ oc set data secret/osp-secret "HeatAuthEncryptionKey=$HEAT_AUTH_ENCRYPTION_KEY"
$ oc set data secret/osp-secret "HeatPassword=$HEAT_PASSWORD"
$ oc set data secret/osp-secret "HeatStackDomainAdminPassword=$HEAT_STACK_DOMAIN_ADMIN_PASSWORD"
$ oc set data secret/osp-secret "IronicPassword=$IRONIC_PASSWORD"
$ oc set data secret/osp-secret "IronicInspectorPassword=$IRONIC_PASSWORD"
$ oc set data secret/osp-secret "ManilaPassword=$MANILA_PASSWORD"
$ oc set data secret/osp-secret "MetadataSecret=$METADATA_SECRET"
$ oc set data secret/osp-secret "NeutronPassword=$NEUTRON_PASSWORD"
$ oc set data secret/osp-secret "NovaPassword=$NOVA_PASSWORD"
$ oc set data secret/osp-secret "OctaviaPassword=$OCTAVIA_PASSWORD"
$ oc set data secret/osp-secret "PlacementPassword=$PLACEMENT_PASSWORD"
$ oc set data secret/osp-secret "SwiftPassword=$SWIFT_PASSWORD"</pre>
</div>
</div>
</li>
<li>
<p>Deploy the <code>OpenStackControlPlane</code> CR. Ensure that you only enable the DNS, Galera, Memcached, and RabbitMQ services. All other services must
be disabled:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">$ oc apply -f - &lt;&lt;EOF
apiVersion: core.openstack.org/v1beta1
kind: OpenStackControlPlane
metadata:
  name: openstack
spec:
  secret: osp-secret
  storageClass: &lt;storage_class&gt; <i class="conum" data-value="1"></i><b>(1)</b>

  barbican:
    enabled: false
    template:
      barbicanAPI: {}
      barbicanWorker: {}
      barbicanKeystoneListener: {}

  cinder:
    enabled: false
    template:
      cinderAPI: {}
      cinderScheduler: {}
      cinderBackup: {}
      cinderVolumes: {}

  dns:
    template:
      override:
        service:
          metadata:
            annotations:
              metallb.universe.tf/address-pool: ctlplane
              metallb.universe.tf/allow-shared-ip: ctlplane
              metallb.universe.tf/loadBalancerIPs: 192.168.122.80 <i class="conum" data-value="2"></i><b>(2)</b>

          spec:
            type: LoadBalancer
      options:
      - key: server
        values:
        - 192.168.122.1
      replicas: 1

  glance:
    enabled: false
    template:
      glanceAPIs: {}

  heat:
    enabled: false
    template: {}

  horizon:
    enabled: false
    template: {}

  ironic:
    enabled: false
    template:
      ironicConductors: []

  keystone:
    enabled: false
    template: {}

  manila:
    enabled: false
    template:
      manilaAPI: {}
      manilaScheduler: {}
      manilaShares: {}

  galera:
    enabled: true
    templates:
      openstack:
        secret: osp-secret
        replicas: 3
        storageRequest: 5G
      openstack-cell1: <i class="conum" data-value="3"></i><b>(3)</b>
        secret: osp-secret
        replicas: 3
        storageRequest: 5G
      openstack-cell2:
        secret: osp-secret
        replicas: 1
        storageRequest: 5G
      openstack-cell3:
        secret: osp-secret
        replicas: 1
        storageRequest: 5G
  memcached:
    enabled: true
    templates:
      memcached:
        replicas: 3

  neutron:
    enabled: false
    template: {}

  nova:
    enabled: false
    template: {}

  ovn:
    enabled: false
    template:
      ovnController:
        networkAttachment: tenant
        nodeSelector:
          node: non-existing-node-name
      ovnNorthd:
        replicas: 0
      ovnDBCluster:
        ovndbcluster-nb:
          replicas: 3
          dbType: NB
          networkAttachment: internalapi
        ovndbcluster-sb:
          replicas: 3
          dbType: SB
          networkAttachment: internalapi

  placement:
    enabled: false
    template: {}

  rabbitmq:
    templates:
      rabbitmq:
        override:
          service:
            metadata:
              annotations:
                metallb.universe.tf/address-pool: internalapi
                metallb.universe.tf/loadBalancerIPs: 172.17.0.85
            spec:
              type: LoadBalancer
      rabbitmq-cell1:
        persistence:
          storage: 1G
        override:
          service:
            metadata:
              annotations:
                metallb.universe.tf/address-pool: internalapi
                metallb.universe.tf/loadBalancerIPs: 172.17.0.86

            spec:
              type: LoadBalancer
      rabbitmq-cell2:
        persistence:
          storage: 1G
        override:
          service:
            metadata:
              annotations:
                metallb.universe.tf/address-pool: internalapi
                metallb.universe.tf/loadBalancerIPs: 172.17.0.87
            spec:
              type: LoadBalancer
      rabbitmq-cell3:
        persistence:
          storage: 1G
        override:
          service:
            metadata:
              annotations:
                metallb.universe.tf/address-pool: internalapi
                metallb.universe.tf/loadBalancerIPs: 172.17.0.88
            spec:
              type: LoadBalancer
  telemetry:
    enabled: false
  tls: <i class="conum" data-value="4"></i><b>(4)</b>
    podLevel:
      enabled: false
    ingress:
      enabled: false
  swift:
    enabled: false
    template:
      swiftRing:
        ringReplicas: 1
      swiftStorage:
        replicas: 0
      swiftProxy:
        replicas: 1
EOF</code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Select an existing <code>&lt;storage_class&gt;</code> in your RHOCP cluster.</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>Replace <code>&lt;loadBalancer_IP&gt;</code> with the LoadBalancer IP address.</td>
</tr>
<tr>
<td><i class="conum" data-value="3"></i><b>3</b></td>
<td>This example provides the required infrastructure database and messaging services for 3 Compute cells named <code>cell1</code>, <code>cell2</code>, and <code>cell3</code>. Adjust the values for fields such as <code>replicas</code>, <code>storage</code>, or <code>storageRequest</code>, for each Compute cell as needed.</td>
</tr>
<tr>
<td><i class="conum" data-value="4"></i><b>4</b></td>
<td>If you enabled TLS-e in your RHOSP environment, in the <code>spec:tls</code> section set <code>tls</code> to the following:</td>
</tr>
</table>
</div>
<div class="listingblock">
<div class="content">
<pre>spec:
  ...
  tls:
    podLevel:
      enabled: true
      internal:
        ca:
          customIssuer: rootca-internal
      libvirt:
        ca:
          customIssuer: rootca-internal
      ovn:
        ca:
          customIssuer: rootca-internal
    ingress:
      ca:
        customIssuer: rootca-internal
      enabled: true</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>If you use IPv6, change the load balancer IPs to the IPs in your environment, for example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>...
metallb.universe.tf/allow-shared-ip: ctlplane
metallb.universe.tf/loadBalancerIPs: fd00:aaaa::80
...
metallb.universe.tf/address-pool: internalapi
metallb.universe.tf/loadBalancerIPs: fd00:bbbb::85
...
metallb.universe.tf/address-pool: internalapi
metallb.universe.tf/loadBalancerIPs: fd00:bbbb::86</pre>
</div>
</div>
</td>
</tr>
</table>
</div>
</li>
</ol>
</div>
<div class="ulist">
<div class="title">Verification</div>
<ul>
<li>
<p>Verify that the Galera and RabbitMQ status is <code>Running</code> for all defined cells:</p>
<div class="listingblock">
<div class="content">
<pre>$ RENAMED_CELLS="cell1 cell2 cell3"
$ oc get pod openstack-galera-0 -o jsonpath='{.status.phase}{"\n"}'
$ oc get pod rabbitmq-server-0 -o jsonpath='{.status.phase}{"\n"}'
$ for CELL in $(echo $RENAMED_CELLS); do
&gt;     oc get pod openstack-$CELL-galera-0 -o jsonpath='{.status.phase}{"\n"}'
&gt;     oc get pod rabbitmq-$CELL-server-0 -o jsonpath='{.status.phase}{"\n"}'
&gt; done</pre>
</div>
</div>
<div class="paragraph">
<p>The given cells names are later referred to by using the environment variable <code>RENAMED_CELLS</code>.
During the database migration procedure, the Nova cells are renamed. <code>RENAMED_CELLS</code> variable represents the new cell names used in the RHOSO deployment.</p>
</div>
</li>
<li>
<p>Ensure that the statuses of all the Rabbitmq and Galera CRs are <code>Setup complete</code>:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc get Rabbitmqs,Galera
NAME                                                                  STATUS   MESSAGE
rabbitmq.rabbitmq.openstack.org/rabbitmq                              True     Setup complete
rabbitmq.rabbitmq.openstack.org/rabbitmq-cell1                        True     Setup complete

NAME                                                               READY   MESSAGE
galera.mariadb.openstack.org/openstack                             True     Setup complete
galera.mariadb.openstack.org/openstack-cell1                       True     Setup complete</pre>
</div>
</div>
</li>
<li>
<p>Verify that the <code>OpenStackControlPlane</code> CR is waiting for deployment
of the <code>openstackclient</code> pod:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc get OpenStackControlPlane openstack
NAME        STATUS    MESSAGE
openstack   Unknown   OpenStackControlPlane Client not started</pre>
</div>
</div>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="configuring-a-ceph-backend_migrating-databases"><a class="anchor" href="#configuring-a-ceph-backend_migrating-databases"></a>Configuring a Red Hat Ceph Storage back end</h3>
<div class="paragraph _abstract">
<p>If your Red&#160;Hat OpenStack Platform (RHOSP) 17.1 deployment uses a Red Hat Ceph Storage back end for any service, such as Image Service (glance), Block Storage service (cinder), Compute service (nova), or Shared File Systems service (manila), you must configure the custom resources (CRs) to use the same back end in the Red&#160;Hat OpenStack Services on OpenShift (RHOSO) 18.0 deployment.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
To run <code>ceph</code> commands, you must use SSH to connect to a Red Hat Ceph Storage node and run <code>sudo cephadm shell</code>. This generates a Ceph orchestrator container that enables you to run administrative commands against the Red Hat Ceph Storage cluster. If you deployed the Red Hat Ceph Storage cluster by using director, you can launch the <code>cephadm</code> shell from an RHOSP Controller node.
</td>
</tr>
</table>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>The <code>OpenStackControlPlane</code> CR is created.</p>
</li>
<li>
<p>If your RHOSP 17.1 deployment uses the Shared File Systems service, the openstack keyring is updated. Modify the <code>openstack</code> user so that you can use it across all RHOSP services:</p>
<div class="listingblock">
<div class="content">
<pre>ceph auth caps client.openstack \
  mgr 'allow *' \
  mon 'allow r, profile rbd' \
  osd 'profile rbd pool=vms, profile rbd pool=volumes, profile rbd pool=images, allow rw pool manila_data'</pre>
</div>
</div>
<div class="paragraph">
<p>Using the same user across all services makes it simpler to create a common Red Hat Ceph Storage secret that includes the keyring and <code>ceph.conf</code> file and propagate the secret to all the services that need it.</p>
</div>
</li>
<li>
<p>The following shell variables are defined. Replace the following example values with values that are correct for your environment:</p>
<div class="listingblock">
<div class="content">
<pre>CEPH_SSH="ssh -i <strong>&lt;path to SSH key&gt;</strong> root@<strong>&lt;node IP&gt;</strong>"
CEPH_KEY=$($CEPH_SSH "cat /etc/ceph/ceph.client.openstack.keyring | base64 -w 0")
CEPH_CONF=$($CEPH_SSH "cat /etc/ceph/ceph.conf | base64 -w 0")</pre>
</div>
</div>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Create the <code>ceph-conf-files</code> secret that includes the Red Hat Ceph Storage configuration:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc apply -f - &lt;&lt;EOF
apiVersion: v1
data:
  ceph.client.openstack.keyring: $CEPH_KEY
  ceph.conf: $CEPH_CONF
kind: Secret
metadata:
  name: ceph-conf-files
type: Opaque
EOF</pre>
</div>
</div>
<div class="paragraph">
<p>The content of the file should be similar to the following example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: v1
kind: Secret
metadata:
  name: ceph-conf-files
stringData:
  ceph.client.openstack.keyring: |
    [client.openstack]
        key = &lt;secret key&gt;
        caps mgr = "allow *"
        caps mon = "allow r, profile rbd"
        caps osd = "pool=vms, profile rbd pool=volumes, profile rbd pool=images, allow rw pool manila_data'
  ceph.conf: |
    [global]
    fsid = 7a1719e8-9c59-49e2-ae2b-d7eb08c695d4
    mon_host = 10.1.1.2,10.1.1.3,10.1.1.4 <i class="conum" data-value="1"></i><b>(1)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>If you use IPv6, use brackets for the <code>mon_host</code>. For example:
<code>mon_host = [v2:[fd00:cccc::100]:3300/0,v1:[fd00:cccc::100]:6789/0]</code></td>
</tr>
</table>
</div>
</li>
<li>
<p>In your <code>OpenStackControlPlane</code> CR, inject <code>ceph.conf</code> and <code>ceph.client.openstack.keyring</code> to the RHOSP services that are defined in the propagation list. For example:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">$ oc patch openstackcontrolplane openstack --type=merge --patch '
spec:
  extraMounts:
    - name: v1
      region: r1
      extraVol:
        - propagation:
          - CinderVolume
          - CinderBackup
          - GlanceAPI
          - ManilaShare
          extraVolType: Ceph
          volumes:
          - name: ceph
            projected:
              sources:
              - secret:
                  name: ceph-conf-files
          mounts:
          - name: ceph
            mountPath: "/etc/ceph"
            readOnly: true
'</code></pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="stopping-openstack-services_migrating-databases"><a class="anchor" href="#stopping-openstack-services_migrating-databases"></a>Stopping Red&#160;Hat OpenStack Platform services</h3>
<div class="paragraph _abstract">
<p>Before you start the Red&#160;Hat OpenStack Services on OpenShift (RHOSO) adoption, you must stop the Red&#160;Hat OpenStack Platform (RHOSP) services to avoid inconsistencies in the data that you migrate for the data plane adoption. Inconsistencies are caused by resource changes after the database is copied to the new deployment.</p>
</div>
<div class="paragraph">
<p>You should not stop the infrastructure management services yet, such as:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Database</p>
</li>
<li>
<p>RabbitMQ</p>
</li>
<li>
<p>HAProxy Load Balancer</p>
</li>
<li>
<p>Ceph-nfs</p>
</li>
<li>
<p>Compute service</p>
</li>
<li>
<p>Containerized modular libvirt daemons</p>
</li>
<li>
<p>Object Storage service (swift) back-end services</p>
</li>
</ul>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>Ensure that there no long-running tasks that require the services that you plan to stop, such as instance live migrations, volume migrations, volume creation, backup and restore, attaching, detaching, and other similar operations:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack server list --all-projects -c ID -c Status |grep -E '\| .+ing \|'
$ openstack volume list --all-projects -c ID -c Status |grep -E '\| .+ing \|'| grep -vi error
$ openstack volume backup list --all-projects -c ID -c Status |grep -E '\| .+ing \|' | grep -vi error
$ openstack share list --all-projects -c ID -c Status |grep -E '\| .+ing \|'| grep -vi error
$ openstack image list -c ID -c Status |grep -E '\| .+ing \|'</pre>
</div>
</div>
</li>
<li>
<p>Collect the services topology-specific configuration. For more information, see <a href="#proc_retrieving-topology-specific-service-configuration_migrating-databases">Retrieving topology-specific service configuration</a>.</p>
</li>
<li>
<p>Define the following shell variables. The values are examples and refer to a single node standalone director deployment. Replace these example values with values that are correct for your environment:</p>
<div class="listingblock">
<div class="content">
<pre>CONTROLLER1_SSH="ssh -i <strong>&lt;path to SSH key&gt;</strong> root@<strong>&lt;controller-1 IP&gt;</strong>" <i class="conum" data-value="1"></i><b>(1)</b>
CONTROLLER2_SSH="ssh -i <strong>&lt;path to SSH key&gt;</strong> root@<strong>&lt;controller-2 IP&gt;</strong>"
CONTROLLER3_SSH="ssh -i <strong>&lt;path to SSH key&gt;</strong> root@<strong>&lt;controller-3 IP&gt;</strong>"</pre>
</div>
</div>
</li>
<li>
<p>Specify the IP addresses of all Controller nodes, for example:</p>
<div class="listingblock">
<div class="content">
<pre>CONTROLLER1_SSH="ssh -i <strong>&lt;path to SSH key&gt;</strong> root@<strong>&lt;controller-1 IP&gt;</strong>" <i class="conum" data-value="2"></i><b>(2)</b>
CONTROLLER2_SSH="ssh -i <strong>&lt;path to SSH key&gt;</strong> root@<strong>&lt;controller-2 IP&gt;</strong>"
CONTROLLER3_SSH="ssh -i <strong>&lt;path to SSH key&gt;</strong> root@<strong>&lt;controller-3 IP&gt;</strong>"
# ...</pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Replace <code>&lt;path_to_SSH_key&gt;</code> with the path to your SSH key.</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>Replace <code>&lt;controller-&lt;X&gt; IP&gt;</code> with IP addresses of all Controller nodes.</td>
</tr>
</table>
</div>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>If your deployment enables CephFS through NFS as a back end for Shared File Systems service (manila), remove the following Pacemaker ordering and co-location constraints that govern the Virtual IP address of the <code>ceph-nfs</code> service and the <code>manila-share</code> service:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml"># check the co-location and ordering constraints concerning "manila-share"
sudo pcs constraint list --full

# remove these constraints
sudo pcs constraint remove colocation-openstack-manila-share-ceph-nfs-INFINITY
sudo pcs constraint remove order-ceph-nfs-openstack-manila-share-Optional</code></pre>
</div>
</div>
</li>
<li>
<p>Disable RHOSP control plane services:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml"># Update the services list to be stopped
ServicesToStop=("tripleo_aodh_api.service"
                "tripleo_aodh_api_cron.service"
                "tripleo_aodh_evaluator.service"
                "tripleo_aodh_listener.service"
                "tripleo_aodh_notifier.service"
                "tripleo_ceilometer_agent_central.service"
                "tripleo_ceilometer_agent_notification.service"
                "tripleo_octavia_api.service"
                "tripleo_octavia_health_manager.service"
                "tripleo_octavia_rsyslog.service"
                "tripleo_octavia_driver_agent.service"
                "tripleo_octavia_housekeeping.service"
                "tripleo_octavia_worker.service"
                "tripleo_horizon.service"
                "tripleo_keystone.service"
                "tripleo_barbican_api.service"
                "tripleo_barbican_worker.service"
                "tripleo_barbican_keystone_listener.service"
                "tripleo_cinder_api.service"
                "tripleo_cinder_api_cron.service"
                "tripleo_cinder_scheduler.service"
                "tripleo_cinder_volume.service"
                "tripleo_cinder_backup.service"
                "tripleo_collectd.service"
                "tripleo_glance_api.service"
                "tripleo_gnocchi_api.service"
                "tripleo_gnocchi_metricd.service"
                "tripleo_gnocchi_statsd.service"
                "tripleo_manila_api.service"
                "tripleo_manila_api_cron.service"
                "tripleo_manila_scheduler.service"
                "tripleo_neutron_api.service"
                "tripleo_placement_api.service"
                "tripleo_nova_api_cron.service"
                "tripleo_nova_api.service"
                "tripleo_nova_conductor.service"
                "tripleo_nova_metadata.service"
                "tripleo_nova_scheduler.service"
                "tripleo_nova_vnc_proxy.service"
                "tripleo_aodh_api.service"
                "tripleo_aodh_api_cron.service"
                "tripleo_aodh_evaluator.service"
                "tripleo_aodh_listener.service"
                "tripleo_aodh_notifier.service"
                "tripleo_ceilometer_agent_central.service"
                "tripleo_ceilometer_agent_compute.service"
                "tripleo_ceilometer_agent_ipmi.service"
                "tripleo_ceilometer_agent_notification.service"
                "tripleo_ovn_cluster_northd.service"
                "tripleo_ironic_neutron_agent.service"
                "tripleo_ironic_api.service"
                "tripleo_ironic_inspector.service"
                "tripleo_ironic_conductor.service"
                "tripleo_ironic_inspector_dnsmasq.service"
                "tripleo_ironic_pxe_http.service"
                "tripleo_ironic_pxe_tftp.service")

PacemakerResourcesToStop=("openstack-cinder-volume"
                          "openstack-cinder-backup"
                          "openstack-manila-share")

echo "Stopping systemd OpenStack services"
for service in ${ServicesToStop[*]}; do
    for i in {1..3}; do
        SSH_CMD=CONTROLLER${i}_SSH
        if [ ! -z "${!SSH_CMD}" ]; then
            echo "Stopping the $service in controller $i"
            if ${!SSH_CMD} sudo systemctl is-active $service; then
                ${!SSH_CMD} sudo systemctl stop $service
            fi
        fi
    done
done

echo "Checking systemd OpenStack services"
for service in ${ServicesToStop[*]}; do
    for i in {1..3}; do
        SSH_CMD=CONTROLLER${i}_SSH
        if [ ! -z "${!SSH_CMD}" ]; then
            if ! ${!SSH_CMD} systemctl show $service | grep ActiveState=inactive &gt;/dev/null; then
                echo "ERROR: Service $service still running on controller $i"
            else
                echo "OK: Service $service is not running on controller $i"
            fi
        fi
    done
done

echo "Stopping pacemaker OpenStack services"
for i in {1..3}; do
    SSH_CMD=CONTROLLER${i}_SSH
    if [ ! -z "${!SSH_CMD}" ]; then
        echo "Using controller $i to run pacemaker commands"
        for resource in ${PacemakerResourcesToStop[*]}; do
            if ${!SSH_CMD} sudo pcs resource config $resource &amp;&gt;/dev/null; then
                echo "Stopping $resource"
                ${!SSH_CMD} sudo pcs resource disable $resource
            else
                echo "Service $resource not present"
            fi
    done
        break
    fi
done

echo "Checking pacemaker OpenStack services"
for i in {1..3}; do
    SSH_CMD=CONTROLLER${i}_SSH
    if [ ! -z "${!SSH_CMD}" ]; then
        echo "Using controller $i to run pacemaker commands"
        for resource in ${PacemakerResourcesToStop[*]}; do
            if ${!SSH_CMD} sudo pcs resource config $resource &amp;&gt;/dev/null; then
                if ! ${!SSH_CMD} sudo pcs resource status $resource | grep Started; then
                    echo "OK: Service $resource is stopped"
                else
                    echo "ERROR: Service $resource is started"
                fi
            fi
        done
        break
    fi
    done</code></pre>
</div>
</div>
<div class="paragraph">
<p>If the status of each service is <code>OK</code>, then the services stopped successfully.</p>
</div>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="migrating-databases-to-mariadb-instances_migrating-databases"><a class="anchor" href="#migrating-databases-to-mariadb-instances_migrating-databases"></a>Migrating databases to MariaDB instances</h3>
<div class="paragraph _abstract">
<p>Migrate your databases from the original Red&#160;Hat OpenStack Platform (RHOSP) deployment to the MariaDB instances in the Red Hat OpenShift Container Platform (RHOCP) cluster.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>Ensure that the control plane MariaDB and RabbitMQ are running, and that no other control plane services are running.</p>
</li>
<li>
<p>Retrieve the topology-specific service configuration. For more information, see <a href="#proc_retrieving-topology-specific-service-configuration_migrating-databases">Retrieving topology-specific service configuration</a>.</p>
</li>
<li>
<p>Stop the RHOSP services. For more information, see <a href="#stopping-openstack-services_migrating-databases">Stopping Red&#160;Hat OpenStack Platform services</a>.</p>
</li>
<li>
<p>Ensure that there is network routability between the original MariaDB and the MariaDB for the control plane.</p>
</li>
<li>
<p>Define the following shell variables. Replace the following example values with values that are correct for your environment:</p>
<div class="listingblock">
<div class="content">
<pre>$ STORAGE_CLASS=local-storage
$ MARIADB_IMAGE=registry.redhat.io/rhoso/openstack-mariadb-rhel9:18.0


$ CELLS="default cell1 cell2" <i class="conum" data-value="1"></i><b>(1)</b>
$ DEFAULT_CELL_NAME="cell3"
$ RENAMED_CELLS="cell1 cell2 $DEFAULT_CELL_NAME"

$ CHARACTER_SET=utf8 <i class="conum" data-value="2"></i><b>(2)</b>
$ COLLATION=utf8_general_ci

$ declare -A PODIFIED_DB_ROOT_PASSWORD
$ for CELL in $(echo "super $RENAMED_CELLS"); do
&gt;   PODIFIED_DB_ROOT_PASSWORD[$CELL]=$(oc get -o json secret/osp-secret | jq -r .data.DbRootPassword | base64 -d)
&gt; done

$ declare -A PODIFIED_MARIADB_IP
$ for CELL in $(echo "super $RENAMED_CELLS"); do
&gt;   if [ "$CELL" = "super" ]; then
&gt;     PODIFIED_MARIADB_IP[$CELL]=$(oc get svc --selector "mariadb/name=openstack" -ojsonpath='{.items[0].spec.clusterIP}')
&gt;   else
&gt;     PODIFIED_MARIADB_IP[$CELL]=$(oc get svc --selector "mariadb/name=openstack-$CELL" -ojsonpath='{.items[0].spec.clusterIP}')
&gt;   fi
&gt; done

$ declare -A TRIPLEO_PASSWORDS
$ for CELL in $(echo $CELLS); do
&gt;   if [ "$CELL" = "default" ]; then
&gt;     TRIPLEO_PASSWORDS[default]="$HOME/overcloud-passwords.yaml"
&gt;   else
&gt;     # in a split-stack source cloud, it should take a stack-specific passwords file instead
&gt;     TRIPLEO_PASSWORDS[$CELL]="$HOME/overcloud-passwords.yaml"
&gt;   fi
&gt; done

$ declare -A SOURCE_DB_ROOT_PASSWORD
$ for CELL in $(echo $CELLS); do
&gt;   SOURCE_DB_ROOT_PASSWORD[$CELL]=$(cat ${TRIPLEO_PASSWORDS[$CELL]} | grep ' MysqlRootPassword:' | awk -F ': ' '{ print $2; }')
&gt; done

$ declare -A SOURCE_MARIADB_IP
$ SOURCE_MARIADB_IP[default]=*&lt;galera cluster VIP&gt;* <i class="conum" data-value="3"></i><b>(3)</b>
$ SOURCE_MARIADB_IP[cell1]=*&lt;galera cell1 cluster VIP&gt;* <i class="conum" data-value="4"></i><b>(4)</b>
$ SOURCE_MARIADB_IP[cell2]=*&lt;galera cell2 cluster VIP&gt;* <i class="conum" data-value="5"></i><b>(5)</b>
# ...

$ declare -A SOURCE_GALERA_MEMBERS_DEFAULT
$ SOURCE_GALERA_MEMBERS_DEFAULT=(
&gt;   ["standalone.localdomain"]=172.17.0.100 <i class="conum" data-value="6"></i><b>(6)</b>
&gt;   # [...]=...
&gt; )
$ declare -A SOURCE_GALERA_MEMBERS_CELL1
$ SOURCE_GALERA_MEMBERS_CELL1=(
&gt;   # ...
&gt; )
$ declare -A SOURCE_GALERA_MEMBERS_CELL2
$ SOURCE_GALERA_MEMBERS_CELL2=(
&gt;   # ...
&gt; )</pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td><code>CELLS</code> and <code>RENAMED_CELLS</code> represent changes that are going to be made after you import the databases. The <code>default</code> cell takes a new name from <code>DEFAULT_CELL_NAME</code>.
In a multi-cell adoption scenario, <code>default</code> cell might retain its original 'default' name as well.</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>The <code>CHARACTER_SET</code> variable and collation should match the source database. If they do not match, then foreign key relationships break for any tables that are created in the future as part of database sync.</td>
</tr>
<tr>
<td><i class="conum" data-value="3"></i><b>3</b></td>
<td>Add data in  <code>SOURCE_MARIADB_IP[*]= &#8230;&#8203;</code> for each cell that is defined in <code>CELLS</code>. Provide records for the cell names and VIP addresses of MariaDB Galera clusters.</td>
</tr>
<tr>
<td><i class="conum" data-value="4"></i><b>4</b></td>
<td>Replace <code>&lt;galera_cell1_cluster_VIP&gt;</code> with the VIP of your galera cell1 cluster.</td>
</tr>
<tr>
<td><i class="conum" data-value="5"></i><b>5</b></td>
<td>Replace <code>&lt;galera_cell2_cluster_VIP&gt;</code> with the VIP of your galera cell2 cluster, and so on.</td>
</tr>
<tr>
<td><i class="conum" data-value="6"></i><b>6</b></td>
<td>For each cell defined in <code>CELLS</code>, in <code>SOURCE_GALERA_MEMBERS_CELL&lt;X&gt;</code>, add the names of the MariaDB Galera cluster members and its IP address. Replace <code>["standalone.localdomain"]="172.17.0.100"</code> with the real hosts data.</td>
</tr>
</table>
</div>
</li>
</ul>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
A standalone director environment only creates a 'default' cell, which should be the only <code>CELLS</code> value in this case. The <code>DEFAULT_CELL_NAME</code> value should be <code>cell1</code>.
</td>
</tr>
</table>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The <code>super</code> is the top-scope Nova API upcall database instance. A super conductor connects to that database. In subsequent examples, the upcall and cells databases use the same password that is defined in <code>osp-secret</code>. Old passwords are only needed to prepare the data exports.
</td>
</tr>
</table>
</div>
<div class="ulist">
<ul>
<li>
<p>To get the values for <code>SOURCE_MARIADB_IP</code>, query the puppet-generated configurations in the Controller and CellController nodes:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo grep -rI 'listen mysql' -A10 /var/lib/config-data/puppet-generated/ | grep bind</pre>
</div>
</div>
</li>
<li>
<p>To get the values for <code>SOURCE_GALERA_MEMBERS_*</code>, query the puppet-generated configurations in the Controller and CellController nodes:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo grep -rI 'listen mysql' -A10 /var/lib/config-data/puppet-generated/ | grep server</pre>
</div>
</div>
<div class="paragraph">
<p>The source cloud always uses the same password for cells databases. For that reason, the same passwords file is used for all cells stacks. However, split-stack topology allows using different passwords files for each stack.</p>
</div>
</li>
<li>
<p>Prepare the MariaDB adoption helper pod:</p>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Create a temporary volume claim and a pod for the database data copy. Edit the volume claim storage request if necessary, to give it enough space for the overcloud databases:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc apply -f - &lt;&lt;EOF
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mariadb-data
spec:
  storageClassName: $STORAGE_CLASS
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
---
apiVersion: v1
kind: Pod
metadata:
  name: mariadb-copy-data
  annotations:
    openshift.io/scc: anyuid
    k8s.v1.cni.cncf.io/networks: internalapi
  labels:
    app: adoption
spec:
  containers:
  - image: $MARIADB_IMAGE
    command: [ "sh", "-c", "sleep infinity"]
    name: adoption
    volumeMounts:
    - mountPath: /backup
      name: mariadb-data
  securityContext:
    allowPrivilegeEscalation: false
    capabilities:
      drop: ALL
    runAsNonRoot: true
    seccompProfile:
      type: RuntimeDefault
  volumes:
  - name: mariadb-data
    persistentVolumeClaim:
      claimName: mariadb-data
EOF</pre>
</div>
</div>
</li>
<li>
<p>Wait for the pod to be ready:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc wait --for condition=Ready pod/mariadb-copy-data --timeout=30s</pre>
</div>
</div>
</li>
</ol>
</div>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Check that the source Galera database clusters in each cell have its members online and synced:</p>
<div class="listingblock">
<div class="content">
<pre>$ for CELL in $(echo $CELLS); do
&gt;   MEMBERS=SOURCE_GALERA_MEMBERS_$(echo ${CELL}|tr '[:lower:]' '[:upper:]')[@]
&gt;   for i in "${!MEMBERS}"; do
&gt;     echo "Checking for the database node $i WSREP status Synced"
&gt;     oc rsh mariadb-copy-data mysql \
&gt;       -h "$i" -uroot -p"${SOURCE_DB_ROOT_PASSWORD[$CELL]}" \
&gt;       -e "show global status like 'wsrep_local_state_comment'" | \
&gt;       grep -qE "\bSynced\b"
&gt;   done
&gt; done</pre>
</div>
</div>
<div class="paragraph">
<p>Each additional Compute service (nova) v2 cell runs a dedicated Galera database cluster, so the command checks each cell.</p>
</div>
</li>
<li>
<p>Get the count of source databases with the <code>NOK</code> (not-OK) status:</p>
<div class="listingblock">
<div class="content">
<pre>$ for CELL in $(echo $CELLS); do
&gt;   oc rsh mariadb-copy-data mysql -h "${SOURCE_MARIADB_IP[$CELL]}" -uroot -p"${SOURCE_DB_ROOT_PASSWORD[$CELL]}" -e "SHOW databases;"
&gt; end</pre>
</div>
</div>
</li>
<li>
<p>Check that <code>mysqlcheck</code> had no errors:</p>
<div class="listingblock">
<div class="content">
<pre>$ for CELL in $(echo $CELLS); do
&gt;   set +u
&gt;   . ~/.source_cloud_exported_variables_$CELL
&gt;   set -u
&gt; done
$ test -z "$PULL_OPENSTACK_CONFIGURATION_MYSQLCHECK_NOK"  || [ "x$PULL_OPENSTACK_CONFIGURATION_MYSQLCHECK_NOK" = "x " ] &amp;&amp; echo "OK" || echo "CHECK FAILED"</pre>
</div>
</div>
</li>
<li>
<p>Test the connection to the control plane upcall and cells databases:</p>
<div class="listingblock">
<div class="content">
<pre>$ for CELL in $(echo "super $RENAMED_CELLS"); do
&gt;   oc run mariadb-client --image $MARIADB_IMAGE -i --rm --restart=Never -- \
&gt;     mysql -rsh "${PODIFIED_MARIADB_IP[$CELL]}" -uroot -p"${PODIFIED_DB_ROOT_PASSWORD[$CELL]}" -e 'SHOW databases;'
&gt; done</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
You must transition Compute services that you import later into a superconductor architecture by deleting the old service records in the cell databases, starting with <code>cell1</code>. New records are registered with different hostnames that are provided by the Compute service operator. All Compute services, except the Compute agent, have no internal state, and you can safely delete their service records. You also need to rename the former <code>default</code> cell to <code>DEFAULT_CELL_NAME</code>.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Create a dump of the original databases:</p>
<div class="listingblock">
<div class="content">
<pre>$ for CELL in $(echo $CELLS); do
&gt;   oc rsh mariadb-copy-data &lt;&lt; EOF
&gt;     mysql -h"${SOURCE_MARIADB_IP[$CELL]}" -uroot -p"${SOURCE_DB_ROOT_PASSWORD[$CELL]}" \
&gt;     -N -e "show databases" | grep -E -v "schema|mysql|gnocchi|aodh" | \
&gt;     while read dbname; do
&gt;       echo "Dumping $CELL cell \${dbname}";
&gt;       mysqldump -h"${SOURCE_MARIADB_IP[$CELL]}" -uroot -p"${SOURCE_DB_ROOT_PASSWORD[$CELL]}" \
&gt;         --single-transaction --complete-insert --skip-lock-tables --lock-tables=0 \
&gt;         "\${dbname}" &gt; /backup/"${CELL}.\${dbname}".sql;
&gt;     done
&gt; EOF
&gt; done</pre>
</div>
</div>
<div class="paragraph">
<p>Note filtering the information and performance schema tables.
Gnocchi is no longer used as a metric store as well</p>
</div>
</li>
<li>
<p>Restore the databases from <code>.sql</code> files into the control plane MariaDB:</p>
<div class="listingblock">
<div class="content">
<pre>$ for CELL in $(echo $CELLS); do
&gt;   RCELL=$CELL
&gt;   [ "$CELL" = "default" ] &amp;&amp; RCELL=$DEFAULT_CELL_NAME
&gt;   oc rsh mariadb-copy-data &lt;&lt; EOF
&gt;     declare -A db_name_map  <i class="conum" data-value="1"></i><b>(1)</b>
&gt;     db_name_map['nova']="nova_$RCELL"
&gt;     db_name_map['ovs_neutron']='neutron'
&gt;     db_name_map['ironic-inspector']='ironic_inspector'
&gt;     declare -A db_cell_map  <i class="conum" data-value="2"></i><b>(2)</b>
&gt;     db_cell_map['nova']="nova_$DEFAULT_CELL_NAME"
&gt;     db_cell_map["nova_$RCELL"]="nova_$RCELL"  <i class="conum" data-value="3"></i><b>(3)</b>
&gt;     declare -A db_server_map  <i class="conum" data-value="4"></i><b>(4)</b>
&gt;     db_server_map['default']=${PODIFIED_MARIADB_IP['super']}
&gt;     db_server_map["nova"]=${PODIFIED_MARIADB_IP[$DEFAULT_CELL_NAME]}
&gt;     db_server_map["nova_$RCELL"]=${PODIFIED_MARIADB_IP[$RCELL]}
&gt;     declare -A db_server_password_map  <i class="conum" data-value="5"></i><b>(5)</b>
&gt;     db_server_password_map['default']=${PODIFIED_DB_ROOT_PASSWORD['super']}
&gt;     db_server_password_map["nova"]=${PODIFIED_DB_ROOT_PASSWORD[$DEFAULT_CELL_NAME]}
&gt;     db_server_password_map["nova_$RCELL"]=${PODIFIED_DB_ROOT_PASSWORD[$RCELL]}
&gt;     cd /backup
&gt;     for db_file in \$(ls ${CELL}.*.sql); do
&gt;       db_name=\$(echo \${db_file} | awk -F'.' '{ print \$2; }')
&gt;       [[ "$CELL" != "default" &amp;&amp; ! -v "db_cell_map[\${db_name}]" ]] &amp;&amp; continue
&gt;       if [[ "$CELL" == "default" &amp;&amp; -v "db_cell_map[\${db_name}]" ]] ; then
&gt;         target=$DEFAULT_CELL_NAME
&gt;       elif [[ "$CELL" == "default" &amp;&amp; ! -v "db_cell_map[\${db_name}]" ]] ; then
&gt;         target=super
&gt;       else
&gt;         target=$RCELL
&gt;       fi  <i class="conum" data-value="6"></i><b>(6)</b>
&gt;       renamed_db_file="\${target}_new.\${db_name}.sql"
&gt;       mv -f \${db_file} \${renamed_db_file}
&gt;       if [[ -v "db_name_map[\${db_name}]" ]]; then
&gt;         echo "renaming $CELL cell \${db_name} to \$target \${db_name_map[\${db_name}]}"
&gt;         db_name=\${db_name_map[\${db_name}]}
&gt;       fi
&gt;       db_server=\${db_server_map["default"]}
&gt;       if [[ -v "db_server_map[\${db_name}]" ]]; then
&gt;         db_server=\${db_server_map[\${db_name}]}
&gt;       fi
&gt;       db_password=\${db_server_password_map['default']}
&gt;       if [[ -v "db_server_password_map[\${db_name}]" ]]; then
&gt;         db_password=\${db_server_password_map[\${db_name}]}
&gt;       fi
&gt;       echo "creating $CELL cell \${db_name} in \$target \${db_server}"
&gt;       mysql -h"\${db_server}" -uroot "-p\${db_password}" -e \
&gt;         "CREATE DATABASE IF NOT EXISTS \${db_name} DEFAULT \
&gt;         CHARACTER SET ${CHARACTER_SET} DEFAULT COLLATE ${COLLATION};"
&gt;       echo "importing $CELL cell \${db_name} into \$target \${db_server} from \${renamed_db_file}"
&gt;       mysql -h "\${db_server}" -uroot "-p\${db_password}" "\${db_name}" &lt; "\${renamed_db_file}"
&gt;     done
&gt;     if [ "$CELL" = "default" ] ; then
&gt;       mysql -h "\${db_server_map['default']}" -uroot -p"\${db_server_password_map['default']}" -e \
&gt;         "update nova_api.cell_mappings set name='$DEFAULT_CELL_NAME' where name='default';"
&gt;     fi
&gt;     mysql -h "\${db_server_map["nova_$RCELL"]}" -uroot -p"\${db_server_password_map["nova_$RCELL"]}" -e \
&gt;       "delete from nova_${RCELL}.services where host not like '%nova_${RCELL}-%' and services.binary != 'nova-compute';"
&gt; EOF
&gt; done</pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Defines which common databases to rename when importing them.</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>Defines which cells databases to import, and how to rename them, if needed.</td>
</tr>
<tr>
<td><i class="conum" data-value="3"></i><b>3</b></td>
<td>Omits importing special <code>cell0</code> databases of the cells, as its contents cannot be consolidated during adoption.</td>
</tr>
<tr>
<td><i class="conum" data-value="4"></i><b>4</b></td>
<td>Defines which databases to import into which servers, usually dedicated for cells.</td>
</tr>
<tr>
<td><i class="conum" data-value="5"></i><b>5</b></td>
<td>Defines the root passwords map for database servers. You can only use the same password for now.</td>
</tr>
<tr>
<td><i class="conum" data-value="6"></i><b>6</b></td>
<td>Assigns which databases to import into which hosts when extracting databases from the <code>default</code> cell.</td>
</tr>
</table>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<div class="title">Verification</div>
<p>Compare the following outputs with the topology-specific service configuration.
For more information, see <a href="#proc_retrieving-topology-specific-service-configuration_migrating-databases">Retrieving topology-specific service configuration</a>.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Check that the databases are imported correctly:</p>
<div class="listingblock">
<div class="content">
<pre>$ set +u
$ . ~/.source_cloud_exported_variables_default
$ set -u
$ dbs=$(oc exec openstack-galera-0 -c galera -- mysql -rs -uroot -p"${PODIFIED_DB_ROOT_PASSWORD['super']}" -e 'SHOW databases;')
$ echo $dbs | grep -Eq '\bkeystone\b' &amp;&amp; echo "OK" || echo "CHECK FAILED"
$ echo $dbs | grep -Eq '\bneutron\b' &amp;&amp; echo "OK" || echo "CHECK FAILED"
$ echo "${PULL_OPENSTACK_CONFIGURATION_DATABASES[@]}" | grep -Eq '\bovs_neutron\b' &amp;&amp; echo "OK" || echo "CHECK FAILED" <i class="conum" data-value="1"></i><b>(1)</b>
$ novadb_mapped_cells=$(oc exec openstack-galera-0 -c galera -- mysql -rs -uroot -p"${PODIFIED_DB_ROOT_PASSWORD['super']}" \
&gt;   nova_api -e 'select uuid,name,transport_url,database_connection,disabled from cell_mappings;') <i class="conum" data-value="2"></i><b>(2)</b>
$ uuidf='\S{8,}-\S{4,}-\S{4,}-\S{4,}-\S{12,}'
$ default=$(printf "%s\n" "$PULL_OPENSTACK_CONFIGURATION_NOVADB_MAPPED_CELLS" | sed -rn "s/^($uuidf)\s+default\b.*$/\1/p")
$ difference=$(diff -ZNua \
&gt;   &lt;(printf "%s\n" "$PULL_OPENSTACK_CONFIGURATION_NOVADB_MAPPED_CELLS") \
&gt;   &lt;(printf "%s\n" "$novadb_mapped_cells")) || true
$ if [ "$DEFAULT_CELL_NAME" != "default" ]; then
&gt;   printf "%s\n" "$difference" | grep -qE "^\-$default\s+default\b" &amp;&amp; echo "OK" || echo "CHECK FAILED"
&gt;   printf "%s\n" "$difference" | grep -qE "^\+$default\s+$DEFAULT_CELL_NAME\b" &amp;&amp; echo "OK" || echo "CHECK FAILED"
&gt;   [ $(grep -E "^[-\+]$uuidf" &lt;&lt;&lt;"$difference" | wc -l) -eq 2 ] &amp;&amp; echo "OK" || echo "CHECK FAILED"
&gt; else
&gt;   [ "x$difference" = "x" ] &amp;&amp; echo "OK" || echo "CHECK FAILED"
&gt; fi
$ for CELL in $(echo $RENAMED_CELLS); do <i class="conum" data-value="3"></i><b>(3)</b>
&gt;   RCELL=$CELL
&gt;   [ "$CELL" = "$DEFAULT_CELL_NAME" ] &amp;&amp; RCELL=default
&gt;   set +u
&gt;   . ~/.source_cloud_exported_variables_$RCELL
&gt;   set -u
&gt;   c1dbs=$(oc exec openstack-$CELL-galera-0 -c galera -- mysql -rs -uroot -p${PODIFIED_DB_ROOT_PASSWORD[$CELL]} -e 'SHOW databases;') <i class="conum" data-value="4"></i><b>(4)</b>
&gt;   echo $c1dbs | grep -Eq "\bnova_${CELL}\b" &amp;&amp; echo "OK" || echo "CHECK FAILED"
&gt;   novadb_svc_records=$(oc exec openstack-$CELL-galera-0 -c galera -- mysql -rs -uroot -p${PODIFIED_DB_ROOT_PASSWORD[$CELL]} \
&gt;     nova_$CELL -e "select host from services where services.binary='nova-compute' and deleted=0 order by host asc;")
&gt;   diff -Z &lt;(echo "x$novadb_svc_records") &lt;(echo "x${PULL_OPENSTACK_CONFIGURATION_NOVA_COMPUTE_HOSTNAMES[@]}") &amp;&amp; echo "OK" || echo "CHECK FAILED" <i class="conum" data-value="5"></i><b>(5)</b>
&gt; done</pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Ensures that the Networking service (neutron) database is renamed from <code>ovs_neutron</code>.</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>Ensures that the <code>default</code> cell is renamed to <code>$DEFAULT_CELL_NAME</code>, and the cell UUIDs are retained.</td>
</tr>
<tr>
<td><i class="conum" data-value="3"></i><b>3</b></td>
<td>Ensures that the registered Compute services names have not changed.</td>
</tr>
<tr>
<td><i class="conum" data-value="4"></i><b>4</b></td>
<td>Ensures Compute service cells databases are extracted to separate database servers, and renamed from <code>nova</code> to <code>nova_cell&lt;X&gt;</code>.</td>
</tr>
<tr>
<td><i class="conum" data-value="5"></i><b>5</b></td>
<td>Ensures that the registered Compute service name has not changed.</td>
</tr>
</table>
</div>
</li>
<li>
<p>Delete the <code>mariadb-data</code> pod and the <code>mariadb-copy-data</code> persistent volume claim that contains the database backup:</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Consider taking a snapshot of them before deleting.
</td>
</tr>
</table>
</div>
<div class="listingblock">
<div class="content">
<pre>$ oc delete pod mariadb-copy-data
$ oc delete pvc mariadb-data</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
During the pre-checks and post-checks, the <code>mariadb-client</code> pod might return a pod security warning related to the <code>restricted:latest</code> security context constraint. This warning is due to default security context constraints and does not prevent the admission controller from creating a pod. You see a warning for the short-lived pod, but it does not interfere with functionality.
For more information, see <a href="https://learn.redhat.com/t5/DO280-Red-Hat-OpenShift/About-pod-security-standards-and-warnings/m-p/32502">About pod security standards and warnings</a>.
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="migrating-ovn-data_migrating-databases"><a class="anchor" href="#migrating-ovn-data_migrating-databases"></a>Migrating OVN data</h3>
<div class="paragraph _abstract">
<p>Migrate the data in the OVN databases from the original Red&#160;Hat OpenStack Platform deployment to <code>ovsdb-server</code> instances that are running in the Red Hat OpenShift Container Platform (RHOCP) cluster.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>The <code>OpenStackControlPlane</code> resource is created.</p>
</li>
<li>
<p><code>NetworkAttachmentDefinition</code> custom resources (CRs) for the original cluster are defined. Specifically, the <code>internalapi</code> network is defined.</p>
</li>
<li>
<p>The original Networking service (neutron) and OVN <code>northd</code> are not running.</p>
</li>
<li>
<p>There is network routability between the control plane services and the adopted cluster.</p>
</li>
<li>
<p>The cloud is migrated to the Modular Layer 2 plug-in with Open Virtual Networking (ML2/OVN) mechanism driver.</p>
</li>
<li>
<p>Define the following shell variables. Replace the example values with values that are correct for your environment:</p>
<div class="listingblock">
<div class="content">
<pre>STORAGE_CLASS=local-storage
OVSDB_IMAGE=registry.redhat.io/rhoso/openstack-ovn-base-rhel9:18.0
SOURCE_OVSDB_IP=172.17.0.100 # For IPv4
SOURCE_OVSDB_IP=[fd00:bbbb::100] # For IPv6</pre>
</div>
</div>
<div class="paragraph">
<p>To get the value to set <code>SOURCE_OVSDB_IP</code>, query the puppet-generated configurations in a Controller node:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>$ grep -rI 'ovn_[ns]b_conn' /var/lib/config-data/puppet-generated/</pre>
</div>
</div>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Prepare a temporary <code>PersistentVolume</code> claim and the helper pod for the OVN backup. Adjust the storage requests for a large database, if needed:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">$ oc apply -f - &lt;&lt;EOF
---
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: ovn-data-cert
spec:
  commonName: ovn-data-cert
  secretName: ovn-data-cert
  issuerRef:
    name: rootca-internal
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ovn-data
spec:
  storageClassName: $STORAGE_CLASS
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
---
apiVersion: v1
kind: Pod
metadata:
  name: ovn-copy-data
  annotations:
    openshift.io/scc: anyuid
    k8s.v1.cni.cncf.io/networks: internalapi
  labels:
    app: adoption
spec:
  containers:
  - image: $OVSDB_IMAGE
    command: [ "sh", "-c", "sleep infinity"]
    name: adoption
    volumeMounts:
    - mountPath: /backup
      name: ovn-data
    - mountPath: /etc/pki/tls/misc
      name: ovn-data-cert
      readOnly: true
  securityContext:
    allowPrivilegeEscalation: false
    capabilities:
      drop: ALL
    runAsNonRoot: true
    seccompProfile:
      type: RuntimeDefault
  volumes:
  - name: ovn-data
    persistentVolumeClaim:
      claimName: ovn-data
  - name: ovn-data-cert
    secret:
      secretName: ovn-data-cert
EOF</code></pre>
</div>
</div>
</li>
<li>
<p>Wait for the pod to be ready:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc wait --for=condition=Ready pod/ovn-copy-data --timeout=30s</pre>
</div>
</div>
</li>
<li>
<p>If the podified internalapi cidr is different than the source internalapi cidr, add an iptables accept rule on the Controller nodes:</p>
<div class="listingblock">
<div class="content">
<pre>$ $CONTROLLER1_SSH sudo iptables -I INPUT -s {PODIFIED_INTERNALAPI_NETWORK} -p tcp -m tcp --dport 6641 -m conntrack --ctstate NEW -j ACCEPT
$ $CONTROLLER1_SSH sudo iptables -I INPUT -s {PODIFIED_INTERNALAPI_NETWORK} -p tcp -m tcp --dport 6642 -m conntrack --ctstate NEW -j ACCEPT</pre>
</div>
</div>
</li>
<li>
<p>Back up your OVN databases:</p>
<div class="ulist">
<ul>
<li>
<p>If you did not enable TLS everywhere, run the following command:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc exec ovn-copy-data -- bash -c "ovsdb-client backup tcp:$SOURCE_OVSDB_IP:6641 &gt; /backup/ovs-nb.db"
$ oc exec ovn-copy-data -- bash -c "ovsdb-client backup tcp:$SOURCE_OVSDB_IP:6642 &gt; /backup/ovs-sb.db"</pre>
</div>
</div>
</li>
<li>
<p>If you enabled TLS everywhere, run the following command:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc exec ovn-copy-data -- bash -c "ovsdb-client backup --ca-cert=/etc/pki/tls/misc/ca.crt --private-key=/etc/pki/tls/misc/tls.key --certificate=/etc/pki/tls/misc/tls.crt ssl:$SOURCE_OVSDB_IP:6641 &gt; /backup/ovs-nb.db"
$ oc exec ovn-copy-data -- bash -c "ovsdb-client backup --ca-cert=/etc/pki/tls/misc/ca.crt --private-key=/etc/pki/tls/misc/tls.key --certificate=/etc/pki/tls/misc/tls.crt ssl:$SOURCE_OVSDB_IP:6642 &gt; /backup/ovs-sb.db"</pre>
</div>
</div>
</li>
</ul>
</div>
</li>
<li>
<p>Start the control plane OVN database services prior to import, with <code>northd</code> disabled:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">$ oc patch openstackcontrolplane openstack --type=merge --patch '
spec:
  ovn:
    enabled: true
    template:
      ovnDBCluster:
        ovndbcluster-nb:
          replicas: 3
          dbType: NB
          storageRequest: 10G
          networkAttachment: internalapi
        ovndbcluster-sb:
          replicas: 3
          dbType: SB
          storageRequest: 10G
          networkAttachment: internalapi
      ovnNorthd:
        replicas: 0
'</code></pre>
</div>
</div>
</li>
<li>
<p>Wait for the OVN database services to reach the <code>Running</code> phase:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc wait --for=jsonpath='{.status.phase}'=Running pod --selector=service=ovsdbserver-nb
$ oc wait --for=jsonpath='{.status.phase}'=Running pod --selector=service=ovsdbserver-sb</pre>
</div>
</div>
</li>
<li>
<p>Fetch the OVN database IP addresses on the <code>clusterIP</code> service network:</p>
<div class="listingblock">
<div class="content">
<pre>PODIFIED_OVSDB_NB_IP=$(oc get svc --selector "statefulset.kubernetes.io/pod-name=ovsdbserver-nb-0" -ojsonpath='{.items[0].spec.clusterIP}')
PODIFIED_OVSDB_SB_IP=$(oc get svc --selector "statefulset.kubernetes.io/pod-name=ovsdbserver-sb-0" -ojsonpath='{.items[0].spec.clusterIP}')</pre>
</div>
</div>
</li>
<li>
<p>If you are  using IPv6, adjust the address to the format expected by <code>ovsdb-*</code> tools:</p>
<div class="listingblock">
<div class="content">
<pre>PODIFIED_OVSDB_NB_IP=[$PODIFIED_OVSDB_NB_IP]
PODIFIED_OVSDB_SB_IP=[$PODIFIED_OVSDB_SB_IP]</pre>
</div>
</div>
</li>
<li>
<p>Upgrade the database schema for the backup files:</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>If you did not enable TLS everywhere, use the following command:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc exec ovn-copy-data -- bash -c "ovsdb-client get-schema tcp:$PODIFIED_OVSDB_NB_IP:6641 &gt; /backup/ovs-nb.ovsschema &amp;&amp; ovsdb-tool convert /backup/ovs-nb.db /backup/ovs-nb.ovsschema"
$ oc exec ovn-copy-data -- bash -c "ovsdb-client get-schema tcp:$PODIFIED_OVSDB_SB_IP:6642 &gt; /backup/ovs-sb.ovsschema &amp;&amp; ovsdb-tool convert /backup/ovs-sb.db /backup/ovs-sb.ovsschema"</pre>
</div>
</div>
</li>
<li>
<p>If you enabled TLS everywhere, use the following command:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc exec ovn-copy-data -- bash -c "ovsdb-client get-schema --ca-cert=/etc/pki/tls/misc/ca.crt --private-key=/etc/pki/tls/misc/tls.key --certificate=/etc/pki/tls/misc/tls.crt ssl:$PODIFIED_OVSDB_NB_IP:6641 &gt; /backup/ovs-nb.ovsschema &amp;&amp; ovsdb-tool convert /backup/ovs-nb.db /backup/ovs-nb.ovsschema"
$ oc exec ovn-copy-data -- bash -c "ovsdb-client get-schema --ca-cert=/etc/pki/tls/misc/ca.crt --private-key=/etc/pki/tls/misc/tls.key --certificate=/etc/pki/tls/misc/tls.crt ssl:$PODIFIED_OVSDB_SB_IP:6642 &gt; /backup/ovs-sb.ovsschema &amp;&amp; ovsdb-tool convert /backup/ovs-sb.db /backup/ovs-sb.ovsschema"</pre>
</div>
</div>
</li>
</ol>
</div>
</li>
<li>
<p>Restore the database backup to the new OVN database servers:</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>If you did not enable TLS everywhere, use the following command:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc exec ovn-copy-data -- bash -c "ovsdb-client restore tcp:$PODIFIED_OVSDB_NB_IP:6641 &lt; /backup/ovs-nb.db"
$ oc exec ovn-copy-data -- bash -c "ovsdb-client restore tcp:$PODIFIED_OVSDB_SB_IP:6642 &lt; /backup/ovs-sb.db"</pre>
</div>
</div>
</li>
<li>
<p>If you enabled TLS everywhere, use the following command:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc exec ovn-copy-data -- bash -c "ovsdb-client restore --ca-cert=/etc/pki/tls/misc/ca.crt --private-key=/etc/pki/tls/misc/tls.key --certificate=/etc/pki/tls/misc/tls.crt ssl:$PODIFIED_OVSDB_NB_IP:6641 &lt; /backup/ovs-nb.db"
$ oc exec ovn-copy-data -- bash -c "ovsdb-client restore --ca-cert=/etc/pki/tls/misc/ca.crt --private-key=/etc/pki/tls/misc/tls.key --certificate=/etc/pki/tls/misc/tls.crt ssl:$PODIFIED_OVSDB_SB_IP:6642 &lt; /backup/ovs-sb.db"</pre>
</div>
</div>
</li>
</ol>
</div>
</li>
<li>
<p>Check that the data was successfully migrated by running the following commands against the new database servers, for example:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc exec -it ovsdbserver-nb-0 -- ovn-nbctl show
$ oc exec -it ovsdbserver-sb-0 -- ovn-sbctl list Chassis</pre>
</div>
</div>
</li>
<li>
<p>Start the control plane <code>ovn-northd</code> service to keep both OVN databases in sync:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">$ oc patch openstackcontrolplane openstack --type=merge --patch '
spec:
  ovn:
    enabled: true
    template:
      ovnNorthd:
        replicas: 1
'</code></pre>
</div>
</div>
</li>
<li>
<p>If you are running OVN gateway services on RHOCP nodes, enable the control plane <code>ovn-controller</code> service:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">$ oc patch openstackcontrolplane openstack --type=merge --patch '
spec:
  ovn:
    enabled: true
    template:
      ovnController:
        nicMappings:
          physNet: NIC <i class="conum" data-value="1"></i><b>(1)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td><code>physNet</code> is the name of your physical network. <code>NIC</code> is the name of the physical interface that is connected to your physical network.</td>
</tr>
</table>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Running OVN gateways on RHOCP nodes might be prone to data plane downtime during Open vSwitch upgrades. Consider running OVN gateways on dedicated <code>Networker</code> data plane nodes for production deployments instead.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Delete the <code>ovn-data</code> helper pod and the temporary <code>PersistentVolumeClaim</code> that is used to store OVN database backup files:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc delete --ignore-not-found=true pod ovn-copy-data
$ oc delete --ignore-not-found=true pvc ovn-data</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Consider taking a snapshot of the <code>ovn-data</code> helper pod and the temporary <code>PersistentVolumeClaim</code> before deleting them. For more information, see <a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/html/storage/index#lvms-about-volume-snapsot_logical-volume-manager-storage">About volume snapshots</a> in <em>OpenShift Container Platform storage overview</em>.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Stop the adopted OVN database servers:</p>
<div class="listingblock">
<div class="content">
<pre>ServicesToStop=("tripleo_ovn_cluster_north_db_server.service"
                "tripleo_ovn_cluster_south_db_server.service")

echo "Stopping systemd OpenStack services"
for service in ${ServicesToStop[*]}; do
    for i in {1..3}; do
        SSH_CMD=CONTROLLER${i}_SSH
        if [ ! -z "${!SSH_CMD}" ]; then
            echo "Stopping the $service in controller $i"
            if ${!SSH_CMD} sudo systemctl is-active $service; then
                ${!SSH_CMD} sudo systemctl stop $service
            fi
        fi
    done
done

echo "Checking systemd OpenStack services"
for service in ${ServicesToStop[*]}; do
    for i in {1..3}; do
        SSH_CMD=CONTROLLER${i}_SSH
        if [ ! -z "${!SSH_CMD}" ]; then
            if ! ${!SSH_CMD} systemctl show $service | grep ActiveState=inactive &gt;/dev/null; then
                echo "ERROR: Service $service still running on controller $i"
            else
                echo "OK: Service $service is not running on controller $i"
            fi
        fi
    done
done</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
</div>
</div>
<nav class="pagination">
  <span class="prev"><a href="adoption-overview.html">RHOSP 17.1 to RHOSO 18 adoption overview</a></span>
  <span class="next"><a href="adoption-cp.html">Control plane Adoption</a></span>
</nav>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
