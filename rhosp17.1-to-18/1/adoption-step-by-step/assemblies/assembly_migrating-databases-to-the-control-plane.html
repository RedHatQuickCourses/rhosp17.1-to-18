<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Migrating databases to the control plane :: Upgrade a RHOSP 17.1 to RHOSO 18 using the adoption mechanism</title>
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../../_/css/site.css">
    <script>var uiRootPath = '../../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../../..">Upgrade a RHOSP 17.1 to RHOSO 18 using the adoption mechanism</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/REPLACEREPONAME/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="rhosp17.1-to-18" data-version="1">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../../index.html">Upgrade a RHOSP 17.1 to RHOSO 18 using the adoption mechanism</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../../index.html">Home</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../../LABENV/index-lab-role.html">Lab Environment</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../../lab_environment/index.html">Network Diagram of the Lab Environment</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../index.html">Upgrade a RHOSP 17.1 to RHOSO 18 using the adoption mechanism through an example</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../prereqs.html">Perform OpenShift Prerequisite</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../install-operators.html">Install the Red Hat OpenStack Platform Service Operators</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../network-isolation.html">Prepare OCP for OpenStack Network Isolation</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../adoption-overview.html">RHOSP 17.1 to RHOSO 18 adoption overview</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../migrating-databases.html">Migrating RHOSP 17.1 Database to RHOSO 18</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../adoption-cp.html">Control plane Adoption</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../adoption-dp.html">Data plane Adoption</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">Upgrade a RHOSP 17.1 to RHOSO 18 using the adoption mechanism</span>
    <span class="version">1</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../../index.html">Upgrade a RHOSP 17.1 to RHOSO 18 using the adoption mechanism</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../../index.html">1</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../../index.html">Upgrade a RHOSP 17.1 to RHOSO 18 using the adoption mechanism</a></li>
    <li><a href="assembly_migrating-databases-to-the-control-plane.html">Migrating databases to the control plane</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Migrating databases to the control plane</h1>
<div id="preamble">
<div class="sectionbody">
<div class="paragraph _abstract">
<p>To begin creating the control plane, enable back-end services and import the databases from your original {rhos_prev_long} {rhos_prev_ver} deployment.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="proc_retrieving-topology-specific-service-configuration_migrating-databases"><a class="anchor" href="#proc_retrieving-topology-specific-service-configuration_migrating-databases"></a>Retrieving topology-specific service configuration</h2>
<div class="sectionbody">
<div class="paragraph _abstract">
<p>Before you migrate your databases to the {rhos_long} control plane, retrieve the topology-specific service configuration from your {rhos_prev_long} ({OpenStackShort}) environment. You need this configuration for the following reasons:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>To check your current database for inaccuracies</p>
</li>
<li>
<p>To ensure that you have the data you need before the migration</p>
</li>
<li>
<p>To compare your {OpenStackShort} database with the adopted {rhos_acro} database
Get the controller VIP, to use it later in the environment variable SOURCE_MARIADB_IP.</p>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>From the workstation, access to the director host and change to the stack user:</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">ssh 172.25.250.15</code></pre>
</div>
</div>
</li>
<li>
<p>Escalate to root:</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">sudo -i</code></pre>
</div>
</div>
</li>
<li>
<p>Change to the stack user:</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">su - stack</code></pre>
</div>
</div>
</li>
<li>
<p>Load the stackrc file:</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">. stackrc</code></pre>
</div>
</div>
</li>
</ol>
</div>
</li>
<li>
<p>In the <strong>director host</strong>, get the internal_api VIP from this file:</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">cat /home/stack/templates/overcloud-vip-deployed.yaml</code></pre>
</div>
</div>
<div class="paragraph">
<p>Sample output:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>[...]
  VipPortMap:
    external:
      ip_address: 172.25.250.109
      ip_address_uri: 172.25.250.109
      ip_subnet: 172.25.250.109/24
    internal_api:
      ip_address: 192.168.52.55
      ip_address_uri: 192.168.52.55
      ip_subnet: 192.168.52.55/24
    storage:
      ip_address: 192.168.50.78
      ip_address_uri: 192.168.50.78
      ip_subnet: 192.168.50.78/24
    storage_mgmt:
      ip_address: 192.168.51.41
      ip_address_uri: 192.168.51.41
      ip_subnet: 192.168.51.41/24
[...]</pre>
</div>
</div>
<div class="paragraph">
<p>You can also get the internal_api VIP by accessing one of the controller nodes:</p>
</div>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>List the nodes:</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">[student@director ~] metalsmith list</code></pre>
</div>
</div>
<div class="paragraph">
<p>Sample output:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>+--------------------------------------+---------------------+--------------------------------------+-------------------------+--------+-------------------------+
| UUID                                 | Node Name           | Allocation UUID                      | Hostname                | State  | IP Addresses            |
+--------------------------------------+---------------------+--------------------------------------+-------------------------+--------+-------------------------+
| 1c708c39-91d6-4500-a395-42c081adea78 | overcloud-control0  | 3b077ad2-ddb4-462d-a573-4a3ab6aa7b84 | overcloud-controller-0  | ACTIVE | ctlplane=172.25.249.16  |
| ffca0025-c9b9-44ff-8aa1-a15d2b6f9163 | overcloud-control1  | a6416542-3e50-45c8-b824-6673722d861e | overcloud-controller-1  | ACTIVE | ctlplane=172.25.249.17  |
| 4b493c16-2dfe-4db6-985b-4aa119e98b14 | overcloud-control2  | 8d82bd92-a096-4841-8214-841a0af1f62d | overcloud-controller-2  | ACTIVE | ctlplane=172.25.249.18  |
| 91e2a13e-0b62-4181-88a3-178efd70c941 | overcloud-compute0  | 3e8426f4-e8bc-40d2-84aa-3c68ccca0dad | overcloud-novacompute-0 | ACTIVE | ctlplane=172.25.249.19  |
| ed1f3a65-e80f-458b-9c2d-873e9073e3bb | overcloud-compute1  | 1f6c4f74-f7cd-4c4f-bd41-89083d441d9d | overcloud-novacompute-1 | ACTIVE | ctlplane=172.25.249.22  |
| 51d45604-8bf8-43f9-9718-d704398296e3 | overcloud-networker | b5f71cd8-5f42-46a8-8c51-a75608745744 | overcloud-networker     | ACTIVE | ctlplane=172.25.249.250 |
+--------------------------------------+---------------------+--------------------------------------+-------------------------+--------+-------------------------+</pre>
</div>
</div>
</li>
<li>
<p>SSH to one of the controller nodes:</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">ssh tripleo-admin@172.25.249.16</code></pre>
</div>
</div>
</li>
<li>
<p>Query the internal_api VIP:</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">sudo grep -rI 'listen mysql' -A10 /var/lib/config-data/puppet-generated/ | grep bind</code></pre>
</div>
</div>
<div class="paragraph">
<p>Sample output:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>/var/lib/config-data/puppet-generated/haproxy/etc/haproxy/haproxy.cfg-  bind 192.168.52.55:3306 transparent</pre>
</div>
</div>
</li>
<li>
<p>Logout from the controller node:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">logout</code></pre>
</div>
</div>
<div class="paragraph">
<p>Then the values for SOURCE_MARIADB_IP and CONTROLLER_IP are:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>SOURCE_MARIADB_IP is then 192.168.52.55
CONTROLLER1_IP is then 172.25.249.16
CONTROLLER2_IP is then 172.25.249.17
CONTROLLER3_IP is then 172.25.249.18</pre>
</div>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>In the <strong>director host</strong>, copy the overcloud-passwords from the director node to the student user home directory:</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">sudo cp /home/stack/overcloud-deploy/overcloud/overcloud-passwords.yaml /home/student/overcloud-passwords.yaml
sudo chown student:student /home/student/overcloud-passwords.yaml
sudo cp /home/stack/.ssh/id_rsa_tripleo* /home/student/.ssh/
sudo chown student:student /home/student/.ssh/id_rsa_tripleo
sudo chown student:student /home/student/.ssh/id_rsa_tripleo.pub</code></pre>
</div>
</div>
</li>
<li>
<p>In the <strong>workstation host</strong>, copy the overcloud-passwords from the director node to the utiliy node:</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">scp student@172.25.250.15:~/overcloud-passwords.yaml lab@utility:~/overcloud-passwords.yaml
scp student@172.25.250.15:~/.ssh/id_rsa_tripleo* lab@utility:~/.ssh/</code></pre>
</div>
</div>
</li>
<li>
<p>In the <strong>utility host</strong> log in to the Red Hat registry and create the secret, it will be used to pull the helper pod images:</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc get secret/pull-secret -n openshift-config -o json | jq -r '.data.".dockerconfigjson"' | base64 -d > authfile</code></pre>
</div>
</div>
</li>
<li>
<p>Authenticate to the Red Hat registry:</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">podman login registry.redhat.io --authfile authfile</code></pre>
</div>
</div>
</li>
<li>
<p>Set the pull secret:</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc set data secret/pull-secret -n openshift-config --from-file=.dockerconfigjson=authfile</code></pre>
</div>
</div>
</li>
</ol>
</div>
</li>
</ol>
</div>
<div class="olist arabic">
<div class="title">Prerequisites</div>
<ol class="arabic">
<li>
<p>In the <strong>utility host</strong>, define the following shell variables. Replace the example values with values that are correct for your environment:</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">PASSWORD_FILE="$HOME/overcloud-passwords.yaml"
MARIADB_IMAGE=registry.redhat.io/rhoso/openstack-mariadb-rhel9:18.0
declare -A TRIPLEO_PASSWORDS
CELLS="default"
for CELL in $(echo $CELLS); do
  TRIPLEO_PASSWORDS[$CELL]="$PASSWORD_FILE"
done
declare -A SOURCE_DB_ROOT_PASSWORD
for CELL in $(echo $CELLS); do
  SOURCE_DB_ROOT_PASSWORD[$CELL]=$(cat ${TRIPLEO_PASSWORDS[$CELL]} | grep ' MysqlRootPassword:' | awk -F ': ' '{ print $2; }')
done</code></pre>
</div>
</div>
</li>
<li>
<p>Define the following shell variables. Replace the example values with values that are correct for your environment:</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">MARIADB_CLIENT_ANNOTATIONS='--annotations=k8s.v1.cni.cncf.io/networks=internalapi'
MARIADB_RUN_OVERRIDES="$MARIADB_CLIENT_ANNOTATIONS"
CONTROLLER1_SSH="ssh -i /home/lab/.ssh/id_rsa_tripleo tripleo-admin@172.25.249.16"

declare -A SOURCE_MARIADB_IP
SOURCE_MARIADB_IP[default]=192.168.52.55</code></pre>
</div>
</div>
</li>
</ol>
</div>
</li>
</ul>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The source cloud always uses the same password for cells databases. For that reason, the same passwords file is used for all cells stacks. However, split-stack topology allows using different passwords files for each stack.
</td>
</tr>
</table>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Export the shell variables for the following outputs and test the connection to the {OpenStackShort} database:</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">unset PULL_OPENSTACK_CONFIGURATION_DATABASES
declare -xA PULL_OPENSTACK_CONFIGURATION_DATABASES
for CELL in $(echo $CELLS); do
  PULL_OPENSTACK_CONFIGURATION_DATABASES[$CELL]=$(oc run mariadb-client-1-$CELL ${MARIADB_RUN_OVERRIDES} -q --image ${MARIADB_IMAGE} -i --rm --restart=Never -- \
    mysql -rsh "${SOURCE_MARIADB_IP[$CELL]}" -uroot -p"${SOURCE_DB_ROOT_PASSWORD[$CELL]}" -e 'SHOW databases;')
done</code></pre>
</div>
</div>
<div class="paragraph">
<p>If the connection is successful, the expected output is nothing.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The <code>nova</code>, <code>nova_api</code>, and <code>nova_cell0</code> databases are included in the same database host for the main overcloud {orchestration_first_ref} stack.
Additional cells use the <code>nova</code> database of their local Galera clusters.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Run <code>mysqlcheck</code> on the {OpenStackShort} database to check for inaccuracies:</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">unset PULL_OPENSTACK_CONFIGURATION_MYSQLCHECK_NOK
declare -xA PULL_OPENSTACK_CONFIGURATION_MYSQLCHECK_NOK
run_mysqlcheck() {
  PULL_OPENSTACK_CONFIGURATION_MYSQLCHECK_NOK=$(oc run mariadb-client-2-$1 ${MARIADB_RUN_OVERRIDES} -q --image ${MARIADB_IMAGE} -i --rm --restart=Never -- \
    mysqlcheck --all-databases -h ${SOURCE_MARIADB_IP[$CELL]} -u root -p"${SOURCE_DB_ROOT_PASSWORD[$CELL]}" | grep -v OK)
}
for CELL in $(echo $CELLS); do
  run_mysqlcheck $CELL
done
if [ "$PULL_OPENSTACK_CONFIGURATION_MYSQLCHECK_NOK" != "" ]; then
  for CELL in $(echo $CELLS); do
    MYSQL_UPGRADE=$(oc run mariadb-client-3 ${MARIADB_RUN_OVERRIDES} -q --image ${MARIADB_IMAGE} -i --rm --restart=Never -- \
      mysql_upgrade -v -h ${SOURCE_MARIADB_IP[$CELL]} -u root -p"${SOURCE_DB_ROOT_PASSWORD[$CELL]}")
    # rerun mysqlcheck to check if problem is resolved
    run_mysqlcheck
  done
fi</code></pre>
</div>
</div>
</li>
<li>
<p>Get the {compute_service_first_ref} cell mappings:</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">export PULL_OPENSTACK_CONFIGURATION_NOVADB_MAPPED_CELLS=$(oc run mariadb-client-1 ${MARIADB_RUN_OVERRIDES} -q --image ${MARIADB_IMAGE} -i --rm --restart=Never -- \
    mysql -rsh "${SOURCE_MARIADB_IP['default']}" -uroot -p"${SOURCE_DB_ROOT_PASSWORD['default']}" nova_api -e \
    'select uuid,name,transport_url,database_connection,disabled from cell_mappings;')</code></pre>
</div>
</div>
</li>
<li>
<p>Get the hostnames of the registered Compute services:</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">unset PULL_OPENSTACK_CONFIGURATION_NOVA_COMPUTE_HOSTNAMES
declare -xA PULL_OPENSTACK_CONFIGURATION_NOVA_COMPUTE_HOSTNAMES
for CELL in $(echo $CELLS); do
  PULL_OPENSTACK_CONFIGURATION_NOVA_COMPUTE_HOSTNAMES[$CELL]=$(oc run mariadb-client-4-$CELL ${MARIADB_RUN_OVERRIDES} -q --image ${MARIADB_IMAGE} -i --rm --restart=Never -- \
    mysql -rsh "${SOURCE_MARIADB_IP[$CELL]}" -uroot -p"${SOURCE_DB_ROOT_PASSWORD[$CELL]}" -e \
      "select host from nova.services where services.binary='nova-compute' and deleted=0;")
done</code></pre>
</div>
</div>
</li>
<li>
<p>Get the list of the mapped {compute_service} cells:</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">export PULL_OPENSTACK_CONFIGURATION_NOVAMANAGE_CELL_MAPPINGS=$($CONTROLLER1_SSH sudo podman exec -it nova_conductor nova-manage cell_v2 list_cells)</code></pre>
</div>
</div>
</li>
<li>
<p>Store the exported variables for future use:</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">unset SRIOV_AGENTS
declare -xA SRIOV_AGENTS
for CELL in $(echo $CELLS); do
  RCELL=$CELL
  [ "$CELL" = "$DEFAULT_CELL_NAME" ] &amp;&amp; RCELL=default
  cat &gt; ~/.source_cloud_exported_variables_$CELL &lt;&lt; EOF
unset PULL_OPENSTACK_CONFIGURATION_DATABASES
unset PULL_OPENSTACK_CONFIGURATION_MYSQLCHECK_NOK
unset PULL_OPENSTACK_CONFIGURATION_NOVA_COMPUTE_HOSTNAMES
declare -xA PULL_OPENSTACK_CONFIGURATION_DATABASES
declare -xA PULL_OPENSTACK_CONFIGURATION_MYSQLCHECK_NOK
declare -xA PULL_OPENSTACK_CONFIGURATION_NOVA_COMPUTE_HOSTNAMES
PULL_OPENSTACK_CONFIGURATION_DATABASES[$CELL]="$(oc run mariadb-client-5-$CELL ${MARIADB_RUN_OVERRIDES} -q --image ${MARIADB_IMAGE} -i --rm --restart=Never -- \
  mysql -rsh ${SOURCE_MARIADB_IP[$RCELL]} -uroot -p${SOURCE_DB_ROOT_PASSWORD[$RCELL]} -e 'SHOW databases;')"
PULL_OPENSTACK_CONFIGURATION_MYSQLCHECK_NOK[$CELL]="$(oc run mariadb-client-6-$CELL ${MARIADB_RUN_OVERRIDES} -q --image ${MARIADB_IMAGE} -i --rm --restart=Never -- \
  mysqlcheck --all-databases -h ${SOURCE_MARIADB_IP[$RCELL]} -u root -p${SOURCE_DB_ROOT_PASSWORD[$RCELL]} | grep -v OK)"
PULL_OPENSTACK_CONFIGURATION_NOVA_COMPUTE_HOSTNAMES[$CELL]="$(oc run mariadb-client-7-$CELL ${MARIADB_RUN_OVERRIDES} -q --image ${MARIADB_IMAGE} -i --rm --restart=Never -- \
  mysql -rsh ${SOURCE_MARIADB_IP[$RCELL]} -uroot -p${SOURCE_DB_ROOT_PASSWORD[$RCELL]} -e \
  "select host from nova.services where services.binary='nova-compute' and deleted=0;")"
if [ "$RCELL" = "default" ]; then
  PULL_OPENSTACK_CONFIGURATION_NOVADB_MAPPED_CELLS="$(oc run mariadb-client-2 ${MARIADB_RUN_OVERRIDES} -q --image ${MARIADB_IMAGE} -i --rm --restart=Never -- \
    mysql -rsh ${SOURCE_MARIADB_IP[$RCELL]} -uroot -p${SOURCE_DB_ROOT_PASSWORD[$RCELL]} nova_api -e \
      'select uuid,name,transport_url,database_connection,disabled from cell_mappings;')"
  PULL_OPENSTACK_CONFIGURATION_NOVAMANAGE_CELL_MAPPINGS="$($CONTROLLER1_SSH sudo podman exec -it nova_conductor nova-manage cell_v2 list_cells)"
fi
EOF
done
chmod 0600 ~/.source_cloud_exported_variables*</code></pre>
</div>
</div>
<div class="paragraph">
<div class="title">Next steps</div>
<p>This configuration and the exported values are required later, during the data plane adoption post-checks. After the {OpenStackShort} control plane services are shut down, if any of the exported values are lost, re-running the <code>export</code> command fails because the control plane services are no longer running on the source cloud, and the data cannot be retrieved. To avoid data loss, preserve the exported values in an environment file before shutting down the control plane services.</p>
</div>
</li>
</ol>
</div>
</div>
</div>
<div class="sect1">
<h2 id="deploying-backend-services_migrating-databases"><a class="anchor" href="#deploying-backend-services_migrating-databases"></a>Deploying back-end services</h2>
<div class="sectionbody">
<div class="paragraph _abstract">
<p>Create the <code>OpenStackControlPlane</code> custom resource (CR) with the basic back-end services deployed, and disable all the {rhos_prev_long} ({OpenStackShort}) services. This CR is the foundation of the control plane.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>The cloud that you want to adopt is running, and it is on the latest minor version of {OpenStackShort} {rhos_prev_ver}.</p>
</li>
<li>
<p>All control plane and data plane hosts of the source cloud are running, and continue to run throughout the adoption procedure.</p>
</li>
<li>
<p>The <code>openstack-operator</code> is deployed, but <code>OpenStackControlPlane</code> is
not deployed.</p>
</li>
<li>
<p>There are free PVs available for Galera and RabbitMQ.</p>
</li>
<li>
<p>We will use the existing {OpenStackShort} deployment password:</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">declare -A TRIPLEO_PASSWORDS
TRIPLEO_PASSWORDS[default]="$HOME/overcloud-passwords.yaml"
ADMIN_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' AdminPassword:' | awk -F ': ' '{ print $2; }')</code></pre>
</div>
</div>
</li>
<li>
<p>Set the service password variables to match the original deployment.
Database passwords can differ in the control plane environment, but
you must synchronize the service account passwords.</p>
<div class="paragraph">
<p>For example, in developer environments with {OpenStackPreviousInstaller} Standalone, the passwords can be extracted:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">AODH_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' AodhPassword:' | awk -F ': ' '{ print $2; }')
BARBICAN_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' BarbicanPassword:' | awk -F ': ' '{ print $2; }')
CEILOMETER_METERING_SECRET=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' CeilometerMeteringSecret:' | awk -F ': ' '{ print $2; }')
CEILOMETER_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' CeilometerPassword:' | awk -F ': ' '{ print $2; }')
CINDER_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' CinderPassword:' | awk -F ': ' '{ print $2; }')
GLANCE_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' GlancePassword:' | awk -F ': ' '{ print $2; }')
HEAT_AUTH_ENCRYPTION_KEY=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' HeatAuthEncryptionKey:' | awk -F ': ' '{ print $2; }')
HEAT_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' HeatPassword:' | awk -F ': ' '{ print $2; }')
HEAT_STACK_DOMAIN_ADMIN_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' HeatStackDomainAdminPassword:' | awk -F ': ' '{ print $2; }')
IRONIC_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' IronicPassword:' | awk -F ': ' '{ print $2; }')
MANILA_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' ManilaPassword:' | awk -F ': ' '{ print $2; }')
NEUTRON_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' NeutronPassword:' | awk -F ': ' '{ print $2; }')
NOVA_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' NovaPassword:' | awk -F ': ' '{ print $2; }')
OCTAVIA_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' OctaviaPassword:' | awk -F ': ' '{ print $2; }')
PLACEMENT_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' PlacementPassword:' | awk -F ': ' '{ print $2; }')
SWIFT_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' SwiftPassword:' | awk -F ': ' '{ print $2; }')</code></pre>
</div>
</div>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Create the {OpenStackShort} secret.</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">cd /home/lab/labrepo/files
oc apply -f osp-ng-ctlplane-secret.yaml</code></pre>
</div>
</div>
</li>
<li>
<p>If the <code>$ADMIN_PASSWORD</code> is different than the password you set
in <code>osp-secret</code>, amend the <code>AdminPassword</code> key in the <code>osp-secret</code>:</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc set data secret/osp-secret "AdminPassword=$ADMIN_PASSWORD"</code></pre>
</div>
</div>
</li>
<li>
<p>Set service account passwords in <code>osp-secret</code> to match the service
account passwords from the original deployment:</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc set data secret/osp-secret "AodhPassword=$AODH_PASSWORD"
oc set data secret/osp-secret "BarbicanPassword=$BARBICAN_PASSWORD"
oc set data secret/osp-secret "CeilometerPassword=$CEILOMETER_PASSWORD"
oc set data secret/osp-secret "CinderPassword=$CINDER_PASSWORD"
oc set data secret/osp-secret "GlancePassword=$GLANCE_PASSWORD"
oc set data secret/osp-secret "HeatAuthEncryptionKey=$HEAT_AUTH_ENCRYPTION_KEY"
oc set data secret/osp-secret "HeatPassword=$HEAT_PASSWORD"
oc set data secret/osp-secret "HeatStackDomainAdminPassword=$HEAT_STACK_DOMAIN_ADMIN_PASSWORD"
oc set data secret/osp-secret "IronicPassword=$IRONIC_PASSWORD"
oc set data secret/osp-secret "IronicInspectorPassword=$IRONIC_PASSWORD"
oc set data secret/osp-secret "ManilaPassword=$MANILA_PASSWORD"
oc set data secret/osp-secret "MetadataSecret=$METADATA_SECRET"
oc set data secret/osp-secret "NeutronPassword=$NEUTRON_PASSWORD"
oc set data secret/osp-secret "NovaPassword=$NOVA_PASSWORD"
oc set data secret/osp-secret "OctaviaPassword=$OCTAVIA_PASSWORD"
oc set data secret/osp-secret "PlacementPassword=$PLACEMENT_PASSWORD"
oc set data secret/osp-secret "SwiftPassword=$SWIFT_PASSWORD"</code></pre>
</div>
</div>
</li>
<li>
<p>Deploy the <code>OpenStackControlPlane</code> CR. Ensure that you only enable the DNS, Galera, Memcached, and RabbitMQ services. All other services must
be disabled:</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc apply -f osp-ng-ctlplane-deploy-backend.yaml</code></pre>
</div>
</div>
<div class="ulist">
<div class="title">Verification</div>
<ul>
<li>
<p>Verify that the Galera and RabbitMQ status is <code>Running</code> for all defined cells:</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">RENAMED_CELLS="cell1"
oc get pod openstack-galera-0 -o jsonpath='{.status.phase}{"\n"}'
oc get pod rabbitmq-server-0 -o jsonpath='{.status.phase}{"\n"}'
for CELL in $(echo $RENAMED_CELLS); do
  oc get pod openstack-$CELL-galera-0 -o jsonpath='{.status.phase}{"\n"}'
  oc get pod rabbitmq-$CELL-server-0 -o jsonpath='{.status.phase}{"\n"}'
done</code></pre>
</div>
</div>
<div class="paragraph">
<p>The given cells names are later referred to by using the environment variable <code>RENAMED_CELLS</code>.
During the database migration procedure, the Nova cells are renamed. <code>RENAMED_CELLS</code> variable represents the new cell names used in the RHOSO deployment.</p>
</div>
</li>
<li>
<p>Ensure that the statuses of all the Rabbitmq and Galera CRs are <code>Setup complete</code>:</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc get Rabbitmqs,Galera</code></pre>
</div>
</div>
<div class="paragraph">
<p>Sample output:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>NAME                                                                  STATUS   MESSAGE
rabbitmq.rabbitmq.openstack.org/rabbitmq                              True     Setup complete
rabbitmq.rabbitmq.openstack.org/rabbitmq-cell1                        True     Setup complete

NAME                                                               READY   MESSAGE
galera.mariadb.openstack.org/openstack                             True     Setup complete
galera.mariadb.openstack.org/openstack-cell1                       True     Setup complete</pre>
</div>
</div>
</li>
<li>
<p>Verify that the <code>OpenStackControlPlane</code> CR is waiting for deployment
of the <code>openstackclient</code> pod:</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc get OpenStackControlPlane openstack</code></pre>
</div>
</div>
<div class="paragraph">
<p>Sample output:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>NAME        STATUS    MESSAGE
openstack   Unknown   OpenStackControlPlane Client not started</pre>
</div>
</div>
</li>
</ul>
</div>
</li>
</ol>
</div>
</div>
</div>
<div class="sect1">
<h2 id="configuring-a-ceph-backend_migrating-databases"><a class="anchor" href="#configuring-a-ceph-backend_migrating-databases"></a>Configuring a {Ceph} back end</h2>
<div class="sectionbody">
<div class="paragraph _abstract">
<p>If your {rhos_prev_long} ({OpenStackShort}) {rhos_prev_ver} deployment uses a {Ceph} back end for any service, such as {image_service_first_ref}, {block_storage_first_ref}, {compute_service_first_ref}, or {rhos_component_storage_file_first_ref}, you must configure the custom resources (CRs) to use the same back end in the {rhos_long} {rhos_curr_ver} deployment.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
To run <code>ceph</code> commands, you must use SSH to connect to a {Ceph} node and run <code>sudo cephadm shell</code>. This generates a Ceph orchestrator container that enables you to run administrative commands against the {CephCluster} cluster. If you deployed the {CephCluster} cluster by using {OpenStackPreviousInstaller}, you can launch the <code>cephadm</code> shell from an {OpenStackShort} Controller node.
</td>
</tr>
</table>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>The <code>OpenStackControlPlane</code> CR is created.</p>
</li>
<li>
<p>If your {OpenStackShort} {rhos_prev_ver} deployment uses the {rhos_component_storage_file}, the openstack keyring is updated.
Using the same user across all services makes it simpler to create a common {Ceph} secret that includes the keyring and <code>ceph.conf</code> file and propagate the secret to all the services that need it.</p>
</li>
<li>
<p>The following shell variables are defined. Replace the following example values with values that are correct for your environment:</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">CEPH_SSH="ssh -i /home/lab/.ssh/lab_rsa student@ceph"
CEPH_KEY=$($CEPH_SSH "sudo cat /etc/ceph/ceph.client.openstack.keyring | base64 -w 0")
CEPH_CONF=$($CEPH_SSH "sudo cat /etc/ceph/ceph.conf | base64 -w 0")</code></pre>
</div>
</div>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Create the <code>ceph-conf-files</code> secret that includes the {Ceph} configuration:</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc apply -f - &lt;&lt;EOF
apiVersion: v1
data:
  ceph.client.openstack.keyring: $CEPH_KEY
  ceph.conf: $CEPH_CONF
kind: Secret
metadata:
  name: ceph-conf-files
type: Opaque
EOF</code></pre>
</div>
</div>
</li>
<li>
<p>In your <code>OpenStackControlPlane</code> CR, inject <code>ceph.conf</code> and <code>ceph.client.openstack.keyring</code> to the {OpenStackShort} services that are defined in the propagation list. For example:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">oc patch openstackcontrolplane openstack --type=merge --patch '
spec:
  extraMounts:
    - name: v1
      region: r1
      extraVol:
        - propagation:
          - CinderVolume
          - CinderBackup
          - GlanceAPI
          extraVolType: Ceph
          volumes:
          - name: ceph
            projected:
              sources:
              - secret:
                  name: ceph-conf-files
          mounts:
          - name: ceph
            mountPath: "/etc/ceph"
            readOnly: true
'</code></pre>
</div>
</div>
</li>
</ol>
</div>
</div>
</div>
<div class="sect1">
<h2 id="stopping-openstack-services_migrating-databases"><a class="anchor" href="#stopping-openstack-services_migrating-databases"></a>Stopping {rhos_prev_long} services</h2>
<div class="sectionbody">
<div class="paragraph _abstract">
<p>Before you start the {rhos_long} adoption, you must stop the {rhos_prev_long} ({OpenStackShort}) services to avoid inconsistencies in the data that you migrate for the data plane adoption. Inconsistencies are caused by resource changes after the database is copied to the new deployment.</p>
</div>
<div class="paragraph">
<p>You should not stop the infrastructure management services yet, such as:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Database</p>
</li>
<li>
<p>RabbitMQ</p>
</li>
<li>
<p>HAProxy Load Balancer</p>
</li>
<li>
<p>Ceph-nfs</p>
</li>
<li>
<p>Compute service</p>
</li>
<li>
<p>Containerized modular libvirt daemons</p>
</li>
<li>
<p>{object_storage_first_ref} back-end services</p>
</li>
</ul>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>Ensure that there no long-running tasks that require the services that you plan to stop, such as instance live migrations, volume migrations, volume creation, backup and restore, attaching, detaching, and other similar operations:</p>
</li>
<li>
<p>Collect the services topology-specific configuration. For more information, see <a href="#proc_retrieving-topology-specific-service-configuration_migrating-databases">Retrieving topology-specific service configuration</a>.</p>
</li>
<li>
<p>Define the following shell variables. The values are examples and refer to a single node standalone {OpenStackPreviousInstaller} deployment. Replace these example values with values that are correct for your environment:</p>
</li>
<li>
<p>Specify the IP addresses of all Controller nodes, for example:</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">CONTROLLER1_SSH="ssh -i /home/lab/.ssh/id_rsa_tripleo tripleo-admin@172.25.249.16"
CONTROLLER2_SSH="ssh -i /home/lab/.ssh/id_rsa_tripleo tripleo-admin@172.25.249.17"
CONTROLLER3_SSH="ssh -i /home/lab/.ssh/id_rsa_tripleo tripleo-admin@172.25.249.18"</code></pre>
</div>
</div>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Disable {OpenStackShort} control plane services:</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash"># Update the services list to be stopped
ServicesToStop=("tripleo_aodh_api.service"
                "tripleo_aodh_api_cron.service"
                "tripleo_aodh_evaluator.service"
                "tripleo_aodh_listener.service"
                "tripleo_aodh_notifier.service"
                "tripleo_ceilometer_agent_central.service"
                "tripleo_ceilometer_agent_notification.service"
                "tripleo_octavia_api.service"
                "tripleo_octavia_health_manager.service"
                "tripleo_octavia_rsyslog.service"
                "tripleo_octavia_driver_agent.service"
                "tripleo_octavia_housekeeping.service"
                "tripleo_octavia_worker.service"
                "tripleo_horizon.service"
                "tripleo_keystone.service"
                "tripleo_barbican_api.service"
                "tripleo_barbican_worker.service"
                "tripleo_barbican_keystone_listener.service"
                "tripleo_cinder_api.service"
                "tripleo_cinder_api_cron.service"
                "tripleo_cinder_scheduler.service"
                "tripleo_cinder_volume.service"
                "tripleo_cinder_backup.service"
                "tripleo_collectd.service"
                "tripleo_glance_api.service"
                "tripleo_gnocchi_api.service"
                "tripleo_gnocchi_metricd.service"
                "tripleo_gnocchi_statsd.service"
                "tripleo_manila_api.service"
                "tripleo_manila_api_cron.service"
                "tripleo_manila_scheduler.service"
                "tripleo_neutron_api.service"
                "tripleo_placement_api.service"
                "tripleo_nova_api_cron.service"
                "tripleo_nova_api.service"
                "tripleo_nova_conductor.service"
                "tripleo_nova_metadata.service"
                "tripleo_nova_scheduler.service"
                "tripleo_nova_vnc_proxy.service"
                "tripleo_aodh_api.service"
                "tripleo_aodh_api_cron.service"
                "tripleo_aodh_evaluator.service"
                "tripleo_aodh_listener.service"
                "tripleo_aodh_notifier.service"
                "tripleo_ceilometer_agent_central.service"
                "tripleo_ceilometer_agent_compute.service"
                "tripleo_ceilometer_agent_ipmi.service"
                "tripleo_ceilometer_agent_notification.service"
                "tripleo_ovn_cluster_northd.service"
                "tripleo_ironic_neutron_agent.service"
                "tripleo_ironic_api.service"
                "tripleo_ironic_inspector.service"
                "tripleo_ironic_conductor.service"
                "tripleo_ironic_inspector_dnsmasq.service"
                "tripleo_ironic_pxe_http.service"
                "tripleo_ironic_pxe_tftp.service")

PacemakerResourcesToStop=("openstack-cinder-volume"
                          "openstack-cinder-backup"
                          "openstack-manila-share")

echo "Stopping systemd OpenStack services"
for service in ${ServicesToStop[*]}; do
    for i in {1..3}; do
        SSH_CMD=CONTROLLER${i}_SSH
        if [ ! -z "${!SSH_CMD}" ]; then
            echo "Stopping the $service in controller $i"
            if ${!SSH_CMD} sudo systemctl is-active $service; then
                ${!SSH_CMD} sudo systemctl stop $service
            fi
        fi
    done
done

echo "Checking systemd OpenStack services"
for service in ${ServicesToStop[*]}; do
    for i in {1..3}; do
        SSH_CMD=CONTROLLER${i}_SSH
        if [ ! -z "${!SSH_CMD}" ]; then
            if ! ${!SSH_CMD} systemctl show $service | grep ActiveState=inactive &gt;/dev/null; then
                echo "ERROR: Service $service still running on controller $i"
            else
                echo "OK: Service $service is not running on controller $i"
            fi
        fi
    done
done

echo "Stopping pacemaker OpenStack services"
for i in {1..3}; do
    SSH_CMD=CONTROLLER${i}_SSH
    if [ ! -z "${!SSH_CMD}" ]; then
        echo "Using controller $i to run pacemaker commands"
        for resource in ${PacemakerResourcesToStop[*]}; do
            if ${!SSH_CMD} sudo pcs resource config $resource &amp;&gt;/dev/null; then
                echo "Stopping $resource"
                ${!SSH_CMD} sudo pcs resource disable $resource
            else
                echo "Service $resource not present"
            fi
    done
        break
    fi
done

echo "Checking pacemaker OpenStack services"
for i in {1..3}; do
    SSH_CMD=CONTROLLER${i}_SSH
    if [ ! -z "${!SSH_CMD}" ]; then
        echo "Using controller $i to run pacemaker commands"
        for resource in ${PacemakerResourcesToStop[*]}; do
            if ${!SSH_CMD} sudo pcs resource config $resource &amp;&gt;/dev/null; then
                if ! ${!SSH_CMD} sudo pcs resource status $resource | grep Started; then
                    echo "OK: Service $resource is stopped"
                else
                    echo "ERROR: Service $resource is started"
                fi
            fi
        done
        break
    fi
    done</code></pre>
</div>
</div>
<div class="paragraph">
<p>If the status of each service is <code>OK</code>, then the services stopped successfully.</p>
</div>
</li>
</ol>
</div>
</div>
</div>
<div class="sect1">
<h2 id="migrating-databases-to-mariadb-instances_migrating-databases"><a class="anchor" href="#migrating-databases-to-mariadb-instances_migrating-databases"></a>Migrating databases to MariaDB instances</h2>
<div class="sectionbody">
<div class="paragraph _abstract">
<p>Migrate your databases from the original {rhos_prev_long} ({OpenStackShort}) deployment to the MariaDB instances in the {rhocp_long} cluster.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>Ensure that the control plane MariaDB and RabbitMQ are running, and that no other control plane services are running.</p>
</li>
<li>
<p>Retrieve the topology-specific service configuration. For more information, see <a href="#proc_retrieving-topology-specific-service-configuration_migrating-databases">Retrieving topology-specific service configuration</a>.</p>
</li>
<li>
<p>Stop the {OpenStackShort} services. For more information, see <a href="#stopping-openstack-services_migrating-databases">Stopping {rhos_prev_long} services</a>.</p>
</li>
<li>
<p>Ensure that there is network routability between the original MariaDB and the MariaDB for the control plane.</p>
</li>
<li>
<p>Define the following shell variables. Replace the following example values with values that are correct for your environment:</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">STORAGE_CLASS=nfs-storage
MARIADB_IMAGE=registry.redhat.io/rhoso/openstack-mariadb-rhel9:18.0

CELLS="default"
DEFAULT_CELL_NAME="cell1"
RENAMED_CELLS="$DEFAULT_CELL_NAME"

CHARACTER_SET=utf8
COLLATION=utf8_general_ci

declare -A PODIFIED_DB_ROOT_PASSWORD
for CELL in $(echo "super $RENAMED_CELLS"); do
  PODIFIED_DB_ROOT_PASSWORD[$CELL]=$(oc get -o json secret/osp-secret | jq -r .data.DbRootPassword | base64 -d)
done

declare -A PODIFIED_MARIADB_IP
for CELL in $(echo "super $RENAMED_CELLS"); do
  if [ "$CELL" = "super" ]; then
    PODIFIED_MARIADB_IP[$CELL]=$(oc get svc --selector "mariadb/name=openstack" -ojsonpath='{.items[0].spec.clusterIP}')
  else
    PODIFIED_MARIADB_IP[$CELL]=$(oc get svc --selector "mariadb/name=openstack-$CELL" -ojsonpath='{.items[0].spec.clusterIP}')
  fi
done

declare -A TRIPLEO_PASSWORDS
for CELL in $(echo $CELLS); do
  if [ "$CELL" = "default" ]; then
    TRIPLEO_PASSWORDS[default]="$HOME/overcloud-passwords.yaml"
  else
    # in a split-stack source cloud, it should take a stack-specific passwords file instead
    TRIPLEO_PASSWORDS[$CELL]="$HOME/overcloud-passwords.yaml"
  fi
done

declare -A SOURCE_DB_ROOT_PASSWORD
for CELL in $(echo $CELLS); do
  SOURCE_DB_ROOT_PASSWORD[$CELL]=$(cat ${TRIPLEO_PASSWORDS[$CELL]} | grep ' MysqlRootPassword:' | awk -F ': ' '{ print $2; }')
done


declare -A SOURCE_MARIADB_IP
SOURCE_MARIADB_IP[default]=192.168.52.55
SOURCE_MARIADB_IP[cell1]=192.168.52.55</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
A standalone {OpenStackPreviousInstaller} environment only creates a 'default' cell, which should be the only <code>CELLS</code> value in this case. The <code>DEFAULT_CELL_NAME</code> value should be <code>cell1</code>.
</td>
</tr>
</table>
</div>
</li>
</ul>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>The <code>super</code> is the top-scope Nova API upcall database instance. A super conductor connects to that database. In subsequent examples, the upcall and cells databases use the same password that is defined in <code>osp-secret</code>. Old passwords are only needed to prepare the data exports.</p>
</div>
<div class="paragraph">
<p>To get the values for <code>SOURCE_MARIADB_IP</code>, query the puppet-generated configurations in the Controller and CellController nodes. You don&#8217;t need to run it on your environment:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>sudo grep -rI 'listen mysql' -A10 /var/lib/config-data/puppet-generated/ | grep bind</pre>
</div>
</div>
<div class="paragraph">
<p>The source cloud always uses the same password for cells databases. For that reason, the same passwords file is used for all cells stacks. However, split-stack topology allows using different passwords files for each stack.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="ulist">
<ul>
<li>
<p>Prepare the MariaDB adoption helper pod:</p>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Create a temporary volume claim and a pod for the database data copy. Edit the volume claim storage request if necessary, to give it enough space for the overcloud databases:</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc apply -f - &lt;&lt;EOF
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mariadb-data
spec:
  storageClassName: $STORAGE_CLASS
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
---
apiVersion: v1
kind: Pod
metadata:
  name: mariadb-copy-data
  annotations:
    openshift.io/scc: anyuid
    k8s.v1.cni.cncf.io/networks: internalapi
  labels:
    app: adoption
spec:
  containers:
  - image: $MARIADB_IMAGE
    command: [ "sh", "-c", "sleep infinity"]
    name: adoption
    volumeMounts:
    - mountPath: /backup
      name: mariadb-data
  securityContext:
    allowPrivilegeEscalation: false
    capabilities:
      drop: ALL
    runAsNonRoot: true
    seccompProfile:
      type: RuntimeDefault
  volumes:
  - name: mariadb-data
    persistentVolumeClaim:
      claimName: mariadb-data
EOF</code></pre>
</div>
</div>
</li>
<li>
<p>Wait for the pod to be ready:</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc wait --for condition=Ready pod/mariadb-copy-data --timeout=30s</code></pre>
</div>
</div>
</li>
</ol>
</div>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Get the count of source databases with the <code>NOK</code> (not-OK) status:</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">for CELL in $(echo $CELLS); do
  oc rsh mariadb-copy-data mysql -h "${SOURCE_MARIADB_IP[$CELL]}" -uroot -p"${SOURCE_DB_ROOT_PASSWORD[$CELL]}" -e "SHOW databases;"
done</code></pre>
</div>
</div>
</li>
<li>
<p>Check that <code>mysqlcheck</code> had no errors:</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">for CELL in $(echo $CELLS); do
  set +u
  . ~/.source_cloud_exported_variables_$CELL
  set -u
done
test -z "$PULL_OPENSTACK_CONFIGURATION_MYSQLCHECK_NOK"  || [ "x$PULL_OPENSTACK_CONFIGURATION_MYSQLCHECK_NOK" = "x " ] &amp;&amp; echo "OK" || echo "CHECK FAILED"</code></pre>
</div>
</div>
</li>
<li>
<p>Test the connection to the control plane upcall and cells databases:</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">for CELL in $(echo "super $RENAMED_CELLS"); do
  oc run mariadb-client --image $MARIADB_IMAGE -i --rm --restart=Never -- \
    mysql -rsh "${PODIFIED_MARIADB_IP[$CELL]}" -uroot -p"${PODIFIED_DB_ROOT_PASSWORD[$CELL]}" -e 'SHOW databases;'
done</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
You must transition Compute services that you import later into a superconductor architecture by deleting the old service records in the cell databases, starting with <code>cell1</code>. New records are registered with different hostnames that are provided by the {compute_service} operator. All Compute services, except the Compute agent, have no internal state, and you can safely delete their service records. You also need to rename the former <code>default</code> cell to <code>DEFAULT_CELL_NAME</code>.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Create a dump of the original databases:</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">for CELL in $(echo $CELLS); do
  oc rsh mariadb-copy-data &lt;&lt; EOF
    mysql -h"${SOURCE_MARIADB_IP[$CELL]}" -uroot -p"${SOURCE_DB_ROOT_PASSWORD[$CELL]}" \
    -N -e "show databases" | grep -E -v "schema|mysql|gnocchi|aodh" | \
    while read dbname; do
      echo "Dumping $CELL cell \${dbname}";
      mysqldump -h"${SOURCE_MARIADB_IP[$CELL]}" -uroot -p"${SOURCE_DB_ROOT_PASSWORD[$CELL]}" \
        --single-transaction --complete-insert --skip-lock-tables --lock-tables=0 \
        "\${dbname}" &gt; /backup/"${CELL}.\${dbname}".sql;
    done
EOF
done</code></pre>
</div>
</div>
<div class="paragraph">
<p>Note filtering the information and performance schema tables.
Gnocchi is no longer used as a metric store as well</p>
</div>
</li>
<li>
<p>Restore the databases from <code>.sql</code> files into the control plane MariaDB:</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">for CELL in $(echo $CELLS); do
  RCELL=$CELL
  [ "$CELL" = "default" ] &amp;&amp; RCELL=$DEFAULT_CELL_NAME
  oc rsh mariadb-copy-data &lt;&lt; EOF
    declare -A db_name_map
    db_name_map['nova']="nova_$RCELL"
    db_name_map['ovs_neutron']='neutron'
    db_name_map['ironic-inspector']='ironic_inspector'
    declare -A db_cell_map
    db_cell_map['nova']="nova_$DEFAULT_CELL_NAME"
    db_cell_map["nova_$RCELL"]="nova_$RCELL"
    declare -A db_server_map
    db_server_map['default']=${PODIFIED_MARIADB_IP['super']}
    db_server_map["nova"]=${PODIFIED_MARIADB_IP[$DEFAULT_CELL_NAME]}
    db_server_map["nova_$RCELL"]=${PODIFIED_MARIADB_IP[$RCELL]}
    declare -A db_server_password_map
    db_server_password_map['default']=${PODIFIED_DB_ROOT_PASSWORD['super']}
    db_server_password_map["nova"]=${PODIFIED_DB_ROOT_PASSWORD[$DEFAULT_CELL_NAME]}
    db_server_password_map["nova_$RCELL"]=${PODIFIED_DB_ROOT_PASSWORD[$RCELL]}
    cd /backup
    for db_file in \$(ls ${CELL}.*.sql); do
      db_name=\$(echo \${db_file} | awk -F'.' '{ print \$2; }')
      [[ "$CELL" != "default" &amp;&amp; ! -v "db_cell_map[\${db_name}]" ]] &amp;&amp; continue
      if [[ "$CELL" == "default" &amp;&amp; -v "db_cell_map[\${db_name}]" ]] ; then
        target=$DEFAULT_CELL_NAME
      elif [[ "$CELL" == "default" &amp;&amp; ! -v "db_cell_map[\${db_name}]" ]] ; then
        target=super
      else
        target=$RCELL
      fi
      renamed_db_file="\${target}_new.\${db_name}.sql"
      mv -f \${db_file} \${renamed_db_file}
      if [[ -v "db_name_map[\${db_name}]" ]]; then
        echo "renaming $CELL cell \${db_name} to \$target \${db_name_map[\${db_name}]}"
        db_name=\${db_name_map[\${db_name}]}
      fi
      db_server=\${db_server_map["default"]}
      if [[ -v "db_server_map[\${db_name}]" ]]; then
        db_server=\${db_server_map[\${db_name}]}
      fi
      db_password=\${db_server_password_map['default']}
      if [[ -v "db_server_password_map[\${db_name}]" ]]; then
        db_password=\${db_server_password_map[\${db_name}]}
      fi
      echo "creating $CELL cell \${db_name} in \$target \${db_server}"
      mysql -h"\${db_server}" -uroot "-p\${db_password}" -e \
        "CREATE DATABASE IF NOT EXISTS \${db_name} DEFAULT \
        CHARACTER SET ${CHARACTER_SET} DEFAULT COLLATE ${COLLATION};"
      echo "importing $CELL cell \${db_name} into \$target \${db_server} from \${renamed_db_file}"
      mysql -h "\${db_server}" -uroot "-p\${db_password}" "\${db_name}" &lt; "\${renamed_db_file}"
    done
    if [ "$CELL" = "default" ] ; then
      mysql -h "\${db_server_map['default']}" -uroot -p"\${db_server_password_map['default']}" -e \
        "update nova_api.cell_mappings set name='$DEFAULT_CELL_NAME' where name='default';"
    fi
    mysql -h "\${db_server_map["nova_$RCELL"]}" -uroot -p"\${db_server_password_map["nova_$RCELL"]}" -e \
      "delete from nova_${RCELL}.services where host not like '%nova_${RCELL}-%' and services.binary != 'nova-compute';"
EOF
done</code></pre>
</div>
</div>
<div class="paragraph">
<div class="title">Verification</div>
<p>Compare the following outputs with the topology-specific service configuration.
For more information, see <a href="#proc_retrieving-topology-specific-service-configuration_migrating-databases">Retrieving topology-specific service configuration</a>.</p>
</div>
</li>
<li>
<p>Check that the databases are imported correctly:</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">set +u
. ~/.source_cloud_exported_variables_default
set -u
dbs=$(oc exec openstack-galera-0 -c galera -- mysql -rs -uroot -p"${PODIFIED_DB_ROOT_PASSWORD['super']}" -e 'SHOW databases;')
echo $dbs | grep -Eq '\bkeystone\b' &amp;&amp; echo "OK" || echo "CHECK FAILED"
echo $dbs | grep -Eq '\bneutron\b' &amp;&amp; echo "OK" || echo "CHECK FAILED"
echo "${PULL_OPENSTACK_CONFIGURATION_DATABASES[@]}" | grep -Eq '\bovs_neutron\b' &amp;&amp; echo "OK" || echo "CHECK FAILED"
novadb_mapped_cells=$(oc exec openstack-galera-0 -c galera -- mysql -rs -uroot -p"${PODIFIED_DB_ROOT_PASSWORD['super']}" \
nova_api -e 'select uuid,name,transport_url,database_connection,disabled from cell_mappings;')
uuidf='\S{8,}-\S{4,}-\S{4,}-\S{4,}-\S{12,}'
default=$(printf "%s\n" "$PULL_OPENSTACK_CONFIGURATION_NOVADB_MAPPED_CELLS" | sed -rn "s/^($uuidf)\s+default\b.*$/\1/p")
difference=$(diff -ZNua \
  &lt;(printf "%s\n" "$PULL_OPENSTACK_CONFIGURATION_NOVADB_MAPPED_CELLS") \
  &lt;(printf "%s\n" "$novadb_mapped_cells")) || true
if [ "$DEFAULT_CELL_NAME" != "default" ]; then
  printf "%s\n" "$difference" | grep -qE "^\-$default\s+default\b" &amp;&amp; echo "OK" || echo "CHECK FAILED"
  printf "%s\n" "$difference" | grep -qE "^\+$default\s+$DEFAULT_CELL_NAME\b" &amp;&amp; echo "OK" || echo "CHECK FAILED"
  [ $(grep -E "^[-\+]$uuidf" &lt;&lt;&lt;"$difference" | wc -l) -eq 2 ] &amp;&amp; echo "OK" || echo "CHECK FAILED"
else
  [ "x$difference" = "x" ] &amp;&amp; echo "OK" || echo "CHECK FAILED"
fi
for CELL in $(echo $RENAMED_CELLS); do
  RCELL=$CELL
  [ "$CELL" = "$DEFAULT_CELL_NAME" ] &amp;&amp; RCELL=default
  set +u
  . ~/.source_cloud_exported_variables_$RCELL
  set -u
  c1dbs=$(oc exec openstack-$CELL-galera-0 -c galera -- mysql -rs -uroot -p${PODIFIED_DB_ROOT_PASSWORD[$CELL]} -e 'SHOW databases;')
  echo $c1dbs | grep -Eq "\bnova_${CELL}\b" &amp;&amp; echo "OK" || echo "CHECK FAILED"
  novadb_svc_records=$(oc exec openstack-$CELL-galera-0 -c galera -- mysql -rs -uroot -p${PODIFIED_DB_ROOT_PASSWORD[$CELL]} \
    nova_$CELL -e "select host from services where services.binary='nova-compute' and deleted=0 order by host asc;")
  diff -Z &lt;(echo "x$novadb_svc_records") &lt;(echo "x${PULL_OPENSTACK_CONFIGURATION_NOVA_COMPUTE_HOSTNAMES[@]}") &amp;&amp; echo "OK" || echo "CHECK FAILED"
done</code></pre>
</div>
</div>
</li>
<li>
<p>Delete the <code>mariadb-data</code> pod and the <code>mariadb-copy-data</code> persistent volume claim that contains the database backup:</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Consider taking a snapshot of them before deleting.
</td>
</tr>
</table>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc delete pod mariadb-copy-data
oc delete pvc mariadb-data</code></pre>
</div>
</div>
</li>
</ol>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
During the pre-checks and post-checks, the <code>mariadb-client</code> pod might return a pod security warning related to the <code>restricted:latest</code> security context constraint. This warning is due to default security context constraints and does not prevent the admission controller from creating a pod. You see a warning for the short-lived pod, but it does not interfere with functionality.
For more information, see <a href="https://learn.redhat.com/t5/DO280-Red-Hat-OpenShift/About-pod-security-standards-and-warnings/m-p/32502">About pod security standards and warnings</a>.
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect1">
<h2 id="migrating-ovn-data_migrating-databases"><a class="anchor" href="#migrating-ovn-data_migrating-databases"></a>Migrating OVN data</h2>
<div class="sectionbody">
<div class="paragraph _abstract">
<p>Migrate the data in the OVN databases from the original {rhos_prev_long} deployment to <code>ovsdb-server</code> instances that are running in the {rhocp_long} cluster.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>The <code>OpenStackControlPlane</code> resource is created.</p>
</li>
<li>
<p><code>NetworkAttachmentDefinition</code> custom resources (CRs) for the original cluster are defined. Specifically, the <code>internalapi</code> network is defined.</p>
</li>
<li>
<p>The original {networking_first_ref} and OVN <code>northd</code> are not running.</p>
</li>
<li>
<p>There is network routability between the control plane services and the adopted cluster.</p>
</li>
<li>
<p>The cloud is migrated to the Modular Layer 2 plug-in with Open Virtual Networking (ML2/OVN) mechanism driver.</p>
</li>
<li>
<p>Define the following shell variables. Replace the example values with values that are correct for your environment:</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">STORAGE_CLASS=nfs-storage
OVSDB_IMAGE=registry.redhat.io/rhoso/openstack-ovn-base-rhel9:18.0
SOURCE_OVSDB_IP=192.168.52.16</code></pre>
</div>
</div>
<div class="paragraph">
<p>To get the value to set <code>SOURCE_OVSDB_IP</code>, query the puppet-generated configurations in a Controller node. You don&#8217;t need to run this on your environment.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>grep -rI 'ovn_[ns]b_conn' /var/lib/config-data/puppet-generated/</pre>
</div>
</div>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Prepare a temporary <code>PersistentVolume</code> claim and the helper pod for the OVN backup. Adjust the storage requests for a large database, if needed:</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc apply -f - &lt;&lt;EOF
---
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: ovn-data-cert
spec:
  commonName: ovn-data-cert
  secretName: ovn-data-cert
  issuerRef:
    name: rootca-internal
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ovn-data
spec:
  storageClassName: $STORAGE_CLASS
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
---
apiVersion: v1
kind: Pod
metadata:
  name: ovn-copy-data
  annotations:
    openshift.io/scc: anyuid
    k8s.v1.cni.cncf.io/networks: internalapi
  labels:
    app: adoption
spec:
  containers:
  - image: $OVSDB_IMAGE
    command: [ "sh", "-c", "sleep infinity"]
    name: adoption
    volumeMounts:
    - mountPath: /backup
      name: ovn-data
    - mountPath: /etc/pki/tls/misc
      name: ovn-data-cert
      readOnly: true
  securityContext:
    allowPrivilegeEscalation: false
    capabilities:
      drop: ALL
    runAsNonRoot: true
    seccompProfile:
      type: RuntimeDefault
  volumes:
  - name: ovn-data
    persistentVolumeClaim:
      claimName: ovn-data
  - name: ovn-data-cert
    secret:
      secretName: ovn-data-cert
EOF</code></pre>
</div>
</div>
</li>
<li>
<p>Wait for the pod to be ready:</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc wait --for=condition=Ready pod/ovn-copy-data --timeout=30s</code></pre>
</div>
</div>
</li>
<li>
<p>Back up your OVN databases:</p>
<div class="ulist">
<ul>
<li>
<p>If you did not enable TLS everywhere, run the following command:</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc exec ovn-copy-data -- bash -c "ovsdb-client backup tcp:$SOURCE_OVSDB_IP:6641 &gt; /backup/ovs-nb.db"
oc exec ovn-copy-data -- bash -c "ovsdb-client backup tcp:$SOURCE_OVSDB_IP:6642 &gt; /backup/ovs-sb.db"</code></pre>
</div>
</div>
</li>
</ul>
</div>
</li>
<li>
<p>Start the control plane OVN database services prior to import, with <code>northd</code> disabled:</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc patch openstackcontrolplane openstack --type=merge --patch '
spec:
  ovn:
    enabled: true
    template:
      ovnDBCluster:
        ovndbcluster-nb:
          replicas: 3
          dbType: NB
          storageRequest: 10G
          networkAttachment: internalapi
        ovndbcluster-sb:
          replicas: 3
          dbType: SB
          storageRequest: 10G
          networkAttachment: internalapi
      ovnNorthd:
        replicas: 0
'</code></pre>
</div>
</div>
</li>
<li>
<p>Wait for the OVN database services to reach the <code>Running</code> phase:</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc wait --for=jsonpath='{.status.phase}'=Running pod --selector=service=ovsdbserver-nb
oc wait --for=jsonpath='{.status.phase}'=Running pod --selector=service=ovsdbserver-sb</code></pre>
</div>
</div>
</li>
<li>
<p>Fetch the OVN database IP addresses on the <code>clusterIP</code> service network:</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">PODIFIED_OVSDB_NB_IP=$(oc get svc --selector "statefulset.kubernetes.io/pod-name=ovsdbserver-nb-0" -ojsonpath='{.items[0].spec.clusterIP}')
PODIFIED_OVSDB_SB_IP=$(oc get svc --selector "statefulset.kubernetes.io/pod-name=ovsdbserver-sb-0" -ojsonpath='{.items[0].spec.clusterIP}')</code></pre>
</div>
</div>
</li>
<li>
<p>Upgrade the database schema for the backup files:</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>If you did not enable TLS everywhere, use the following command:</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc exec ovn-copy-data -- bash -c "ovsdb-client get-schema tcp:$PODIFIED_OVSDB_NB_IP:6641 &gt; /backup/ovs-nb.ovsschema &amp;&amp; ovsdb-tool convert /backup/ovs-nb.db /backup/ovs-nb.ovsschema"
oc exec ovn-copy-data -- bash -c "ovsdb-client get-schema tcp:$PODIFIED_OVSDB_SB_IP:6642 &gt; /backup/ovs-sb.ovsschema &amp;&amp; ovsdb-tool convert /backup/ovs-sb.db /backup/ovs-sb.ovsschema"</code></pre>
</div>
</div>
</li>
</ol>
</div>
</li>
<li>
<p>Restore the database backup to the new OVN database servers:</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>If you did not enable TLS everywhere, use the following command:</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc exec ovn-copy-data -- bash -c "ovsdb-client restore tcp:$PODIFIED_OVSDB_NB_IP:6641 &lt; /backup/ovs-nb.db"
oc exec ovn-copy-data -- bash -c "ovsdb-client restore tcp:$PODIFIED_OVSDB_SB_IP:6642 &lt; /backup/ovs-sb.db"</code></pre>
</div>
</div>
</li>
</ol>
</div>
</li>
<li>
<p>Check that the data was successfully migrated by running the following commands against the new database servers, for example:</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc exec -it ovsdbserver-nb-0 -- ovn-nbctl show
oc exec -it ovsdbserver-sb-0 -- ovn-sbctl list Chassis</code></pre>
</div>
</div>
</li>
<li>
<p>Start the control plane <code>ovn-northd</code> service to keep both OVN databases in sync:</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc patch openstackcontrolplane openstack --type=merge --patch '
spec:
  ovn:
    enabled: true
    template:
      ovnNorthd:
        replicas: 1
'</code></pre>
</div>
</div>
</li>
<li>
<p>Delete the <code>ovn-data</code> helper pod and the temporary <code>PersistentVolumeClaim</code> that is used to store OVN database backup files:</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc delete --ignore-not-found=true pod ovn-copy-data
oc delete --ignore-not-found=true pvc ovn-data</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Consider taking a snapshot of the <code>ovn-data</code> helper pod and the temporary <code>PersistentVolumeClaim</code> before deleting them. For more information, see <a href="{defaultOCPURL}/storage/index#lvms-about-volume-snapsot_logical-volume-manager-storage">About volume snapshots</a> in <em>OpenShift Container Platform storage overview</em>.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Stop the adopted OVN database servers:</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">ServicesToStop=("tripleo_ovn_cluster_north_db_server.service"
                "tripleo_ovn_cluster_south_db_server.service")

echo "Stopping systemd OpenStack services"
for service in ${ServicesToStop[*]}; do
    for i in {1..3}; do
        SSH_CMD=CONTROLLER${i}_SSH
        if [ ! -z "${!SSH_CMD}" ]; then
            echo "Stopping the $service in controller $i"
            if ${!SSH_CMD} sudo systemctl is-active $service; then
                ${!SSH_CMD} sudo systemctl stop $service
            fi
        fi
    done
done

echo "Checking systemd OpenStack services"
for service in ${ServicesToStop[*]}; do
    for i in {1..3}; do
        SSH_CMD=CONTROLLER${i}_SSH
        if [ ! -z "${!SSH_CMD}" ]; then
            if ! ${!SSH_CMD} systemctl show $service | grep ActiveState=inactive &gt;/dev/null; then
                echo "ERROR: Service $service still running on controller $i"
            else
                echo "OK: Service $service is not running on controller $i"
            fi
        fi
    done
done</code></pre>
</div>
</div>
</li>
</ol>
</div>
</div>
</div>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../../_/js/site.js" data-ui-root-path="../../../../_"></script>
<script async src="../../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
