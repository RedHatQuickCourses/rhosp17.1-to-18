:_mod-docs-content-type: PROCEDURE
[id="adopting-the-block-storage-service_{context}"]

= Adopting the {block_storage}

[role="_abstract"]
To adopt a {OpenStackPreviousInstaller}-deployed {block_storage_first_ref}, create the manifest based on the existing `cinder.conf` file, deploy the {block_storage}, and validate the new deployment.

.Prerequisites

* You have reviewed the {block_storage} limitations. For more information, see xref:block-storage-limitations_storage-requirements[Limitations for adopting the {block_storage}].
* You have planned the placement of the Block Storage services.
* You have prepared the {rhocp_long} nodes where the volume and backup services run. For more information, see xref:openshift-preparation-for-block-storage-adoption_storage-requirements[{OpenShiftShort} preparation for {block_storage} adoption].
* The {block_storage_first_ref} is stopped.
* The service databases are imported into the control plane MariaDB.
* The {identity_service_first_ref} and {key_manager_first_ref} are adopted.
* The Storage network is correctly configured on the {OpenShiftShort} cluster.

.Procedure

. Create a new file, for example, `cinder_api.patch`:
+
[source,bash,role=execute]
----
cat << EOF > cinder_api.patch
spec:
  extraMounts:
  - extraVol:
    - extraVolType: Ceph
      mounts:
      - mountPath: /etc/ceph
        name: ceph
        readOnly: true
      propagation:
      - CinderVolume
      - CinderBackup
      - Glance
      volumes:
      - name: ceph
        projected:
          sources:
          - secret:
              name: ceph-conf-files
  cinder:
    enabled: true
    apiOverride:
      route: {}
    template:
      databaseInstance: openstack
      databaseAccount: cinder
      secret: osp-secret
      cinderAPI:
        override:
          service:
            internal:
              metadata:
                annotations:
                  metallb.universe.tf/address-pool: internalapi
                  metallb.universe.tf/allow-shared-ip: internalapi
                  metallb.universe.tf/loadBalancerIPs: 192.168.52.89
              spec:
                type: LoadBalancer
        replicas: 1
        customServiceConfig: |
          [DEFAULT]
          default_volume_type=tripleo
      cinderScheduler:
        replicas: 0
      cinderBackup:
        replicas: 0
      cinderVolumes:
        ceph:
          replicas: 0
EOF
----

. Apply the configuration::
+
[source,bash,role=execute]
----
oc patch openstackcontrolplane openstack --type=merge --patch-file=cinder_api.patch
----

. Wait for Cinder control plane services' CRs to become ready:
+
[source,bash,role=execute,subs=attributes]
----
oc wait --for condition=Ready --timeout=300s Cinder/cinder
----

. Retrieve the list of the previous scheduler and backup services:
+
[source,bash,role=execute]
----
openstack volume service list

+------------------+------------------------+------+---------+-------+----------------------------+
| Binary           | Host                   | Zone | Status  | State | Updated At                 |
+------------------+------------------------+------+---------+-------+----------------------------+
| cinder-scheduler | overcloud-controller-0 | nova | enabled | down  | 2025-11-15T07:58:05.000000 |
| cinder-scheduler | overcloud-controller-1 | nova | enabled | down  | 2025-11-15T07:58:05.000000 |
| cinder-scheduler | overcloud-controller-2 | nova | enabled | down  | 2025-11-15T07:58:05.000000 |
| cinder-volume    | hostgroup@tripleo_ceph | nova | enabled | down  | 2025-11-15T07:58:05.000000 |
+------------------+------------------------+------+---------+-------+----------------------------+
----

. Remove services for hosts that are in the `down` state:
+
[source,bash,role=execute]
----
oc exec -t cinder-api-0 -c cinder-api -- cinder-manage service remove cinder-scheduler overcloud-controller-0
oc exec -t cinder-api-0 -c cinder-api -- cinder-manage service remove cinder-scheduler overcloud-controller-1
oc exec -t cinder-api-0 -c cinder-api -- cinder-manage service remove cinder-scheduler overcloud-controller-2
oc exec -t cinder-api-0 -c cinder-api -- cinder-manage service remove cinder-volume hostgroup@tripleo_ceph
----

. Deploy the scheduler, backup, and volume services:
+
* Create another file, for example, `cinder_services.patch`, and apply the configuration:
+
[source,yaml]
----
cat << EOF > cinder_services.patch
spec:
  cinder:
    enabled: true
    template:
      cinderScheduler:
        replicas: 1
      cinderBackup:
        replicas: 0
      cinderVolumes:
        ceph:
          replicas: 1
          customServiceConfig: |
            [tripleo_ceph]
            backend_host=hostgroup
            volume_backend_name=tripleo_ceph
            volume_driver=cinder.volume.drivers.rbd.RBDDriver
            rbd_ceph_conf=/etc/ceph/ceph.conf
            rbd_user=openstack
            rbd_pool=volumes
            rbd_flatten_volume_from_snapshot=False
            report_discard_supported=True
EOF
----
. Apply the configuration::
+
[source,bash,role=execute]
----
oc patch openstackcontrolplane openstack --type=merge --patch-file=cinder_services.patch
----
+
. Apply the DB data migrations:
+
[NOTE]
====
You are not required to run the data migrations at this step, but you must run them before the next upgrade. However, for adoption, you can run the migrations now to ensure that there are no issues before you run production workloads on the deployment.
====
+
[source,bash,role=execute]
----
oc exec -it cinder-scheduler-0 -- cinder-manage db online_data_migrations
----

.Verification

. Ensure that the `openstack` alias is defined:
+
[source,bash,role=execute]
----
alias openstack="oc exec -t openstackclient -- openstack"
----

. Confirm that {block_storage} endpoints are defined and pointing to the control plane FQDNs:
+
[source,bash,role=execute]
----
openstack endpoint list --service cinderv3
----
+
* Replace `<endpoint>` with the name of the endpoint that you want to confirm.

. Confirm that the Block Storage services are running:
+
[source,bash,role=execute]
----
openstack volume service list
----
+
[NOTE]
Cinder API services do not appear in the list. However, if you get a response from the `openstack volume service list` command, that means at least one of the cinder API services is running.

. Confirm that you have your previous volume types, volumes, snapshots, and backups:
+
[source,bash,role=execute]
----
openstack volume type list
openstack volume list
openstack volume snapshot list
----

. To confirm that the configuration is working, perform the following steps:

.. Create a volume from an image to check that the connection to {image_service_first_ref} is working:
+
[source,bash,role=execute]
----
openstack volume create --image cirros --bootable --size 1 disk_new
----

.. Back up the previous attached volume:
+
[source,bash,role=execute]
----
openstack --os-volume-api-version 3.47 volume create --backup <backup_name>
----
+
* Replace `<backup_name>` with the name of your new backup location.
+
[NOTE]
You do not boot a {compute_service_first_ref} instance by using the new `volume from` image or try to detach the previous volume because the {compute_service} and the {block_storage} are still not connected.
